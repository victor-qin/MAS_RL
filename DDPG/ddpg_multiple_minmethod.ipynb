{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Taken from https://github.com/marload/DeepRL-TensorFlow2 ##\n",
    "\n",
    "import wandb\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, concatenate\n",
    "# import tensorflow_federated as tff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import argparse\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--gamma', type=float, default=0.99)\n",
    "# parser.add_argument('--actor_lr', type=float, default=0.0005)\n",
    "# parser.add_argument('--critic_lr', type=float, default=0.001)\n",
    "# parser.add_argument('--batch_size', type=int, default=64)\n",
    "# parser.add_argument('--tau', type=float, default=0.05)\n",
    "# parser.add_argument('--train_start', type=int, default=2000)\n",
    "\n",
    "# args = parser.parse_args()\n",
    "\n",
    "\n",
    "\n",
    "# class Args:\n",
    "#     gamma = 0.99\n",
    "#     actor_lr = 0.0005\n",
    "#     critic_lr = 0.0005\n",
    "#     batch_size = 64\n",
    "#     tau = 0.05\n",
    "#     train_start = 400\n",
    "#     episodes = 10\n",
    "#     N = 3\n",
    "#     epochs = 100\n",
    "\n",
    "# args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=20000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def put(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append([state, action, reward, next_state, done])\n",
    "    \n",
    "    def sample(self):\n",
    "        sample = random.sample(self.buffer, wandb.config.batch_size)\n",
    "        states, actions, rewards, next_states, done = map(np.asarray, zip(*sample))\n",
    "        states = np.array(states).reshape(wandb.config.batch_size, -1)\n",
    "        next_states = np.array(next_states).reshape(wandb.config.batch_size, -1)\n",
    "        return states, actions, rewards, next_states, done\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor:\n",
    "    def __init__(self, state_dim, action_dim, action_bound):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.action_bound = action_bound\n",
    "        self.model = self.create_model()\n",
    "        self.opt = tf.keras.optimizers.Adam(wandb.config.actor_lr)\n",
    "\n",
    "    def create_model(self):\n",
    "        return tf.keras.Sequential([\n",
    "            Input((self.state_dim,)),\n",
    "            Dense(wandb.config.actor['layer1'], activation='relu'),\n",
    "            Dense(wandb.config.actor['layer2'], activation='relu'),\n",
    "            Dense(self.action_dim, activation='tanh'),\n",
    "            Lambda(lambda x: x * self.action_bound)\n",
    "        ])\n",
    "\n",
    "    def train(self, states, q_grads):\n",
    "        with tf.GradientTape() as tape:\n",
    "            grads = tape.gradient(self.model(states), self.model.trainable_variables, -q_grads)\n",
    "        self.opt.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "    \n",
    "    def predict(self, state):\n",
    "        return self.model.predict(state)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        state = np.reshape(state, [1, self.state_dim])\n",
    "        return self.model.predict(state)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic:\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.model = self.create_model()\n",
    "        self.opt = tf.keras.optimizers.Adam(wandb.config.critic_lr)\n",
    "\n",
    "    def create_model(self):\n",
    "        state_input = Input((self.state_dim,))\n",
    "        s1 = Dense(wandb.config.critic['state1'], activation='relu')(state_input)\n",
    "        s2 = Dense(wandb.config.critic['state2'], activation='relu')(s1)\n",
    "        action_input = Input((self.action_dim,))\n",
    "        a1 = Dense(wandb.config.critic['actor1'], activation='relu')(action_input)\n",
    "        c1 = concatenate([s2, a1], axis=-1)\n",
    "        c2 = Dense(wandb.config.critic['cat1'], activation='relu')(c1)\n",
    "        output = Dense(1, activation='linear')(c2)\n",
    "        return tf.keras.Model([state_input, action_input], output)\n",
    "    \n",
    "    def predict(self, inputs):\n",
    "        return self.model.predict(inputs)\n",
    "    \n",
    "    def q_grads(self, states, actions):\n",
    "        actions = tf.convert_to_tensor(actions)\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(actions)\n",
    "            q_values = self.model([states, actions])\n",
    "            q_values = tf.squeeze(q_values)\n",
    "        return tape.gradient(q_values, actions)\n",
    "\n",
    "    def compute_loss(self, v_pred, td_targets):\n",
    "        mse = tf.keras.losses.MeanSquaredError()\n",
    "        return mse(td_targets, v_pred)\n",
    "\n",
    "    def train(self, states, actions, td_targets):\n",
    "        with tf.GradientTape() as tape:\n",
    "            v_pred = self.model([states, actions], training=True)\n",
    "            assert v_pred.shape == td_targets.shape\n",
    "            loss = self.compute_loss(v_pred, tf.stop_gradient(td_targets))\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.opt.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env, iden = 0):\n",
    "        self.env = env\n",
    "        self.state_dim = self.env.observation_space.shape[0]\n",
    "        self.action_dim = self.env.action_space.shape[0]\n",
    "        self.action_bound = self.env.action_space.high[0]\n",
    "\n",
    "        self.buffer = ReplayBuffer()\n",
    "\n",
    "        self.actor = Actor(self.state_dim, self.action_dim, self.action_bound)\n",
    "        self.critic = Critic(self.state_dim, self.action_dim)\n",
    "        \n",
    "        self.target_actor = Actor(self.state_dim, self.action_dim, self.action_bound)\n",
    "        self.target_critic = Critic(self.state_dim, self.action_dim)\n",
    "\n",
    "        actor_weights = self.actor.model.get_weights()\n",
    "        critic_weights = self.critic.model.get_weights()\n",
    "        self.target_actor.model.set_weights(actor_weights)\n",
    "        self.target_critic.model.set_weights(critic_weights)\n",
    "        \n",
    "        self.iden = iden\n",
    "        \n",
    "    \n",
    "    def target_update(self):\n",
    "        actor_weights = self.actor.model.get_weights()\n",
    "        t_actor_weights = self.target_actor.model.get_weights()\n",
    "        critic_weights = self.critic.model.get_weights()\n",
    "        t_critic_weights = self.target_critic.model.get_weights()\n",
    "\n",
    "        for i in range(len(actor_weights)):\n",
    "            t_actor_weights[i] = wandb.config.tau * actor_weights[i] + (1 - wandb.config.tau) * t_actor_weights[i]\n",
    "\n",
    "        for i in range(len(critic_weights)):\n",
    "            t_critic_weights[i] = wandb.config.tau * critic_weights[i] + (1 - wandb.config.tau) * t_critic_weights[i]\n",
    "        \n",
    "        self.target_actor.model.set_weights(t_actor_weights)\n",
    "        self.target_critic.model.set_weights(t_critic_weights)\n",
    "\n",
    "\n",
    "    def td_target(self, rewards, q_values, dones):\n",
    "        targets = np.asarray(q_values)\n",
    "        for i in range(q_values.shape[0]):\n",
    "            if dones[i]:\n",
    "                targets[i] = rewards[i]\n",
    "            else:\n",
    "                targets[i] = wandb.config.gamma * q_values[i]\n",
    "        return targets\n",
    "\n",
    "    def list_to_batch(self, list):\n",
    "        batch = list[0]\n",
    "        for elem in list[1:]:\n",
    "            batch = np.append(batch, elem, axis=0)\n",
    "        return batch\n",
    "    \n",
    "    def ou_noise(self, x, rho=0.15, mu=0, dt=1e-1, sigma=0.2, dim=1):\n",
    "        return x + rho * (mu-x) * dt + sigma * np.sqrt(dt) * np.random.normal(size=dim)\n",
    "    \n",
    "    def replay(self):\n",
    "        for _ in range(10):\n",
    "            states, actions, rewards, next_states, dones = self.buffer.sample()\n",
    "            target_q_values = self.target_critic.predict([next_states, self.target_actor.predict(next_states)])\n",
    "            td_targets = self.td_target(rewards, target_q_values, dones)\n",
    "            \n",
    "            self.critic.train(states, actions, td_targets)\n",
    "            \n",
    "            s_actions = self.actor.predict(states)\n",
    "            s_grads = self.critic.q_grads(states, s_actions)\n",
    "            grads = np.array(s_grads).reshape((-1, self.action_dim))\n",
    "            self.actor.train(states, grads)\n",
    "            self.target_update()\n",
    "\n",
    "    def train(self, max_episodes=1000):\n",
    "        for ep in range(max_episodes):      # train a bunch of episodes\n",
    "            episode_reward, done = 0, False\n",
    "\n",
    "            state = self.env.reset()\n",
    "            bg_noise = np.zeros(self.action_dim)\n",
    "            while not done:    # run till done by hitting the action that's done\n",
    "#                 self.env.render()\n",
    "                action = self.actor.get_action(state)   # pick an action, add noise, clip the action\n",
    "                noise = self.ou_noise(bg_noise, dim=self.action_dim)\n",
    "                action = np.clip(action + noise, -self.action_bound, self.action_bound)\n",
    "\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                self.buffer.put(state, action, (reward+8)/8, next_state, done)\n",
    "                bg_noise = noise     # why does the noise wander in such a weird way\n",
    "                episode_reward += reward\n",
    "                state = next_state\n",
    "                \n",
    "            if self.buffer.size() >= wandb.config.batch_size and self.buffer.size() >= wandb.config.train_start:    # update the states if enough\n",
    "                self.replay()                \n",
    "            print('EP{} EpisodeReward={}'.format(ep, episode_reward))\n",
    "            wandb.log({'Reward' + str(self.iden): episode_reward})\n",
    "            \n",
    "        return episode_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to query for notebook name, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvictor-qin\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "/n/home05/vqin/.conda/envs/es100/lib/python3.7/site-packages/IPython/html.py:14: ShimWarning: The `IPython.html` package has been deprecated since IPython 4.0. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  \"`IPython.html.widgets` has moved to `ipywidgets`.\", ShimWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.10<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">DDPG-multiplemax-long</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/victor-qin/deep-rl-tf2\" target=\"_blank\">https://wandb.ai/victor-qin/deep-rl-tf2</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/victor-qin/deep-rl-tf2/runs/31akm7v5\" target=\"_blank\">https://wandb.ai/victor-qin/deep-rl-tf2/runs/31akm7v5</a><br/>\n",
       "                Run data is saved locally in <code>/n/home05/vqin/fasrc/es100_workspace/wandb/run-20201122_003223-31akm7v5</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.99, 'actor_lr': 0.001, 'critic_lr': 0.0001, 'batch_size': 64, 'tau': 0.005, 'train_start': 400, 'episodes': 5, 'num': 3, 'epochs': 200, 'actor': {'layer1': 128, 'layer2': 128}, 'critic': {'state1': 256, 'state2': 128, 'actor1': 128, 'cat1': 64}}\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1865.661897081727\n",
      "EP1 EpisodeReward=-1896.9704881271523\n",
      "EP2 EpisodeReward=-1332.532200490319\n",
      "EP3 EpisodeReward=-1623.0147684270898\n",
      "EP4 EpisodeReward=-1314.7808665527693\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1275.2120864119536\n",
      "EP1 EpisodeReward=-1602.5868291956035\n",
      "EP2 EpisodeReward=-1302.0490003300629\n",
      "EP3 EpisodeReward=-1478.24253949674\n",
      "EP4 EpisodeReward=-1463.1121287973185\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1684.6560017237125\n",
      "EP1 EpisodeReward=-1713.8583043870194\n",
      "EP2 EpisodeReward=-1444.509307557442\n",
      "EP3 EpisodeReward=-1338.4681808349153\n",
      "EP4 EpisodeReward=-897.5733376772048\n",
      "Epoch=0\t Average reward=-1225.1554443424309\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1403.8583521804526\n",
      "EP1 EpisodeReward=-1778.3824468043952\n",
      "EP2 EpisodeReward=-1695.2957701304513\n",
      "EP3 EpisodeReward=-1614.3480113075072\n",
      "EP4 EpisodeReward=-1671.2555876586434\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1486.861735874117\n",
      "EP1 EpisodeReward=-1873.4537608507994\n",
      "EP2 EpisodeReward=-1767.30890218459\n",
      "EP3 EpisodeReward=-1598.5723491629449\n",
      "EP4 EpisodeReward=-1663.9165542212086\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1156.154040336135\n",
      "EP1 EpisodeReward=-1226.3864126337965\n",
      "EP2 EpisodeReward=-1634.982669510715\n",
      "EP3 EpisodeReward=-1418.315575553963\n",
      "EP4 EpisodeReward=-1540.1380456498143\n",
      "Epoch=1\t Average reward=-1625.103395843222\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1726.9815533508881\n",
      "EP1 EpisodeReward=-1755.8589743402924\n",
      "EP2 EpisodeReward=-1553.5108713282527\n",
      "EP3 EpisodeReward=-1601.4371095019173\n",
      "EP4 EpisodeReward=-1491.858445089248\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1846.6425357803805\n",
      "EP1 EpisodeReward=-1736.1440277033469\n",
      "EP2 EpisodeReward=-1660.8633912077612\n",
      "EP3 EpisodeReward=-1595.6070750817862\n",
      "EP4 EpisodeReward=-1349.178403419389\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1635.3062169677244\n",
      "EP1 EpisodeReward=-1859.5167844458197\n",
      "EP2 EpisodeReward=-1774.6790956550622\n",
      "EP3 EpisodeReward=-1451.8712032091812\n",
      "EP4 EpisodeReward=-1697.208915957248\n",
      "Epoch=2\t Average reward=-1512.748588155295\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1514.572222698368\n",
      "EP1 EpisodeReward=-1522.5520149317874\n",
      "EP2 EpisodeReward=-1544.7328236349624\n",
      "EP3 EpisodeReward=-1539.0570883937728\n",
      "EP4 EpisodeReward=-1522.690781023705\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1433.4218130339082\n",
      "EP1 EpisodeReward=-1523.877368442826\n",
      "EP2 EpisodeReward=-1701.277833993285\n",
      "EP3 EpisodeReward=-1672.433735892666\n",
      "EP4 EpisodeReward=-1622.0914320262057\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1574.8572745893998\n",
      "EP1 EpisodeReward=-1537.144300096699\n",
      "EP2 EpisodeReward=-1509.5854747371166\n",
      "EP3 EpisodeReward=-1616.0775569258994\n",
      "EP4 EpisodeReward=-1529.4208535117139\n",
      "Epoch=3\t Average reward=-1558.0676888538749\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1399.6312796867396\n",
      "EP1 EpisodeReward=-1487.6197805075115\n",
      "EP2 EpisodeReward=-1519.7641932817269\n",
      "EP3 EpisodeReward=-1597.745689693415\n",
      "EP4 EpisodeReward=-1525.453916692941\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1465.9471831638098\n",
      "EP1 EpisodeReward=-1481.8789399106104\n",
      "EP2 EpisodeReward=-1644.911211953729\n",
      "EP3 EpisodeReward=-1677.5518997775523\n",
      "EP4 EpisodeReward=-1652.931536462801\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1534.7897780480007\n",
      "EP1 EpisodeReward=-1412.8743574244277\n",
      "EP2 EpisodeReward=-1640.3355524325336\n",
      "EP3 EpisodeReward=-1512.2882576206805\n",
      "EP4 EpisodeReward=-1534.8826743626119\n",
      "Epoch=4\t Average reward=-1571.0893758394511\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1393.946312471012\n",
      "EP1 EpisodeReward=-1560.5363535676997\n",
      "EP2 EpisodeReward=-1513.0572323188742\n",
      "EP3 EpisodeReward=-1526.7554120646225\n",
      "EP4 EpisodeReward=-1474.2309461700468\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1554.4091202717375\n",
      "EP1 EpisodeReward=-1481.142575390668\n",
      "EP2 EpisodeReward=-1467.2056756890697\n",
      "EP3 EpisodeReward=-1468.9181017947114\n",
      "EP4 EpisodeReward=-1518.9825502427173\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1505.5242852536692\n",
      "EP1 EpisodeReward=-1561.3829438184391\n",
      "EP2 EpisodeReward=-1523.0071179444749\n",
      "EP3 EpisodeReward=-1557.889582338478\n",
      "EP4 EpisodeReward=-1497.525280808207\n",
      "Epoch=5\t Average reward=-1496.9129257403238\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1322.0568390586288\n",
      "EP1 EpisodeReward=-1570.0768784372056\n",
      "EP2 EpisodeReward=-1475.3418643782493\n",
      "EP3 EpisodeReward=-1512.7503694972283\n",
      "EP4 EpisodeReward=-1540.650346144092\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1516.9377539271197\n",
      "EP1 EpisodeReward=-1546.6732068093881\n",
      "EP2 EpisodeReward=-1506.5366248621497\n",
      "EP3 EpisodeReward=-1499.3617533465363\n",
      "EP4 EpisodeReward=-1524.8728760482227\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1584.7032750903668\n",
      "EP1 EpisodeReward=-1479.5081313179423\n",
      "EP2 EpisodeReward=-1482.683505213347\n",
      "EP3 EpisodeReward=-1646.3607593208155\n",
      "EP4 EpisodeReward=-1429.861232738903\n",
      "Epoch=6\t Average reward=-1498.4614849770726\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1647.1064066818599\n",
      "EP1 EpisodeReward=-1639.8298795269895\n",
      "EP2 EpisodeReward=-1209.7870532649736\n",
      "EP3 EpisodeReward=-1546.0231629287994\n",
      "EP4 EpisodeReward=-1464.1543266193385\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1644.596658028506\n",
      "EP1 EpisodeReward=-1344.6711089834307\n",
      "EP2 EpisodeReward=-1495.709207074407\n",
      "EP3 EpisodeReward=-1583.0260317362415\n",
      "EP4 EpisodeReward=-1537.7143923652327\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1372.5692394055272\n",
      "EP1 EpisodeReward=-1510.4884018624148\n",
      "EP2 EpisodeReward=-1570.259739459469\n",
      "EP3 EpisodeReward=-1090.061310949681\n",
      "EP4 EpisodeReward=-1518.2842901834185\n",
      "Epoch=7\t Average reward=-1506.7176697226632\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1605.7893288951566\n",
      "EP1 EpisodeReward=-1624.5912901690142\n",
      "EP2 EpisodeReward=-1384.8706999512515\n",
      "EP3 EpisodeReward=-1527.8311634683603\n",
      "EP4 EpisodeReward=-1325.719512068446\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1464.7761264245112\n",
      "EP1 EpisodeReward=-1525.8927795043937\n",
      "EP2 EpisodeReward=-1557.732696864279\n",
      "EP3 EpisodeReward=-1342.5522674333345\n",
      "EP4 EpisodeReward=-1146.012965666039\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1377.1140934852947\n",
      "EP1 EpisodeReward=-1140.7296954609062\n",
      "EP2 EpisodeReward=-1471.6463601962814\n",
      "EP3 EpisodeReward=-1641.431783803997\n",
      "EP4 EpisodeReward=-1490.852385146289\n",
      "Epoch=8\t Average reward=-1320.8616209602578\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1577.4606104121096\n",
      "EP1 EpisodeReward=-1540.5921913643679\n",
      "EP2 EpisodeReward=-1440.1375192526687\n",
      "EP3 EpisodeReward=-1481.3021682612077\n",
      "EP4 EpisodeReward=-1313.837433747493\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1627.0128087426692\n",
      "EP1 EpisodeReward=-1500.3931834539476\n",
      "EP2 EpisodeReward=-1564.6255282346742\n",
      "EP3 EpisodeReward=-1629.324587383221\n",
      "EP4 EpisodeReward=-1490.6112732096872\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1648.4290182247357\n",
      "EP1 EpisodeReward=-1321.0394649003047\n",
      "EP2 EpisodeReward=-1649.7436717414619\n",
      "EP3 EpisodeReward=-1540.2105819415112\n",
      "EP4 EpisodeReward=-1615.5136746097294\n",
      "Epoch=9\t Average reward=-1473.3207938556363\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1306.0997092022847\n",
      "EP1 EpisodeReward=-1290.7289337247748\n",
      "EP2 EpisodeReward=-1514.847544292274\n",
      "EP3 EpisodeReward=-1576.761615782366\n",
      "EP4 EpisodeReward=-1161.219077174904\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1375.6286023860368\n",
      "EP1 EpisodeReward=-1537.9806948041155\n",
      "EP2 EpisodeReward=-1567.039711269819\n",
      "EP3 EpisodeReward=-1649.6494139931178\n",
      "EP4 EpisodeReward=-1079.8667150703395\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1140.3128822792808\n",
      "EP1 EpisodeReward=-1646.6035727622602\n",
      "EP2 EpisodeReward=-1467.2643136481065\n",
      "EP3 EpisodeReward=-1146.3211342608217\n",
      "EP4 EpisodeReward=-1511.421575303411\n",
      "Epoch=10\t Average reward=-1250.8357891828848\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1559.2633980493142\n",
      "EP1 EpisodeReward=-1622.8491639008203\n",
      "EP2 EpisodeReward=-1647.528546895596\n",
      "EP3 EpisodeReward=-1011.9128618538289\n",
      "EP4 EpisodeReward=-1579.9652712685427\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1367.577569273113\n",
      "EP1 EpisodeReward=-1484.2549793970286\n",
      "EP2 EpisodeReward=-1411.6016645928373\n",
      "EP3 EpisodeReward=-1613.5028158811124\n",
      "EP4 EpisodeReward=-1493.9596709057798\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1288.8794424371658\n",
      "EP1 EpisodeReward=-1516.6527661231742\n",
      "EP2 EpisodeReward=-1619.906503587906\n",
      "EP3 EpisodeReward=-1534.9657944159187\n",
      "EP4 EpisodeReward=-1596.5266118776847\n",
      "Epoch=11\t Average reward=-1556.8171846840023\n",
      "Training Agent 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP0 EpisodeReward=-1376.1430779322347\n",
      "EP1 EpisodeReward=-1489.8308501023776\n",
      "EP2 EpisodeReward=-1344.4822056732296\n",
      "EP3 EpisodeReward=-1499.686513955285\n",
      "EP4 EpisodeReward=-1546.5462261862822\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1520.4357550869474\n",
      "EP1 EpisodeReward=-1520.3315043380028\n",
      "EP2 EpisodeReward=-1513.0137890188375\n",
      "EP3 EpisodeReward=-1285.5423153418553\n",
      "EP4 EpisodeReward=-1620.7046717259002\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1597.3167231577306\n",
      "EP1 EpisodeReward=-1164.4621411901455\n",
      "EP2 EpisodeReward=-1535.3940563827516\n",
      "EP3 EpisodeReward=-1502.6248113279341\n",
      "EP4 EpisodeReward=-1024.5714992777835\n",
      "Epoch=12\t Average reward=-1397.2741323966554\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1636.1134603214489\n",
      "EP1 EpisodeReward=-1513.5148928188196\n",
      "EP2 EpisodeReward=-1598.7229827458652\n",
      "EP3 EpisodeReward=-1640.6944292843118\n",
      "EP4 EpisodeReward=-1509.385596563419\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1647.7491870888955\n",
      "EP1 EpisodeReward=-1252.3224383642826\n",
      "EP2 EpisodeReward=-988.5883547272438\n",
      "EP3 EpisodeReward=-1630.6484746056176\n",
      "EP4 EpisodeReward=-1653.4453155522162\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1653.587467028166\n",
      "EP1 EpisodeReward=-1376.807684658508\n",
      "EP2 EpisodeReward=-1562.104618798477\n",
      "EP3 EpisodeReward=-1163.9461094814594\n",
      "EP4 EpisodeReward=-1204.1694914584523\n",
      "Epoch=13\t Average reward=-1455.6668011913625\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1507.7018549662303\n",
      "EP1 EpisodeReward=-1624.5153310845956\n",
      "EP2 EpisodeReward=-951.0141519371971\n",
      "EP3 EpisodeReward=-1383.0664319648124\n",
      "EP4 EpisodeReward=-1057.1757724708484\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1624.8315504293657\n",
      "EP1 EpisodeReward=-1507.109899992527\n",
      "EP2 EpisodeReward=-950.104277972733\n",
      "EP3 EpisodeReward=-1450.9122594571538\n",
      "EP4 EpisodeReward=-1362.4449587428662\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1595.9032290228845\n",
      "EP1 EpisodeReward=-1336.1138855834788\n",
      "EP2 EpisodeReward=-1523.2459057958918\n",
      "EP3 EpisodeReward=-1347.1297553724876\n",
      "EP4 EpisodeReward=-1406.51161519443\n",
      "Epoch=14\t Average reward=-1275.377448802715\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1642.4044568493491\n",
      "EP1 EpisodeReward=-1641.0004572500015\n",
      "EP2 EpisodeReward=-1510.6385795836056\n",
      "EP3 EpisodeReward=-1531.5075679974327\n",
      "EP4 EpisodeReward=-1296.1854068774096\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1635.7818841809365\n",
      "EP1 EpisodeReward=-1580.3929902976736\n",
      "EP2 EpisodeReward=-1517.395960034332\n",
      "EP3 EpisodeReward=-1176.3653906935162\n",
      "EP4 EpisodeReward=-1496.6101617205288\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1453.324124453729\n",
      "EP1 EpisodeReward=-1642.4831633277245\n",
      "EP2 EpisodeReward=-1215.9462740936171\n",
      "EP3 EpisodeReward=-1509.0913900048402\n",
      "EP4 EpisodeReward=-1106.3475397569623\n",
      "Epoch=15\t Average reward=-1299.7143694516337\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1645.0766006727185\n",
      "EP1 EpisodeReward=-1072.6032130868637\n",
      "EP2 EpisodeReward=-1336.9293049300277\n",
      "EP3 EpisodeReward=-1356.2110881423655\n",
      "EP4 EpisodeReward=-1518.7347491288185\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1493.592332629219\n",
      "EP1 EpisodeReward=-1265.3951222909657\n",
      "EP2 EpisodeReward=-1562.7889852652986\n",
      "EP3 EpisodeReward=-1362.1626302539953\n",
      "EP4 EpisodeReward=-1577.777388512848\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1572.3687598815961\n",
      "EP1 EpisodeReward=-1500.1545905075207\n",
      "EP2 EpisodeReward=-1165.0198878513563\n",
      "EP3 EpisodeReward=-1254.0594159163109\n",
      "EP4 EpisodeReward=-1085.7112716272607\n",
      "Epoch=16\t Average reward=-1394.0744697563089\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1059.9137433014987\n",
      "EP1 EpisodeReward=-1614.1084529961552\n",
      "EP2 EpisodeReward=-1166.4502636100906\n",
      "EP3 EpisodeReward=-1073.2651240767464\n",
      "EP4 EpisodeReward=-1631.04575326792\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1642.1449707587922\n",
      "EP1 EpisodeReward=-1543.7601788890793\n",
      "EP2 EpisodeReward=-972.9275595871379\n",
      "EP3 EpisodeReward=-1438.4266733607303\n",
      "EP4 EpisodeReward=-1654.611898682401\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-945.4606338495695\n",
      "EP1 EpisodeReward=-1429.812150624216\n",
      "EP2 EpisodeReward=-1478.4572344813675\n",
      "EP3 EpisodeReward=-1372.1200584022872\n",
      "EP4 EpisodeReward=-1417.381458710829\n",
      "Epoch=17\t Average reward=-1567.6797035537165\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1343.355333716831\n",
      "EP1 EpisodeReward=-1601.3999427189103\n",
      "EP2 EpisodeReward=-1542.1213758092408\n",
      "EP3 EpisodeReward=-1330.7048018763717\n",
      "EP4 EpisodeReward=-1641.0818974261008\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1646.70667942247\n",
      "EP1 EpisodeReward=-1535.2053014385283\n",
      "EP2 EpisodeReward=-1525.1044859186613\n",
      "EP3 EpisodeReward=-1593.0338446722435\n",
      "EP4 EpisodeReward=-1210.7642388099785\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1376.2589462706833\n",
      "EP1 EpisodeReward=-950.9832729647918\n",
      "EP2 EpisodeReward=-1510.2833030818626\n",
      "EP3 EpisodeReward=-1782.0851749218432\n",
      "EP4 EpisodeReward=-1748.4405880889612\n",
      "Epoch=18\t Average reward=-1533.4289081083468\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1160.7651114036353\n",
      "EP1 EpisodeReward=-1326.3644273773286\n",
      "EP2 EpisodeReward=-1069.851263746865\n",
      "EP3 EpisodeReward=-1469.0198325865858\n",
      "EP4 EpisodeReward=-1530.3109917114261\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1150.4318243521166\n",
      "EP1 EpisodeReward=-1426.9169938804525\n",
      "EP2 EpisodeReward=-1611.4501841685938\n",
      "EP3 EpisodeReward=-1500.6107429461072\n",
      "EP4 EpisodeReward=-1501.0885237346604\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1598.5510852909147\n",
      "EP1 EpisodeReward=-1440.5197054730938\n",
      "EP2 EpisodeReward=-1445.1099539614129\n",
      "EP3 EpisodeReward=-1388.909621892426\n",
      "EP4 EpisodeReward=-1459.5084289320955\n",
      "Epoch=19\t Average reward=-1496.9693147927273\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1627.665484438719\n",
      "EP1 EpisodeReward=-1589.8819685662068\n",
      "EP2 EpisodeReward=-1525.4421209248637\n",
      "EP3 EpisodeReward=-1247.1993859192035\n",
      "EP4 EpisodeReward=-1459.0288515237612\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1539.2691143797535\n",
      "EP1 EpisodeReward=-1077.6622472068848\n",
      "EP2 EpisodeReward=-1621.5809559381728\n",
      "EP3 EpisodeReward=-1507.6004203656582\n",
      "EP4 EpisodeReward=-1566.8418137906262\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1235.3277137714358\n",
      "EP1 EpisodeReward=-1576.882744100206\n",
      "EP2 EpisodeReward=-1360.407655478944\n",
      "EP3 EpisodeReward=-1474.2518650882644\n",
      "EP4 EpisodeReward=-1790.3613677190856\n",
      "Epoch=20\t Average reward=-1605.4106776778244\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1512.31427249504\n",
      "EP1 EpisodeReward=-1087.8147554193827\n",
      "EP2 EpisodeReward=-1412.1469241846155\n",
      "EP3 EpisodeReward=-1582.1653498346768\n",
      "EP4 EpisodeReward=-1405.466741507631\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1313.099677971099\n",
      "EP1 EpisodeReward=-1585.0420192722931\n",
      "EP2 EpisodeReward=-1634.2771884882231\n",
      "EP3 EpisodeReward=-1110.367408889879\n",
      "EP4 EpisodeReward=-1128.170428307114\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1566.4831569792464\n",
      "EP1 EpisodeReward=-1654.2300666606495\n",
      "EP2 EpisodeReward=-1534.7001305675267\n",
      "EP3 EpisodeReward=-1542.882562378326\n",
      "EP4 EpisodeReward=-1397.8505272531272\n",
      "Epoch=21\t Average reward=-1310.4958990226241\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1051.6493824898255\n",
      "EP1 EpisodeReward=-1632.6683198749581\n",
      "EP2 EpisodeReward=-1131.4596582207785\n",
      "EP3 EpisodeReward=-1541.0929925718128\n",
      "EP4 EpisodeReward=-1597.0564342311382\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1475.985556022803\n",
      "EP1 EpisodeReward=-1199.2263904093365\n",
      "EP2 EpisodeReward=-1516.0777994796874\n",
      "EP3 EpisodeReward=-1380.5666671575204\n",
      "EP4 EpisodeReward=-1632.0337102961955\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1105.630358630736\n",
      "EP1 EpisodeReward=-1180.363672673638\n",
      "EP2 EpisodeReward=-1554.4832366518926\n",
      "EP3 EpisodeReward=-912.1524320941\n",
      "EP4 EpisodeReward=-1188.7651238832461\n",
      "Epoch=22\t Average reward=-1472.6184228035265\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1645.044040423119\n",
      "EP1 EpisodeReward=-1725.9852276675517\n",
      "EP2 EpisodeReward=-1651.2381222501485\n",
      "EP3 EpisodeReward=-1318.398327267094\n",
      "EP4 EpisodeReward=-1504.699741464827\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1534.2799385254104\n",
      "EP1 EpisodeReward=-1813.8109235421462\n",
      "EP2 EpisodeReward=-1590.5293236084233\n",
      "EP3 EpisodeReward=-1165.2410108288182\n",
      "EP4 EpisodeReward=-1416.5541439746112\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1634.6993306438608\n",
      "EP1 EpisodeReward=-1848.8502193318805\n",
      "EP2 EpisodeReward=-1458.2354920734028\n",
      "EP3 EpisodeReward=-1617.9355865413772\n",
      "EP4 EpisodeReward=-1504.9058863383377\n",
      "Epoch=23\t Average reward=-1475.386590592592\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1651.9318206501227\n",
      "EP1 EpisodeReward=-1303.593776873252\n",
      "EP2 EpisodeReward=-1640.0126938213623\n",
      "EP3 EpisodeReward=-1493.8137998723291\n",
      "EP4 EpisodeReward=-1575.6232486303122\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1118.7459839347573\n",
      "EP1 EpisodeReward=-1169.4817538795658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP2 EpisodeReward=-1457.539770366491\n",
      "EP3 EpisodeReward=-1544.1437235476417\n",
      "EP4 EpisodeReward=-1522.2601538524193\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1644.0765460123619\n",
      "EP1 EpisodeReward=-1143.9033774880513\n",
      "EP2 EpisodeReward=-1081.0741535888542\n",
      "EP3 EpisodeReward=-1431.2912028612552\n",
      "EP4 EpisodeReward=-1535.85529158993\n",
      "Epoch=24\t Average reward=-1544.579564690887\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1162.5576326487808\n",
      "EP1 EpisodeReward=-1636.7082323647155\n",
      "EP2 EpisodeReward=-1543.6735933109223\n",
      "EP3 EpisodeReward=-1502.410832918514\n",
      "EP4 EpisodeReward=-1606.392627955628\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1480.437899310554\n",
      "EP1 EpisodeReward=-1638.1205177887273\n",
      "EP2 EpisodeReward=-1618.251161779326\n",
      "EP3 EpisodeReward=-1240.9298408372592\n",
      "EP4 EpisodeReward=-1475.6616271401501\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1247.617323156422\n",
      "EP1 EpisodeReward=-1569.012248031042\n",
      "EP2 EpisodeReward=-1497.5703476702636\n",
      "EP3 EpisodeReward=-1332.9689958725203\n",
      "EP4 EpisodeReward=-1054.01627524138\n",
      "Epoch=25\t Average reward=-1378.6901767790525\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1307.7711932709117\n",
      "EP1 EpisodeReward=-1507.7696412686362\n",
      "EP2 EpisodeReward=-1472.0812546236073\n",
      "EP3 EpisodeReward=-1312.813714353426\n",
      "EP4 EpisodeReward=-1567.1005869802375\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1532.1550869188773\n",
      "EP1 EpisodeReward=-1500.8147359398092\n",
      "EP2 EpisodeReward=-1565.9933937734686\n",
      "EP3 EpisodeReward=-1284.7334219105962\n",
      "EP4 EpisodeReward=-1536.3592915880856\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-929.4523644191412\n",
      "EP1 EpisodeReward=-1236.604863432711\n",
      "EP2 EpisodeReward=-1525.8381230142606\n",
      "EP3 EpisodeReward=-1392.3176509086\n",
      "EP4 EpisodeReward=-1372.9539298758443\n",
      "Epoch=26\t Average reward=-1492.1379361480558\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1722.0161823263284\n",
      "EP1 EpisodeReward=-1477.562100983885\n",
      "EP2 EpisodeReward=-1578.5997020097864\n",
      "EP3 EpisodeReward=-1545.450529170003\n",
      "EP4 EpisodeReward=-1500.9181834839378\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1561.8686394449549\n",
      "EP1 EpisodeReward=-1378.74916066356\n",
      "EP2 EpisodeReward=-1452.2460042805317\n",
      "EP3 EpisodeReward=-954.1571144900083\n",
      "EP4 EpisodeReward=-1568.40063333753\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1199.2652634413155\n",
      "EP1 EpisodeReward=-1592.5122856689825\n",
      "EP2 EpisodeReward=-1721.7903005023318\n",
      "EP3 EpisodeReward=-1523.4697656539017\n",
      "EP4 EpisodeReward=-1777.6425380638027\n",
      "Epoch=27\t Average reward=-1615.6537849617569\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1518.239022704638\n",
      "EP1 EpisodeReward=-1496.5075917194922\n",
      "EP2 EpisodeReward=-1225.5136064631456\n",
      "EP3 EpisodeReward=-1286.4772924449537\n",
      "EP4 EpisodeReward=-1336.738484641257\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1617.0644429960125\n",
      "EP1 EpisodeReward=-1554.95316476303\n",
      "EP2 EpisodeReward=-1515.4128539359879\n",
      "EP3 EpisodeReward=-1515.4712072866596\n",
      "EP4 EpisodeReward=-1239.5469356118674\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1505.736625011062\n",
      "EP1 EpisodeReward=-1402.7083678523945\n",
      "EP2 EpisodeReward=-1278.6216419293705\n",
      "EP3 EpisodeReward=-1524.5813802704881\n",
      "EP4 EpisodeReward=-1598.3924861890127\n",
      "Epoch=28\t Average reward=-1391.559302147379\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1203.3927116490834\n",
      "EP1 EpisodeReward=-1573.0656142294363\n",
      "EP2 EpisodeReward=-1341.9905318447152\n",
      "EP3 EpisodeReward=-1437.9285498215288\n",
      "EP4 EpisodeReward=-1154.071072523429\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1227.9142199847101\n",
      "EP1 EpisodeReward=-1541.76723044734\n",
      "EP2 EpisodeReward=-1475.721946074775\n",
      "EP3 EpisodeReward=-1259.50780788931\n",
      "EP4 EpisodeReward=-1216.6679789680336\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1269.051264375423\n",
      "EP1 EpisodeReward=-1191.37169009521\n",
      "EP2 EpisodeReward=-1561.8501567452308\n",
      "EP3 EpisodeReward=-1698.8814718084582\n",
      "EP4 EpisodeReward=-1542.6563465080094\n",
      "Epoch=29\t Average reward=-1304.4651326664905\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1374.4683629093583\n",
      "EP1 EpisodeReward=-1400.158530359107\n",
      "EP2 EpisodeReward=-1493.2692091189906\n",
      "EP3 EpisodeReward=-1177.892928446956\n",
      "EP4 EpisodeReward=-1400.449373640329\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1463.9973579604716\n",
      "EP1 EpisodeReward=-1311.5818598959738\n",
      "EP2 EpisodeReward=-1288.681460510419\n",
      "EP3 EpisodeReward=-1063.5934494880828\n",
      "EP4 EpisodeReward=-1361.1961783413171\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1260.8604998067337\n",
      "EP1 EpisodeReward=-1276.6664137196892\n",
      "EP2 EpisodeReward=-1079.3393441449023\n",
      "EP3 EpisodeReward=-1509.7385748986828\n",
      "EP4 EpisodeReward=-678.9898368592405\n",
      "Epoch=30\t Average reward=-1146.8784629469621\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1565.2884136082473\n",
      "EP1 EpisodeReward=-1517.4205453101392\n",
      "EP2 EpisodeReward=-1606.0469770439822\n",
      "EP3 EpisodeReward=-1622.7358313897814\n",
      "EP4 EpisodeReward=-1146.6646740588683\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1521.295890846542\n",
      "EP1 EpisodeReward=-1392.9451158576958\n",
      "EP2 EpisodeReward=-1338.7932924871498\n",
      "EP3 EpisodeReward=-1295.9204492840024\n",
      "EP4 EpisodeReward=-1517.928443440601\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1660.1682189929224\n",
      "EP1 EpisodeReward=-1751.007421644624\n",
      "EP2 EpisodeReward=-1638.094895166838\n",
      "EP3 EpisodeReward=-1643.8033247076894\n",
      "EP4 EpisodeReward=-1813.6565746793997\n",
      "Epoch=31\t Average reward=-1492.7498973929562\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1654.7739254696776\n",
      "EP1 EpisodeReward=-1262.198455089224\n",
      "EP2 EpisodeReward=-1576.6350199685482\n",
      "EP3 EpisodeReward=-1310.8981888821165\n",
      "EP4 EpisodeReward=-1583.8845542200756\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1554.117680872672\n",
      "EP1 EpisodeReward=-1289.7015890024727\n",
      "EP2 EpisodeReward=-1264.3228662701645\n",
      "EP3 EpisodeReward=-1598.7669578597151\n",
      "EP4 EpisodeReward=-1173.237016772139\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1533.4948938090042\n",
      "EP1 EpisodeReward=-998.9774810246998\n",
      "EP2 EpisodeReward=-1357.5655856654594\n",
      "EP3 EpisodeReward=-1615.898976546549\n",
      "EP4 EpisodeReward=-1454.2793708926058\n",
      "Epoch=32\t Average reward=-1403.8003139616067\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1525.0079749478768\n",
      "EP1 EpisodeReward=-1061.4377750339752\n",
      "EP2 EpisodeReward=-1205.5751934725154\n",
      "EP3 EpisodeReward=-1493.0468442377614\n",
      "EP4 EpisodeReward=-1320.0360303354719\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1584.0269317279005\n",
      "EP1 EpisodeReward=-1485.277410914759\n",
      "EP2 EpisodeReward=-1309.9348766329122\n",
      "EP3 EpisodeReward=-1028.7917777803482\n",
      "EP4 EpisodeReward=-1093.081328421765\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1440.8573487821009\n",
      "EP1 EpisodeReward=-1492.2612293169307\n",
      "EP2 EpisodeReward=-1400.7447594624764\n",
      "EP3 EpisodeReward=-1531.694778265183\n",
      "EP4 EpisodeReward=-1689.9183705489593\n",
      "Epoch=33\t Average reward=-1367.6785764353988\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1465.3274951061635\n",
      "EP1 EpisodeReward=-1382.8930617694843\n",
      "EP2 EpisodeReward=-1205.7746302091377\n",
      "EP3 EpisodeReward=-1570.9336495884304\n",
      "EP4 EpisodeReward=-1301.234959189621\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1587.7532138090485\n",
      "EP1 EpisodeReward=-1374.5283956757864\n",
      "EP2 EpisodeReward=-1582.4071980774206\n",
      "EP3 EpisodeReward=-1274.7178918965753\n",
      "EP4 EpisodeReward=-1513.8263784783871\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1450.7030856530434\n",
      "EP1 EpisodeReward=-1083.4071418418\n",
      "EP2 EpisodeReward=-1218.345061474778\n",
      "EP3 EpisodeReward=-1157.6577998641978\n",
      "EP4 EpisodeReward=-1739.7512351293435\n",
      "Epoch=34\t Average reward=-1518.270857599117\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1260.012787208432\n",
      "EP1 EpisodeReward=-1352.6439073221836\n",
      "EP2 EpisodeReward=-1417.3628352602755\n",
      "EP3 EpisodeReward=-1462.6824887337527\n",
      "EP4 EpisodeReward=-1189.4718526368918\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1056.776361123234\n",
      "EP1 EpisodeReward=-1430.5411427641175\n",
      "EP2 EpisodeReward=-1235.162288695749\n",
      "EP3 EpisodeReward=-1382.6714460493802\n",
      "EP4 EpisodeReward=-1469.061225765422\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1438.870404933829\n",
      "EP1 EpisodeReward=-1540.395657963124\n",
      "EP2 EpisodeReward=-1163.3498901923888\n",
      "EP3 EpisodeReward=-1489.9954880621626\n",
      "EP4 EpisodeReward=-1180.926670448612\n",
      "Epoch=35\t Average reward=-1279.8199162836418\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1630.6778164945813\n",
      "EP1 EpisodeReward=-1583.8427583635885\n",
      "EP2 EpisodeReward=-1666.9951626532684\n",
      "EP3 EpisodeReward=-1522.1279822325769\n",
      "EP4 EpisodeReward=-1187.85914232059\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1609.4063266604144\n",
      "EP1 EpisodeReward=-1607.3785406841516\n",
      "EP2 EpisodeReward=-1655.929469586183\n",
      "EP3 EpisodeReward=-1216.7022930417133\n",
      "EP4 EpisodeReward=-1512.2929772829543\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1497.2001691392923\n",
      "EP1 EpisodeReward=-1544.9153596366905\n",
      "EP2 EpisodeReward=-1480.01422501853\n",
      "EP3 EpisodeReward=-1329.5150589340406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP4 EpisodeReward=-1546.0963073635246\n",
      "Epoch=36\t Average reward=-1415.4161423223566\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1323.7475269489846\n",
      "EP1 EpisodeReward=-1495.414528712887\n",
      "EP2 EpisodeReward=-1066.1798189898502\n",
      "EP3 EpisodeReward=-1102.7273017098519\n",
      "EP4 EpisodeReward=-1341.6664625550263\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1193.886217419976\n",
      "EP1 EpisodeReward=-1511.4043121086866\n",
      "EP2 EpisodeReward=-1301.994129042693\n",
      "EP3 EpisodeReward=-1138.2178991047235\n",
      "EP4 EpisodeReward=-1330.6152872376988\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1307.8407776758959\n",
      "EP1 EpisodeReward=-1312.1155472439195\n",
      "EP2 EpisodeReward=-1376.3257056415691\n",
      "EP3 EpisodeReward=-1616.2352273787785\n",
      "EP4 EpisodeReward=-1284.7701738712162\n",
      "Epoch=37\t Average reward=-1319.0173078879805\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1347.9466028045172\n",
      "EP1 EpisodeReward=-960.9029473485954\n",
      "EP2 EpisodeReward=-1116.3461375799952\n",
      "EP3 EpisodeReward=-1484.6929043358045\n",
      "EP4 EpisodeReward=-1457.6247115714023\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1517.5457388741202\n",
      "EP1 EpisodeReward=-1685.9103770570457\n",
      "EP2 EpisodeReward=-1430.1990154846449\n",
      "EP3 EpisodeReward=-1516.1396605804823\n",
      "EP4 EpisodeReward=-1481.5941493223245\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1610.882674811101\n",
      "EP1 EpisodeReward=-1481.2286814113268\n",
      "EP2 EpisodeReward=-1446.6340188830043\n",
      "EP3 EpisodeReward=-1654.0762409305762\n",
      "EP4 EpisodeReward=-1664.9573604712905\n",
      "Epoch=38\t Average reward=-1534.7254071216723\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1520.7376692244134\n",
      "EP1 EpisodeReward=-1438.2511202827914\n",
      "EP2 EpisodeReward=-1533.938557571526\n",
      "EP3 EpisodeReward=-1458.314489703159\n",
      "EP4 EpisodeReward=-1413.1330456734336\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1401.2453002034865\n",
      "EP1 EpisodeReward=-1374.1682774777728\n",
      "EP2 EpisodeReward=-1438.2649469833357\n",
      "EP3 EpisodeReward=-1356.1786200171093\n",
      "EP4 EpisodeReward=-1394.2008904896704\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1235.394467996776\n",
      "EP1 EpisodeReward=-1343.1889389151531\n",
      "EP2 EpisodeReward=-1174.8723496733373\n",
      "EP3 EpisodeReward=-1467.096835372388\n",
      "EP4 EpisodeReward=-1471.6572081383629\n",
      "Epoch=39\t Average reward=-1426.3303814338224\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1546.4364805942798\n",
      "EP1 EpisodeReward=-1231.3100706089524\n",
      "EP2 EpisodeReward=-1418.4103305489966\n",
      "EP3 EpisodeReward=-1357.617894783314\n",
      "EP4 EpisodeReward=-1203.8503122300453\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1310.0440976439154\n",
      "EP1 EpisodeReward=-1474.4659611395234\n",
      "EP2 EpisodeReward=-1282.443908114541\n",
      "EP3 EpisodeReward=-1340.5047780867117\n",
      "EP4 EpisodeReward=-1571.9226225067678\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1463.2913603116826\n",
      "EP1 EpisodeReward=-1221.5180933314236\n",
      "EP2 EpisodeReward=-1491.730248681372\n",
      "EP3 EpisodeReward=-1038.0554676337817\n",
      "EP4 EpisodeReward=-1536.0467726384427\n",
      "Epoch=40\t Average reward=-1437.273235791752\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1240.3145383541907\n",
      "EP1 EpisodeReward=-1040.9185425680107\n",
      "EP2 EpisodeReward=-1144.0333745568382\n",
      "EP3 EpisodeReward=-1571.5567095913912\n",
      "EP4 EpisodeReward=-1341.5257438005142\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1299.702647223049\n",
      "EP1 EpisodeReward=-1480.3053182781014\n",
      "EP2 EpisodeReward=-1013.4512559745444\n",
      "EP3 EpisodeReward=-1322.412707341952\n",
      "EP4 EpisodeReward=-1212.0679979789843\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1326.6329776883153\n",
      "EP1 EpisodeReward=-1192.854138864695\n",
      "EP2 EpisodeReward=-1386.2885151661137\n",
      "EP3 EpisodeReward=-1057.1156453204474\n",
      "EP4 EpisodeReward=-1263.5756702395681\n",
      "Epoch=41\t Average reward=-1272.3898040063557\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1467.649027624443\n",
      "EP1 EpisodeReward=-1300.832653578425\n",
      "EP2 EpisodeReward=-1227.2439454130254\n",
      "EP3 EpisodeReward=-1187.477411319415\n",
      "EP4 EpisodeReward=-1199.9992600921191\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1324.6094212098049\n",
      "EP1 EpisodeReward=-1199.622863064254\n",
      "EP2 EpisodeReward=-1146.5912559028643\n",
      "EP3 EpisodeReward=-1335.3368632939453\n",
      "EP4 EpisodeReward=-1333.3285225567556\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1323.4402344310618\n",
      "EP1 EpisodeReward=-1341.7368269226472\n",
      "EP2 EpisodeReward=-1221.2459582825527\n",
      "EP3 EpisodeReward=-1238.1772420255168\n",
      "EP4 EpisodeReward=-1446.5076142777011\n",
      "Epoch=42\t Average reward=-1326.6117989755253\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1561.0484539710963\n",
      "EP1 EpisodeReward=-1355.9892377609967\n",
      "EP2 EpisodeReward=-1475.9087802166553\n",
      "EP3 EpisodeReward=-1543.3361033317851\n",
      "EP4 EpisodeReward=-1320.785711695719\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1333.0001825123793\n",
      "EP1 EpisodeReward=-1127.3075743296074\n",
      "EP2 EpisodeReward=-1303.312964584227\n",
      "EP3 EpisodeReward=-1327.0725062279673\n",
      "EP4 EpisodeReward=-1184.6867298258167\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1113.1722467848251\n",
      "EP1 EpisodeReward=-1270.4804357684734\n",
      "EP2 EpisodeReward=-1073.9282096036202\n",
      "EP3 EpisodeReward=-1073.2996588768112\n",
      "EP4 EpisodeReward=-1341.0003122069184\n",
      "Epoch=43\t Average reward=-1282.1575845761515\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1342.7400976179847\n",
      "EP1 EpisodeReward=-1128.057690378653\n",
      "EP2 EpisodeReward=-1229.2756371785017\n",
      "EP3 EpisodeReward=-1323.7931150926904\n",
      "EP4 EpisodeReward=-1390.8049719295495\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1117.7589450004205\n",
      "EP1 EpisodeReward=-1478.5415986442313\n",
      "EP2 EpisodeReward=-1413.6990364408498\n",
      "EP3 EpisodeReward=-1298.2232807144364\n",
      "EP4 EpisodeReward=-1261.1427077282603\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1470.6318667363914\n",
      "EP1 EpisodeReward=-1450.134046010663\n",
      "EP2 EpisodeReward=-1023.4205921628491\n",
      "EP3 EpisodeReward=-1354.455439664625\n",
      "EP4 EpisodeReward=-1508.9081201995527\n",
      "Epoch=44\t Average reward=-1386.9519332857878\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1040.4876892684324\n",
      "EP1 EpisodeReward=-1543.9178180732633\n",
      "EP2 EpisodeReward=-1205.5317369642778\n",
      "EP3 EpisodeReward=-1115.9695910168814\n",
      "EP4 EpisodeReward=-1238.2960051219786\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1298.8268306233485\n",
      "EP1 EpisodeReward=-1073.9147002494008\n",
      "EP2 EpisodeReward=-1008.4717535770835\n",
      "EP3 EpisodeReward=-1203.3393337155442\n",
      "EP4 EpisodeReward=-1244.451118098096\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1184.066298785415\n",
      "EP1 EpisodeReward=-1459.7048688586883\n",
      "EP2 EpisodeReward=-1430.6166258697979\n",
      "EP3 EpisodeReward=-1337.9052148783155\n",
      "EP4 EpisodeReward=-1093.8923935999364\n",
      "Epoch=45\t Average reward=-1192.213172273337\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1585.820093533191\n",
      "EP1 EpisodeReward=-1249.0153736994973\n",
      "EP2 EpisodeReward=-1234.3485873695074\n",
      "EP3 EpisodeReward=-1226.8983485014433\n",
      "EP4 EpisodeReward=-1428.4406189794379\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1490.7354219840943\n",
      "EP1 EpisodeReward=-1524.5585159008942\n",
      "EP2 EpisodeReward=-974.3507846200871\n",
      "EP3 EpisodeReward=-1206.5173326451945\n",
      "EP4 EpisodeReward=-1181.8323225057286\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1480.6417700633212\n",
      "EP1 EpisodeReward=-1541.885411859127\n",
      "EP2 EpisodeReward=-1316.2128815191268\n",
      "EP3 EpisodeReward=-1354.419711004669\n",
      "EP4 EpisodeReward=-1552.4803930591984\n",
      "Epoch=46\t Average reward=-1387.5844448481214\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-964.9133819968717\n",
      "EP1 EpisodeReward=-1288.6670563253886\n",
      "EP2 EpisodeReward=-1405.275700932467\n",
      "EP3 EpisodeReward=-1468.1103056914715\n",
      "EP4 EpisodeReward=-1256.6083475112425\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-981.0273349505065\n",
      "EP1 EpisodeReward=-1529.6893352166906\n",
      "EP2 EpisodeReward=-1512.5307879077034\n",
      "EP3 EpisodeReward=-1232.8705663898281\n",
      "EP4 EpisodeReward=-855.1624894800065\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1464.5183318122026\n",
      "EP1 EpisodeReward=-1205.2890265853991\n",
      "EP2 EpisodeReward=-1515.3842578234046\n",
      "EP3 EpisodeReward=-1586.1472849104084\n",
      "EP4 EpisodeReward=-1232.5004392186804\n",
      "Epoch=47\t Average reward=-1114.7570920699764\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1223.0814549759918\n",
      "EP1 EpisodeReward=-1245.4010357526906\n",
      "EP2 EpisodeReward=-1311.63952376273\n",
      "EP3 EpisodeReward=-1189.6848101701157\n",
      "EP4 EpisodeReward=-1070.9240070078554\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1488.4681412846767\n",
      "EP1 EpisodeReward=-1560.5200872074865\n",
      "EP2 EpisodeReward=-882.6509761199616\n",
      "EP3 EpisodeReward=-1071.845242030593\n",
      "EP4 EpisodeReward=-772.3922218731581\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1505.1131702426305\n",
      "EP1 EpisodeReward=-1339.1406236891953\n",
      "EP2 EpisodeReward=-1223.4916548988017\n",
      "EP3 EpisodeReward=-1382.7992113735322\n",
      "EP4 EpisodeReward=-1370.5761828666275\n",
      "Epoch=48\t Average reward=-1071.297470582547\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1064.8662287009286\n",
      "EP1 EpisodeReward=-1034.5534031453492\n",
      "EP2 EpisodeReward=-1021.7503264012612\n",
      "EP3 EpisodeReward=-1458.9293387371465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP4 EpisodeReward=-856.2618341560701\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-866.196619364701\n",
      "EP1 EpisodeReward=-1443.9921527646718\n",
      "EP2 EpisodeReward=-1184.211280986331\n",
      "EP3 EpisodeReward=-1109.9000565910503\n",
      "EP4 EpisodeReward=-1476.8121216095033\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1210.3204878177148\n",
      "EP1 EpisodeReward=-966.5809981658992\n",
      "EP2 EpisodeReward=-1162.1354511986715\n",
      "EP3 EpisodeReward=-1108.088371590336\n",
      "EP4 EpisodeReward=-1046.3312261459957\n",
      "Epoch=49\t Average reward=-1126.468393970523\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1128.6750746964565\n",
      "EP1 EpisodeReward=-1120.2697621777186\n",
      "EP2 EpisodeReward=-1478.3392323928101\n",
      "EP3 EpisodeReward=-1537.3477460563993\n",
      "EP4 EpisodeReward=-703.7110085614612\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1119.826868058274\n",
      "EP1 EpisodeReward=-1240.2578880039473\n",
      "EP2 EpisodeReward=-1527.7990592158253\n",
      "EP3 EpisodeReward=-954.4306505687496\n",
      "EP4 EpisodeReward=-1091.9572520693355\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1568.9716371995523\n",
      "EP1 EpisodeReward=-1177.6997169796414\n",
      "EP2 EpisodeReward=-1228.563858351078\n",
      "EP3 EpisodeReward=-1245.82532012387\n",
      "EP4 EpisodeReward=-1275.4835221933668\n",
      "Epoch=50\t Average reward=-1023.7172609413878\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1509.008962445922\n",
      "EP1 EpisodeReward=-1023.5120249190246\n",
      "EP2 EpisodeReward=-887.486587434762\n",
      "EP3 EpisodeReward=-1094.4074079380496\n",
      "EP4 EpisodeReward=-1012.4787841665187\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1065.7471503254735\n",
      "EP1 EpisodeReward=-1004.2353444835439\n",
      "EP2 EpisodeReward=-1499.4507604745324\n",
      "EP3 EpisodeReward=-1541.6219620398483\n",
      "EP4 EpisodeReward=-739.9489101836427\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-888.6798505476952\n",
      "EP1 EpisodeReward=-869.6244054394929\n",
      "EP2 EpisodeReward=-1377.0744970795452\n",
      "EP3 EpisodeReward=-1141.2787638573195\n",
      "EP4 EpisodeReward=-860.0518956367938\n",
      "Epoch=51\t Average reward=-870.8265299956516\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1032.6385752843717\n",
      "EP1 EpisodeReward=-757.9511483114441\n",
      "EP2 EpisodeReward=-1436.5142206426726\n",
      "EP3 EpisodeReward=-988.5863311916848\n",
      "EP4 EpisodeReward=-1543.9969349417197\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-786.1539409607017\n",
      "EP1 EpisodeReward=-1082.690517061956\n",
      "EP2 EpisodeReward=-898.0955857600768\n",
      "EP3 EpisodeReward=-1117.630740567038\n",
      "EP4 EpisodeReward=-964.223438189311\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-735.2173652528425\n",
      "EP1 EpisodeReward=-1523.4788174321748\n",
      "EP2 EpisodeReward=-857.2004170358772\n",
      "EP3 EpisodeReward=-881.2271721411047\n",
      "EP4 EpisodeReward=-1121.1754121228246\n",
      "Epoch=52\t Average reward=-1209.7985950846185\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1417.6675984165086\n",
      "EP1 EpisodeReward=-1392.4706260902062\n",
      "EP2 EpisodeReward=-874.2191298700607\n",
      "EP3 EpisodeReward=-1253.931614792388\n",
      "EP4 EpisodeReward=-1369.2704079385062\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1514.8230815904333\n",
      "EP1 EpisodeReward=-947.2929518724173\n",
      "EP2 EpisodeReward=-1133.1097649694918\n",
      "EP3 EpisodeReward=-1097.4289427634203\n",
      "EP4 EpisodeReward=-858.4757272065204\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1200.412864171623\n",
      "EP1 EpisodeReward=-1580.654820717371\n",
      "EP2 EpisodeReward=-1174.4301381878568\n",
      "EP3 EpisodeReward=-1514.3583245902046\n",
      "EP4 EpisodeReward=-580.7231751585515\n",
      "Epoch=53\t Average reward=-936.1564367678593\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1108.16485023443\n",
      "EP1 EpisodeReward=-1463.9173666830268\n",
      "EP2 EpisodeReward=-1583.2858845361834\n",
      "EP3 EpisodeReward=-1326.5582555808408\n",
      "EP4 EpisodeReward=-1210.655746418772\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1140.4543750489472\n",
      "EP1 EpisodeReward=-1302.0104327825336\n",
      "EP2 EpisodeReward=-1499.7641406986988\n",
      "EP3 EpisodeReward=-1267.4921600964258\n",
      "EP4 EpisodeReward=-1402.324303753884\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1353.0139397575408\n",
      "EP1 EpisodeReward=-1112.2838472684857\n",
      "EP2 EpisodeReward=-1015.4762114860303\n",
      "EP3 EpisodeReward=-1382.8491700609711\n",
      "EP4 EpisodeReward=-1346.627889890152\n",
      "Epoch=54\t Average reward=-1319.8693133542693\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1244.7086524597241\n",
      "EP1 EpisodeReward=-1522.0409863407024\n",
      "EP2 EpisodeReward=-1181.3563079850712\n",
      "EP3 EpisodeReward=-952.2306067436756\n",
      "EP4 EpisodeReward=-888.7747028576597\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1294.0533040297828\n",
      "EP1 EpisodeReward=-1101.04497667422\n",
      "EP2 EpisodeReward=-1508.638640662969\n",
      "EP3 EpisodeReward=-1066.1654541357605\n",
      "EP4 EpisodeReward=-1107.514446135468\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1532.8427499084098\n",
      "EP1 EpisodeReward=-1031.25001862183\n",
      "EP2 EpisodeReward=-1199.7041820958564\n",
      "EP3 EpisodeReward=-1300.5752085960253\n",
      "EP4 EpisodeReward=-1165.3201232343874\n",
      "Epoch=55\t Average reward=-1053.8697574091718\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-913.906258055573\n",
      "EP1 EpisodeReward=-1619.7375439690134\n",
      "EP2 EpisodeReward=-901.2730801924172\n",
      "EP3 EpisodeReward=-887.7285690094733\n",
      "EP4 EpisodeReward=-954.6745454103891\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-877.5740821426601\n",
      "EP1 EpisodeReward=-1006.2115541250396\n",
      "EP2 EpisodeReward=-1526.0540137467144\n",
      "EP3 EpisodeReward=-1198.1155076769246\n",
      "EP4 EpisodeReward=-759.4446586661359\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-890.4820262679327\n",
      "EP1 EpisodeReward=-851.7714443531131\n",
      "EP2 EpisodeReward=-981.5524878640902\n",
      "EP3 EpisodeReward=-1017.3621138896523\n",
      "EP4 EpisodeReward=-1583.4970765425514\n",
      "Epoch=56\t Average reward=-1099.2054268730255\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-769.3397110017547\n",
      "EP1 EpisodeReward=-1176.5893056148477\n",
      "EP2 EpisodeReward=-879.3729912388302\n",
      "EP3 EpisodeReward=-753.1784623673032\n",
      "EP4 EpisodeReward=-1419.9273477102743\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1436.9480288767757\n",
      "EP1 EpisodeReward=-1530.2596612597529\n",
      "EP2 EpisodeReward=-1500.9957015481627\n",
      "EP3 EpisodeReward=-1527.362050677124\n",
      "EP4 EpisodeReward=-1444.7726569422334\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1529.1493782313876\n",
      "EP1 EpisodeReward=-1116.3421402723811\n",
      "EP2 EpisodeReward=-1091.6247714048216\n",
      "EP3 EpisodeReward=-878.9381275545455\n",
      "EP4 EpisodeReward=-1297.346696092778\n",
      "Epoch=57\t Average reward=-1387.3489002484287\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1521.1162175296197\n",
      "EP1 EpisodeReward=-1288.9478028835067\n",
      "EP2 EpisodeReward=-1064.6458122927863\n",
      "EP3 EpisodeReward=-1410.4755742201853\n",
      "EP4 EpisodeReward=-1586.921486567764\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1409.205975588516\n",
      "EP1 EpisodeReward=-976.9126761184399\n",
      "EP2 EpisodeReward=-1435.1063655782204\n",
      "EP3 EpisodeReward=-1369.7566509790279\n",
      "EP4 EpisodeReward=-1375.201218559434\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1448.6319983390215\n",
      "EP1 EpisodeReward=-1100.0155430185757\n",
      "EP2 EpisodeReward=-1302.509048088623\n",
      "EP3 EpisodeReward=-1660.0269697417725\n",
      "EP4 EpisodeReward=-1215.9115510896079\n",
      "Epoch=58\t Average reward=-1392.6780854056021\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1326.357360621537\n",
      "EP1 EpisodeReward=-1081.571323376578\n",
      "EP2 EpisodeReward=-1576.6853180870464\n",
      "EP3 EpisodeReward=-971.2106595249435\n",
      "EP4 EpisodeReward=-1364.5358230098182\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1327.1136247001564\n",
      "EP1 EpisodeReward=-1613.0821809496754\n",
      "EP2 EpisodeReward=-1295.9395339535438\n",
      "EP3 EpisodeReward=-1115.5999055239495\n"
     ]
    }
   ],
   "source": [
    "# def main():\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    try: wandb.finish()\n",
    "    except: pass\n",
    "    \n",
    "    ####configurations\n",
    "    wandb.init(name='DDPG-multiplemax-long', project=\"deep-rl-tf2\")\n",
    "    env_name = 'Pendulum-v0'\n",
    "\n",
    "    \n",
    "    wandb.config.gamma = 0.99\n",
    "    wandb.config.actor_lr = 0.001\n",
    "    wandb.config.critic_lr = 0.0001\n",
    "    wandb.config.batch_size = 64\n",
    "    wandb.config.tau = 0.005\n",
    "    wandb.config.train_start = 400\n",
    "    wandb.config.episodes = 5\n",
    "    wandb.config.num = 3\n",
    "    wandb.config.epochs = 200\n",
    "\n",
    "    wandb.config.actor = {'layer1': 128, 'layer2' : 128}\n",
    "    wandb.config.critic = {'state1': 256, 'state2': 128, 'actor1': 128, 'cat1': 64}\n",
    "    \n",
    "    print(wandb.config)\n",
    "    \n",
    "    # main run    \n",
    "    N = wandb.config.num\n",
    "    agents = []\n",
    "    \n",
    "    # set up the agent\n",
    "    for i in range(N):\n",
    "        env_t = gym.make(env_name)\n",
    "        agents.append(Agent(env_t, i))\n",
    "\n",
    "    # start the training\n",
    "    for z in range(wandb.config.epochs):\n",
    "\n",
    "        rewards = []\n",
    "        reward = 0\n",
    "        # train the agent\n",
    "        for j in range(len(agents)):\n",
    "            print('Training Agent {}'.format(agents[j].iden))\n",
    "            rewards.append(agents[j].train(wandb.config.episodes))\n",
    "            reward += rewards[-1]\n",
    "    \n",
    "        reward = reward / N\n",
    "        print('Epoch={}\\t Average reward={}'.format(z, reward))\n",
    "        wandb.log({'batch': z, 'Epoch': reward})\n",
    "\n",
    "        index = np.argmax(np.array(rewards))\n",
    "        \n",
    "        # get the average - actor and critic\n",
    "        critic_avg = []\n",
    "        actor_avg = []\n",
    "\n",
    "        for i in range(len(agents[index].actor.model.get_weights())):\n",
    "            \n",
    "#             actor_t = agents[0].actor.model.get_weights()[i]\n",
    "\n",
    "#             for j in range(1, N):\n",
    "#                 actor_t += agents[j].actor.model.get_weights()[i]\n",
    "\n",
    "#             actor_t = actor_t / N\n",
    "            actor_t = agents[index].actor.model.get_weights()[i]\n",
    "            actor_avg.append(actor_t)\n",
    "\n",
    "\n",
    "        for i in range(len(agents[index].critic.model.get_weights())):\n",
    "#             critic_t = agents[0].critic.model.get_weights()[i]\n",
    "\n",
    "#             for j in range(1, N):\n",
    "#                 critic_t += agents[j].critic.model.get_weights()[i]\n",
    "\n",
    "#             critic_t = critic_t / N\n",
    "            critic_t = agents[index].critic.model.get_weights()[i]\n",
    "            critic_avg.append(critic_t)\n",
    "\n",
    "\n",
    "        # set the average\n",
    "        for j in range(N):\n",
    "            agents[j].actor.model.set_weights(actor_avg)\n",
    "            agents[j].critic.model.set_weights(critic_avg)\n",
    "\n",
    "\n",
    "#     for j in range(N):\n",
    "#         print(\"Agent {}, actor {}\".format(j, agents[j].actor.model.get_weights()))\n",
    "#         print(\"------------------------\")\n",
    "#         print(\"Agent {}, critic {}\".format(j, agents[j].critic.model.get_weights()))\n",
    "#         print(\"------------------------\")\n",
    "\n",
    "    \n",
    "    wandb.finish()\n",
    "    \n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas\n",
    "wandb.finish()\n",
    "api = wandb.Api()\n",
    "run = api.run(\"victor-qin/deep-rl-tf2/1s1ac3wo\")\n",
    "temp = run.history()\n",
    "# print(run.scan_history())\n",
    "# enumerate(run.history())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204 \t -1397.4979261457054\n",
      "411 \t -1314.97677473863\n",
      "616 \t -1675.7039031101483\n",
      "819 \t -1728.7964805138092\n",
      "1026 \t -1527.2705949310778\n",
      "1237 \t -1578.1663064042016\n",
      "1447 \t -1562.6616137080212\n",
      "1656 \t -1570.4740155175275\n",
      "1866 \t -1766.325876451435\n",
      "2070 \t -1645.5299593648642\n",
      "2285 \t -1418.3971006402744\n",
      "2502 \t -1555.6624956007024\n",
      "2718 \t -1214.3700774544943\n",
      "2932 \t -1561.5437283128983\n",
      "3144 \t -1403.9980968619263\n",
      "3407 \t -1367.411475604592\n",
      "3623 \t -1393.962258191116\n",
      "3830 \t -1406.6523690831484\n",
      "4029 \t -1101.6098018066943\n",
      "4231 \t -1269.6366791527905\n",
      "4449 \t -1498.5555164487466\n",
      "4673 \t -1298.3502109839903\n",
      "4894 \t -1638.4867399099965\n",
      "5118 \t -1412.7765626408752\n",
      "5334 \t -994.9669943700595\n",
      "5534 \t -1320.8340664813616\n",
      "5742 \t -1543.0258378183478\n",
      "5951 \t -1445.0463899960896\n",
      "6160 \t -1340.442215837614\n",
      "6363 \t -1392.9779538323928\n",
      "6585 \t -1430.2436505694286\n",
      "6800 \t -1292.2180494001896\n",
      "7001 \t -1243.6286137615236\n",
      "7201 \t -1505.2007945599473\n",
      "7407 \t -1393.522384897824\n",
      "7618 \t -1316.374957483535\n",
      "7821 \t -1227.821696099155\n",
      "8018 \t -1256.3030727925864\n",
      "8233 \t -1475.9687666689742\n",
      "8458 \t -1448.129700424585\n",
      "8691 \t -1589.2101814466962\n",
      "8917 \t -1554.4246337220395\n",
      "9138 \t -1354.0511439462396\n",
      "9368 \t -1483.7708450767216\n",
      "9585 \t -1552.8099394424605\n",
      "9792 \t -1590.763048342363\n",
      "9997 \t -1563.410586560709\n",
      "10202 \t -1571.663311560197\n",
      "10412 \t -1524.646201417664\n",
      "10619 \t -1565.1473699133305\n",
      "10825 \t -1430.25103398209\n",
      "11036 \t -1430.898161358348\n",
      "11246 \t -1567.7661947933484\n",
      "11458 \t -1439.7195566953417\n",
      "11670 \t -1367.7280438567038\n",
      "11882 \t -1233.4157188561635\n",
      "12095 \t -1313.3987942214123\n",
      "12301 \t -988.6851924529747\n",
      "12511 \t -1322.787519071342\n",
      "12724 \t -1142.0564715300918\n",
      "12930 \t -1404.7334101295576\n",
      "13125 \t -1563.9482078835965\n",
      "13336 \t -1317.679937362023\n",
      "13547 \t -1652.7577969825845\n",
      "13751 \t -1367.7524524357098\n",
      "13964 \t -1438.2750032889073\n",
      "14166 \t -1332.6479814298384\n",
      "14375 \t -1314.5392830365247\n",
      "14578 \t -1403.5709815928083\n",
      "14793 \t -1291.5085919747962\n",
      "15006 \t -1352.0295432585706\n",
      "15213 \t -1438.7699369651937\n",
      "15426 \t -1499.1296663572755\n",
      "15641 \t -1496.3781862511057\n",
      "15848 \t -1371.3112961218956\n",
      "16053 \t -1301.509713760541\n",
      "16255 \t -1363.4955050042392\n",
      "16460 \t -1437.0198252885002\n",
      "16666 \t -1086.705557834666\n",
      "16876 \t -1198.4762621403286\n",
      "17083 \t -1285.0994428844763\n",
      "17293 \t -949.6872004066419\n",
      "17500 \t -1040.0539293076038\n",
      "17707 \t -1196.0356590664621\n",
      "17913 \t -1240.428323284336\n",
      "18122 \t -978.9539550671219\n",
      "18326 \t -1505.0273427826821\n",
      "18529 \t -1406.9912177469398\n",
      "18732 \t -1130.9318124475742\n",
      "18938 \t -1071.4994566679757\n",
      "19146 \t -1179.2771363449187\n",
      "19355 \t -1211.6421737082821\n",
      "19564 \t -1312.9479933628381\n",
      "19771 \t -1429.1659107989447\n",
      "19979 \t -1476.0478246654584\n",
      "20185 \t -1337.540776287461\n",
      "20393 \t -1080.4496709679318\n",
      "20601 \t -1116.4012973180927\n",
      "20808 \t -1112.765818102435\n",
      "21016 \t -1073.2144882216273\n"
     ]
    }
   ],
   "source": [
    "if run.state == \"finished\":\n",
    "    for i, row in enumerate(run.scan_history()):\n",
    "        try: print(row[\"_runtime\"],'\\t', row[\"Epoch\"])\n",
    "        except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-es100]",
   "language": "python",
   "name": "conda-env-.conda-es100-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc-autonumbering": true,
  "toc-showcode": true,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
