{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Taken from https://github.com/marload/DeepRL-TensorFlow2 ##\n",
    "\n",
    "import wandb\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, concatenate\n",
    "# import tensorflow_federated as tff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import argparse\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--gamma', type=float, default=0.99)\n",
    "# parser.add_argument('--actor_lr', type=float, default=0.0005)\n",
    "# parser.add_argument('--critic_lr', type=float, default=0.001)\n",
    "# parser.add_argument('--batch_size', type=int, default=64)\n",
    "# parser.add_argument('--tau', type=float, default=0.05)\n",
    "# parser.add_argument('--train_start', type=int, default=2000)\n",
    "\n",
    "# args = parser.parse_args()\n",
    "\n",
    "\n",
    "\n",
    "# class Args:\n",
    "#     gamma = 0.99\n",
    "#     actor_lr = 0.0005\n",
    "#     critic_lr = 0.0005\n",
    "#     batch_size = 64\n",
    "#     tau = 0.05\n",
    "#     train_start = 400\n",
    "#     episodes = 10\n",
    "#     N = 3\n",
    "#     epochs = 100\n",
    "\n",
    "# args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=20000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def put(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append([state, action, reward, next_state, done])\n",
    "    \n",
    "    def sample(self):\n",
    "        sample = random.sample(self.buffer, wandb.config.batch_size)\n",
    "        states, actions, rewards, next_states, done = map(np.asarray, zip(*sample))\n",
    "        states = np.array(states).reshape(wandb.config.batch_size, -1)\n",
    "        next_states = np.array(next_states).reshape(wandb.config.batch_size, -1)\n",
    "        return states, actions, rewards, next_states, done\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor:\n",
    "    def __init__(self, state_dim, action_dim, action_bound):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.action_bound = action_bound\n",
    "        self.model = self.create_model()\n",
    "        self.opt = tf.keras.optimizers.Adam(wandb.config.actor_lr)\n",
    "\n",
    "    def create_model(self):\n",
    "        return tf.keras.Sequential([\n",
    "            Input((self.state_dim,)),\n",
    "            Dense(wandb.config.actor['layer1'], activation='relu'),\n",
    "            Dense(wandb.config.actor['layer2'], activation='relu'),\n",
    "            Dense(self.action_dim, activation='tanh'),\n",
    "            Lambda(lambda x: x * self.action_bound)\n",
    "        ])\n",
    "\n",
    "    def train(self, states, q_grads):\n",
    "        with tf.GradientTape() as tape:\n",
    "            grads = tape.gradient(self.model(states), self.model.trainable_variables, -q_grads)\n",
    "        self.opt.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "    \n",
    "    def predict(self, state):\n",
    "        return self.model.predict(state)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        state = np.reshape(state, [1, self.state_dim])\n",
    "        return self.model.predict(state)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic:\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.model = self.create_model()\n",
    "        self.opt = tf.keras.optimizers.Adam(wandb.config.critic_lr)\n",
    "\n",
    "    def create_model(self):\n",
    "        state_input = Input((self.state_dim,))\n",
    "        s1 = Dense(wandb.config.critic['state1'], activation='relu')(state_input)\n",
    "        s2 = Dense(wandb.config.critic['state2'], activation='relu')(s1)\n",
    "        action_input = Input((self.action_dim,))\n",
    "        a1 = Dense(wandb.config.critic['actor1'], activation='relu')(action_input)\n",
    "        c1 = concatenate([s2, a1], axis=-1)\n",
    "        c2 = Dense(wandb.config.critic['cat1'], activation='relu')(c1)\n",
    "        output = Dense(1, activation='linear')(c2)\n",
    "        return tf.keras.Model([state_input, action_input], output)\n",
    "    \n",
    "    def predict(self, inputs):\n",
    "        return self.model.predict(inputs)\n",
    "    \n",
    "    def q_grads(self, states, actions):\n",
    "        actions = tf.convert_to_tensor(actions)\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(actions)\n",
    "            q_values = self.model([states, actions])\n",
    "            q_values = tf.squeeze(q_values)\n",
    "        return tape.gradient(q_values, actions)\n",
    "\n",
    "    def compute_loss(self, v_pred, td_targets):\n",
    "        mse = tf.keras.losses.MeanSquaredError()\n",
    "        return mse(td_targets, v_pred)\n",
    "\n",
    "    def train(self, states, actions, td_targets):\n",
    "        with tf.GradientTape() as tape:\n",
    "            v_pred = self.model([states, actions], training=True)\n",
    "            assert v_pred.shape == td_targets.shape\n",
    "            loss = self.compute_loss(v_pred, tf.stop_gradient(td_targets))\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.opt.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env, iden = 0):\n",
    "        self.env = env\n",
    "        self.state_dim = self.env.observation_space.shape[0]\n",
    "        self.action_dim = self.env.action_space.shape[0]\n",
    "        self.action_bound = self.env.action_space.high[0]\n",
    "\n",
    "        self.buffer = ReplayBuffer()\n",
    "\n",
    "        self.actor = Actor(self.state_dim, self.action_dim, self.action_bound)\n",
    "        self.critic = Critic(self.state_dim, self.action_dim)\n",
    "        \n",
    "        self.target_actor = Actor(self.state_dim, self.action_dim, self.action_bound)\n",
    "        self.target_critic = Critic(self.state_dim, self.action_dim)\n",
    "\n",
    "        actor_weights = self.actor.model.get_weights()\n",
    "        critic_weights = self.critic.model.get_weights()\n",
    "        self.target_actor.model.set_weights(actor_weights)\n",
    "        self.target_critic.model.set_weights(critic_weights)\n",
    "        \n",
    "        self.iden = iden\n",
    "        \n",
    "    \n",
    "    def target_update(self):\n",
    "        actor_weights = self.actor.model.get_weights()\n",
    "        t_actor_weights = self.target_actor.model.get_weights()\n",
    "        critic_weights = self.critic.model.get_weights()\n",
    "        t_critic_weights = self.target_critic.model.get_weights()\n",
    "\n",
    "        for i in range(len(actor_weights)):\n",
    "            t_actor_weights[i] = wandb.config.tau * actor_weights[i] + (1 - wandb.config.tau) * t_actor_weights[i]\n",
    "\n",
    "        for i in range(len(critic_weights)):\n",
    "            t_critic_weights[i] = wandb.config.tau * critic_weights[i] + (1 - wandb.config.tau) * t_critic_weights[i]\n",
    "        \n",
    "        self.target_actor.model.set_weights(t_actor_weights)\n",
    "        self.target_critic.model.set_weights(t_critic_weights)\n",
    "\n",
    "\n",
    "    def td_target(self, rewards, q_values, dones):\n",
    "        targets = np.asarray(q_values)\n",
    "        for i in range(q_values.shape[0]):\n",
    "            if dones[i]:\n",
    "                targets[i] = rewards[i]\n",
    "            else:\n",
    "                targets[i] = wandb.config.gamma * q_values[i]\n",
    "        return targets\n",
    "\n",
    "    def list_to_batch(self, list):\n",
    "        batch = list[0]\n",
    "        for elem in list[1:]:\n",
    "            batch = np.append(batch, elem, axis=0)\n",
    "        return batch\n",
    "    \n",
    "    def ou_noise(self, x, rho=0.15, mu=0, dt=1e-1, sigma=0.2, dim=1):\n",
    "        return x + rho * (mu-x) * dt + sigma * np.sqrt(dt) * np.random.normal(size=dim)\n",
    "    \n",
    "    def replay(self):\n",
    "        for _ in range(10):\n",
    "            states, actions, rewards, next_states, dones = self.buffer.sample()\n",
    "            target_q_values = self.target_critic.predict([next_states, self.target_actor.predict(next_states)])\n",
    "            td_targets = self.td_target(rewards, target_q_values, dones)\n",
    "            \n",
    "            self.critic.train(states, actions, td_targets)\n",
    "            \n",
    "            s_actions = self.actor.predict(states)\n",
    "            s_grads = self.critic.q_grads(states, s_actions)\n",
    "            grads = np.array(s_grads).reshape((-1, self.action_dim))\n",
    "            self.actor.train(states, grads)\n",
    "            self.target_update()\n",
    "\n",
    "    def train(self, max_episodes=1000):\n",
    "        for ep in range(max_episodes):      # train a bunch of episodes\n",
    "            episode_reward, done = 0, False\n",
    "\n",
    "            state = self.env.reset()\n",
    "            bg_noise = np.zeros(self.action_dim)\n",
    "            while not done:    # run till done by hitting the action that's done\n",
    "#                 self.env.render()\n",
    "                action = self.actor.get_action(state)   # pick an action, add noise, clip the action\n",
    "                noise = self.ou_noise(bg_noise, dim=self.action_dim)\n",
    "                action = np.clip(action + noise, -self.action_bound, self.action_bound)\n",
    "\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                self.buffer.put(state, action, (reward+8)/8, next_state, done)\n",
    "                bg_noise = noise     # why does the noise wander in such a weird way\n",
    "                episode_reward += reward\n",
    "                state = next_state\n",
    "                \n",
    "            if self.buffer.size() >= wandb.config.batch_size and self.buffer.size() >= wandb.config.train_start:    # update the states if enough\n",
    "                self.replay()                \n",
    "            print('EP{} EpisodeReward={}'.format(ep, episode_reward))\n",
    "            wandb.log({'Reward' + str(self.iden): episode_reward})\n",
    "            \n",
    "        return episode_reward\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.10<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">DDPG-long-single</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/victor-qin/deep-rl-tf2\" target=\"_blank\">https://wandb.ai/victor-qin/deep-rl-tf2</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/victor-qin/deep-rl-tf2/runs/96irns88\" target=\"_blank\">https://wandb.ai/victor-qin/deep-rl-tf2/runs/96irns88</a><br/>\n",
       "                Run data is saved locally in <code>/n/home05/vqin/fasrc/es100_workspace/wandb/run-20201118_174936-96irns88</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.99, 'actor_lr': 0.001, 'critic_lr': 0.0001, 'batch_size': 64, 'tau': 0.005, 'train_start': 400, 'episodes': 5, 'num': 1, 'epochs': 200, 'actor': {'layer1': 128, 'layer2': 128}, 'critic': {'state1': 256, 'state2': 128, 'actor1': 128, 'cat1': 64}}\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1322.7037942237268\n",
      "EP1 EpisodeReward=-1168.1260875273176\n",
      "EP2 EpisodeReward=-1043.6276087472104\n",
      "EP3 EpisodeReward=-1577.6668855669561\n",
      "EP4 EpisodeReward=-1523.592785954304\n",
      "Epoch=0\t Average reward=-1523.592785954304\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1538.7762605577761\n",
      "EP1 EpisodeReward=-1556.510539357346\n",
      "EP2 EpisodeReward=-1583.1704520809099\n",
      "EP3 EpisodeReward=-1584.8621237550403\n",
      "EP4 EpisodeReward=-1520.0664558755143\n",
      "Epoch=1\t Average reward=-1520.0664558755143\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1637.6092407501499\n",
      "EP1 EpisodeReward=-1617.8484384568048\n",
      "EP2 EpisodeReward=-1478.1804375334257\n",
      "EP3 EpisodeReward=-1412.6448458876316\n",
      "EP4 EpisodeReward=-1554.3231358117396\n",
      "Epoch=2\t Average reward=-1554.3231358117396\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1515.9476439563045\n",
      "EP1 EpisodeReward=-1604.946478487345\n",
      "EP2 EpisodeReward=-1544.5826953627047\n",
      "EP3 EpisodeReward=-1607.7928685995255\n",
      "EP4 EpisodeReward=-1469.304885458834\n",
      "Epoch=3\t Average reward=-1469.304885458834\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1489.9930355102015\n",
      "EP1 EpisodeReward=-1566.2899756984991\n",
      "EP2 EpisodeReward=-1653.4482458789282\n",
      "EP3 EpisodeReward=-1622.1379541635072\n",
      "EP4 EpisodeReward=-1537.8176099455316\n",
      "Epoch=4\t Average reward=-1537.8176099455316\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1533.2001131600873\n",
      "EP1 EpisodeReward=-1633.9081381500805\n",
      "EP2 EpisodeReward=-1611.6887327914285\n",
      "EP3 EpisodeReward=-1537.1316459353359\n",
      "EP4 EpisodeReward=-1502.183725291001\n",
      "Epoch=5\t Average reward=-1502.183725291001\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1613.776709438186\n",
      "EP1 EpisodeReward=-1614.4115010273451\n",
      "EP2 EpisodeReward=-1599.9312470153475\n",
      "EP3 EpisodeReward=-1598.7688793759546\n",
      "EP4 EpisodeReward=-1587.786844108757\n",
      "Epoch=6\t Average reward=-1587.786844108757\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1477.3836680474617\n",
      "EP1 EpisodeReward=-1528.502374030362\n",
      "EP2 EpisodeReward=-1634.38128794415\n",
      "EP3 EpisodeReward=-1531.7817955473188\n",
      "EP4 EpisodeReward=-1603.5507073926365\n",
      "Epoch=7\t Average reward=-1603.5507073926365\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1567.3340075844708\n",
      "EP1 EpisodeReward=-1570.851043994156\n",
      "EP2 EpisodeReward=-1651.366273350819\n",
      "EP3 EpisodeReward=-1491.5639322441257\n",
      "EP4 EpisodeReward=-1655.1714224001983\n",
      "Epoch=8\t Average reward=-1655.1714224001983\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1606.1118364944036\n",
      "EP1 EpisodeReward=-1525.6215324348093\n",
      "EP2 EpisodeReward=-1465.8439121480653\n",
      "EP3 EpisodeReward=-1638.5113036486148\n",
      "EP4 EpisodeReward=-1605.127544009498\n",
      "Epoch=9\t Average reward=-1605.127544009498\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1359.2376147856905\n",
      "EP1 EpisodeReward=-1411.6785601246484\n",
      "EP2 EpisodeReward=-1530.4219128175148\n",
      "EP3 EpisodeReward=-1574.3011574174316\n",
      "EP4 EpisodeReward=-1555.7477405400516\n",
      "Epoch=10\t Average reward=-1555.7477405400516\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1443.2202464405354\n",
      "EP1 EpisodeReward=-1345.0919382220109\n",
      "EP2 EpisodeReward=-1521.9672784995214\n",
      "EP3 EpisodeReward=-1594.9909153090482\n",
      "EP4 EpisodeReward=-1535.342902601108\n",
      "Epoch=11\t Average reward=-1535.342902601108\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1336.236919493374\n",
      "EP1 EpisodeReward=-1505.3297350879661\n",
      "EP2 EpisodeReward=-1567.1672728271492\n",
      "EP3 EpisodeReward=-1646.2437934124844\n",
      "EP4 EpisodeReward=-1600.0096960455242\n",
      "Epoch=12\t Average reward=-1600.0096960455242\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1419.966620861233\n",
      "EP1 EpisodeReward=-1608.6728439434114\n",
      "EP2 EpisodeReward=-1636.2473376377593\n",
      "EP3 EpisodeReward=-1575.6475070691254\n",
      "EP4 EpisodeReward=-1653.2472308471597\n",
      "Epoch=13\t Average reward=-1653.2472308471597\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1511.7834726589274\n",
      "EP1 EpisodeReward=-1573.9381462778856\n",
      "EP2 EpisodeReward=-1610.2197847575592\n",
      "EP3 EpisodeReward=-1387.4798827593056\n",
      "EP4 EpisodeReward=-1587.022751637138\n",
      "Epoch=14\t Average reward=-1587.022751637138\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1515.6970619933945\n",
      "EP1 EpisodeReward=-1476.2115311458026\n",
      "EP2 EpisodeReward=-1479.1620553541616\n",
      "EP3 EpisodeReward=-1354.8958685636283\n",
      "EP4 EpisodeReward=-1540.7585048620126\n",
      "Epoch=15\t Average reward=-1540.7585048620126\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1434.4861644945054\n",
      "EP1 EpisodeReward=-1473.7555445965925\n",
      "EP2 EpisodeReward=-1540.5931191926445\n",
      "EP3 EpisodeReward=-1331.0523910634668\n",
      "EP4 EpisodeReward=-1108.9758675482892\n",
      "Epoch=16\t Average reward=-1108.9758675482892\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1561.0166285448431\n",
      "EP1 EpisodeReward=-1227.9594052158363\n",
      "EP2 EpisodeReward=-1338.867738856972\n",
      "EP3 EpisodeReward=-1116.6432069381515\n",
      "EP4 EpisodeReward=-1176.3610988863902\n",
      "Epoch=17\t Average reward=-1176.3610988863902\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1521.5466161649988\n",
      "EP1 EpisodeReward=-1423.042545793561\n",
      "EP2 EpisodeReward=-1455.7610461754134\n",
      "EP3 EpisodeReward=-1469.399642609873\n",
      "EP4 EpisodeReward=-1604.3757315908977\n",
      "Epoch=18\t Average reward=-1604.3757315908977\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1442.5734350515484\n",
      "EP1 EpisodeReward=-1879.2399354538159\n",
      "EP2 EpisodeReward=-1351.0866417632142\n",
      "EP3 EpisodeReward=-1662.5554329522909\n",
      "EP4 EpisodeReward=-1801.791256936557\n",
      "Epoch=19\t Average reward=-1801.791256936557\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1081.2726548863695\n",
      "EP1 EpisodeReward=-1755.1500293092658\n",
      "EP2 EpisodeReward=-1699.0878713135576\n",
      "EP3 EpisodeReward=-1402.3920198911624\n",
      "EP4 EpisodeReward=-1762.1368116866652\n",
      "Epoch=20\t Average reward=-1762.1368116866652\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1782.7203349856106\n",
      "EP1 EpisodeReward=-1342.6853492341427\n",
      "EP2 EpisodeReward=-1720.4900627954776\n",
      "EP3 EpisodeReward=-1874.579277241889\n",
      "EP4 EpisodeReward=-1480.8266267734755\n",
      "Epoch=21\t Average reward=-1480.8266267734755\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1797.0977177444354\n",
      "EP1 EpisodeReward=-1624.999907083912\n",
      "EP2 EpisodeReward=-1667.1819412593104\n",
      "EP3 EpisodeReward=-1651.4594094538343\n",
      "EP4 EpisodeReward=-1168.0405096193795\n",
      "Epoch=22\t Average reward=-1168.0405096193795\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1377.6967126497443\n",
      "EP1 EpisodeReward=-1353.3109504982924\n",
      "EP2 EpisodeReward=-1656.2569170817221\n",
      "EP3 EpisodeReward=-1417.289740300828\n",
      "EP4 EpisodeReward=-1367.1034111443523\n",
      "Epoch=23\t Average reward=-1367.1034111443523\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1416.8074748428012\n",
      "EP1 EpisodeReward=-985.2558932233687\n",
      "EP2 EpisodeReward=-1599.593354174028\n",
      "EP3 EpisodeReward=-1218.675820575483\n",
      "EP4 EpisodeReward=-1132.0898536992202\n",
      "Epoch=24\t Average reward=-1132.0898536992202\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1405.9984122959618\n",
      "EP1 EpisodeReward=-1465.0038966960892\n",
      "EP2 EpisodeReward=-1527.0435394452443\n",
      "EP3 EpisodeReward=-1537.485738742494\n",
      "EP4 EpisodeReward=-1761.1632102641101\n",
      "Epoch=25\t Average reward=-1761.1632102641101\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1674.057756730554\n",
      "EP1 EpisodeReward=-1907.1366202379504\n",
      "EP2 EpisodeReward=-1375.0898643899627\n",
      "EP3 EpisodeReward=-1203.3049900648157\n",
      "EP4 EpisodeReward=-1241.5532634129631\n",
      "Epoch=26\t Average reward=-1241.5532634129631\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1597.9161178680401\n",
      "EP1 EpisodeReward=-1391.197711600464\n",
      "EP2 EpisodeReward=-1359.3460154992524\n",
      "EP3 EpisodeReward=-1575.5072956754059\n",
      "EP4 EpisodeReward=-1481.8100336018151\n",
      "Epoch=27\t Average reward=-1481.8100336018151\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1461.065700357467\n",
      "EP1 EpisodeReward=-1142.7287192259912\n",
      "EP2 EpisodeReward=-1451.4527246263247\n",
      "EP3 EpisodeReward=-1174.1270291426113\n",
      "EP4 EpisodeReward=-1262.5569269016137\n",
      "Epoch=28\t Average reward=-1262.5569269016137\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1515.8967134548127\n",
      "EP1 EpisodeReward=-1207.0127220082165\n",
      "EP2 EpisodeReward=-1444.1401480699105\n",
      "EP3 EpisodeReward=-1044.905304102935\n",
      "EP4 EpisodeReward=-1496.4505789023817\n",
      "Epoch=29\t Average reward=-1496.4505789023817\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1484.458455823006\n",
      "EP1 EpisodeReward=-1525.0362557424273\n",
      "EP2 EpisodeReward=-1308.3230953282196\n",
      "EP3 EpisodeReward=-1526.6749989894436\n",
      "EP4 EpisodeReward=-1198.1515861652058\n",
      "Epoch=30\t Average reward=-1198.1515861652058\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1530.9349737009843\n",
      "EP1 EpisodeReward=-1389.3816086560064\n",
      "EP2 EpisodeReward=-1243.2504157149876\n",
      "EP3 EpisodeReward=-1497.7191544068928\n",
      "EP4 EpisodeReward=-1532.65943525487\n",
      "Epoch=31\t Average reward=-1532.65943525487\n",
      "Training Agent 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP0 EpisodeReward=-1532.1253877122117\n",
      "EP1 EpisodeReward=-1505.4308261595365\n",
      "EP2 EpisodeReward=-1507.7122721184487\n",
      "EP3 EpisodeReward=-1628.3214412777613\n",
      "EP4 EpisodeReward=-1516.9848309065228\n",
      "Epoch=32\t Average reward=-1516.9848309065228\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1178.9830469131023\n",
      "EP1 EpisodeReward=-1562.897262070943\n",
      "EP2 EpisodeReward=-1174.7399568632422\n",
      "EP3 EpisodeReward=-1269.5140641438766\n",
      "EP4 EpisodeReward=-1609.1487418962145\n",
      "Epoch=33\t Average reward=-1609.1487418962145\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1548.5629850745925\n",
      "EP1 EpisodeReward=-1100.7691588501173\n",
      "EP2 EpisodeReward=-1557.167151526689\n",
      "EP3 EpisodeReward=-1625.6729039585516\n",
      "EP4 EpisodeReward=-1345.7809236809687\n",
      "Epoch=34\t Average reward=-1345.7809236809687\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1589.3973779163807\n",
      "EP1 EpisodeReward=-1515.9625435225769\n",
      "EP2 EpisodeReward=-1499.9361724804598\n",
      "EP3 EpisodeReward=-1529.7072022997456\n",
      "EP4 EpisodeReward=-1355.544361191\n",
      "Epoch=35\t Average reward=-1355.544361191\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1344.9626911188848\n",
      "EP1 EpisodeReward=-1288.4366660745643\n",
      "EP2 EpisodeReward=-1433.236109582606\n",
      "EP3 EpisodeReward=-1581.597857847755\n",
      "EP4 EpisodeReward=-1453.9884734888976\n",
      "Epoch=36\t Average reward=-1453.9884734888976\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1548.651869546331\n",
      "EP1 EpisodeReward=-1470.2625279008469\n",
      "EP2 EpisodeReward=-1536.8046728317\n",
      "EP3 EpisodeReward=-1296.6214161228954\n",
      "EP4 EpisodeReward=-1315.7359255439746\n",
      "Epoch=37\t Average reward=-1315.7359255439746\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1539.4744900968833\n",
      "EP1 EpisodeReward=-1530.8626310293262\n",
      "EP2 EpisodeReward=-1302.717081880616\n",
      "EP3 EpisodeReward=-1524.6667561181011\n",
      "EP4 EpisodeReward=-1636.8192008443648\n",
      "Epoch=38\t Average reward=-1636.8192008443648\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1376.2941606529148\n",
      "EP1 EpisodeReward=-1617.1034316899772\n",
      "EP2 EpisodeReward=-1541.7695814837284\n",
      "EP3 EpisodeReward=-1492.0511578550454\n",
      "EP4 EpisodeReward=-1582.5240466040168\n",
      "Epoch=39\t Average reward=-1582.5240466040168\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1521.8512658694183\n",
      "EP1 EpisodeReward=-1507.8651649000465\n",
      "EP2 EpisodeReward=-1519.1980583689835\n",
      "EP3 EpisodeReward=-1402.1271164679192\n",
      "EP4 EpisodeReward=-1584.8684195311125\n",
      "Epoch=40\t Average reward=-1584.8684195311125\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1491.5242238087483\n",
      "EP1 EpisodeReward=-1475.2262730681625\n",
      "EP2 EpisodeReward=-1166.294481422876\n",
      "EP3 EpisodeReward=-1505.7455944356525\n",
      "EP4 EpisodeReward=-1333.1012993083393\n",
      "Epoch=41\t Average reward=-1333.1012993083393\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1497.1304048905572\n",
      "EP1 EpisodeReward=-1103.163624644982\n",
      "EP2 EpisodeReward=-1529.8269465622923\n",
      "EP3 EpisodeReward=-1443.8093147218824\n",
      "EP4 EpisodeReward=-1609.399803021053\n",
      "Epoch=42\t Average reward=-1609.399803021053\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1554.131505421537\n",
      "EP1 EpisodeReward=-1503.643964872757\n",
      "EP2 EpisodeReward=-1521.4130588816397\n",
      "EP3 EpisodeReward=-1014.83347983516\n",
      "EP4 EpisodeReward=-1526.669880081031\n",
      "Epoch=43\t Average reward=-1526.669880081031\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1485.476212030447\n",
      "EP1 EpisodeReward=-1227.6077626377507\n",
      "EP2 EpisodeReward=-1438.9924842369844\n",
      "EP3 EpisodeReward=-1416.1785237615152\n",
      "EP4 EpisodeReward=-1622.7615647772031\n",
      "Epoch=44\t Average reward=-1622.7615647772031\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1420.3182398661697\n",
      "EP1 EpisodeReward=-1517.5530518251212\n",
      "EP2 EpisodeReward=-1394.1069273448786\n",
      "EP3 EpisodeReward=-1463.2552183759856\n",
      "EP4 EpisodeReward=-1574.4053957205397\n",
      "Epoch=45\t Average reward=-1574.4053957205397\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1619.2094909250561\n",
      "EP1 EpisodeReward=-1543.2381449624297\n",
      "EP2 EpisodeReward=-1497.5527936207623\n",
      "EP3 EpisodeReward=-1418.166938012892\n",
      "EP4 EpisodeReward=-1524.061164845668\n",
      "Epoch=46\t Average reward=-1524.061164845668\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1477.4270638340317\n",
      "EP1 EpisodeReward=-1609.585443669542\n",
      "EP2 EpisodeReward=-1381.0983218509691\n",
      "EP3 EpisodeReward=-1443.0042991651114\n",
      "EP4 EpisodeReward=-1490.2454948878315\n",
      "Epoch=47\t Average reward=-1490.2454948878315\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1523.2595430240838\n",
      "EP1 EpisodeReward=-1537.645845313907\n",
      "EP2 EpisodeReward=-1560.688259109455\n",
      "EP3 EpisodeReward=-1503.9452483140726\n",
      "EP4 EpisodeReward=-1502.8953437602904\n",
      "Epoch=48\t Average reward=-1502.8953437602904\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1486.4658535412193\n",
      "EP1 EpisodeReward=-1522.6191749477628\n",
      "EP2 EpisodeReward=-1493.7993486395796\n",
      "EP3 EpisodeReward=-1526.8928259984973\n",
      "EP4 EpisodeReward=-1391.3689590286988\n",
      "Epoch=49\t Average reward=-1391.3689590286988\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1496.3616479277496\n",
      "EP1 EpisodeReward=-1517.9238622896746\n",
      "EP2 EpisodeReward=-1522.4630987438925\n",
      "EP3 EpisodeReward=-1634.505367754859\n",
      "EP4 EpisodeReward=-1324.3068943070216\n",
      "Epoch=50\t Average reward=-1324.3068943070216\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1510.5222111980204\n",
      "EP1 EpisodeReward=-1512.8172010365336\n",
      "EP2 EpisodeReward=-1354.673382662311\n",
      "EP3 EpisodeReward=-1630.4721100108684\n",
      "EP4 EpisodeReward=-1533.6019668382971\n",
      "Epoch=51\t Average reward=-1533.6019668382971\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1448.7841111685213\n",
      "EP1 EpisodeReward=-1511.1717555927246\n",
      "EP2 EpisodeReward=-1322.5225200551947\n",
      "EP3 EpisodeReward=-1487.6180797700304\n",
      "EP4 EpisodeReward=-1545.668741312373\n",
      "Epoch=52\t Average reward=-1545.668741312373\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1632.4495557120003\n",
      "EP1 EpisodeReward=-1545.901979442068\n",
      "EP2 EpisodeReward=-1610.163916504426\n",
      "EP3 EpisodeReward=-1597.5148433532547\n",
      "EP4 EpisodeReward=-1549.4345374391728\n",
      "Epoch=53\t Average reward=-1549.4345374391728\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1599.90864297232\n",
      "EP1 EpisodeReward=-1255.3437333309698\n",
      "EP2 EpisodeReward=-1270.646150883902\n",
      "EP3 EpisodeReward=-1641.9480138749736\n",
      "EP4 EpisodeReward=-1409.6791442379363\n",
      "Epoch=54\t Average reward=-1409.6791442379363\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1515.5441976137104\n",
      "EP1 EpisodeReward=-1189.7831348217908\n",
      "EP2 EpisodeReward=-1565.0863375555548\n",
      "EP3 EpisodeReward=-1347.7096759762853\n",
      "EP4 EpisodeReward=-1430.7059689755054\n",
      "Epoch=55\t Average reward=-1430.7059689755054\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1617.7445111846087\n",
      "EP1 EpisodeReward=-1551.5773699424826\n",
      "EP2 EpisodeReward=-983.1312365023799\n",
      "EP3 EpisodeReward=-1264.4356506233435\n",
      "EP4 EpisodeReward=-1223.366656644883\n",
      "Epoch=56\t Average reward=-1223.366656644883\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-934.0585059893073\n",
      "EP1 EpisodeReward=-1326.1704932391272\n",
      "EP2 EpisodeReward=-1514.5401996959927\n",
      "EP3 EpisodeReward=-1357.1774590683763\n",
      "EP4 EpisodeReward=-976.2588803615045\n",
      "Epoch=57\t Average reward=-976.2588803615045\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1589.4225915435866\n",
      "EP1 EpisodeReward=-1231.9902420721303\n",
      "EP2 EpisodeReward=-1525.0775951201495\n",
      "EP3 EpisodeReward=-1136.6610077683367\n",
      "EP4 EpisodeReward=-1118.9051328270223\n",
      "Epoch=58\t Average reward=-1118.9051328270223\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1218.0810794134923\n",
      "EP1 EpisodeReward=-1259.0713578489942\n",
      "EP2 EpisodeReward=-1046.665386679668\n",
      "EP3 EpisodeReward=-1445.461227319537\n",
      "EP4 EpisodeReward=-1201.7582323710621\n",
      "Epoch=59\t Average reward=-1201.7582323710621\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-997.4959316088189\n",
      "EP1 EpisodeReward=-1178.9409510390547\n",
      "EP2 EpisodeReward=-1558.9538453423838\n",
      "EP3 EpisodeReward=-1497.4687488782724\n",
      "EP4 EpisodeReward=-1576.4723693792453\n",
      "Epoch=60\t Average reward=-1576.4723693792453\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1541.6525755910116\n",
      "EP1 EpisodeReward=-1237.1674886645858\n",
      "EP2 EpisodeReward=-1512.6964353246515\n",
      "EP3 EpisodeReward=-1247.7345014898672\n",
      "EP4 EpisodeReward=-994.4588451513497\n",
      "Epoch=61\t Average reward=-994.4588451513497\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1228.7935578056122\n",
      "EP1 EpisodeReward=-1501.0295685501262\n",
      "EP2 EpisodeReward=-1579.1360004665976\n",
      "EP3 EpisodeReward=-1139.784855479811\n",
      "EP4 EpisodeReward=-1333.9377345259131\n",
      "Epoch=62\t Average reward=-1333.9377345259131\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1306.7409540162053\n",
      "EP1 EpisodeReward=-1116.5514294999466\n",
      "EP2 EpisodeReward=-1133.6935393376284\n",
      "EP3 EpisodeReward=-1143.3992887469842\n",
      "EP4 EpisodeReward=-1302.2599045528239\n",
      "Epoch=63\t Average reward=-1302.2599045528239\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1020.7670133747062\n",
      "EP1 EpisodeReward=-1226.7989170734202\n",
      "EP2 EpisodeReward=-1375.8017332291288\n",
      "EP3 EpisodeReward=-1146.6735982787245\n",
      "EP4 EpisodeReward=-1621.2767163430312\n",
      "Epoch=64\t Average reward=-1621.2767163430312\n",
      "Training Agent 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP0 EpisodeReward=-1635.7512411060477\n",
      "EP1 EpisodeReward=-1186.22066311141\n",
      "EP2 EpisodeReward=-1269.5818128052258\n",
      "EP3 EpisodeReward=-1191.9576275891905\n",
      "EP4 EpisodeReward=-1509.5575400201758\n",
      "Epoch=65\t Average reward=-1509.5575400201758\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1144.791821946767\n",
      "EP1 EpisodeReward=-1393.7906601658115\n",
      "EP2 EpisodeReward=-1502.2845004223768\n",
      "EP3 EpisodeReward=-985.6591893977715\n",
      "EP4 EpisodeReward=-1231.2479467626636\n",
      "Epoch=66\t Average reward=-1231.2479467626636\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1195.8839690719406\n",
      "EP1 EpisodeReward=-1288.1853414635098\n",
      "EP2 EpisodeReward=-1350.0241818814545\n",
      "EP3 EpisodeReward=-1365.022100201039\n",
      "EP4 EpisodeReward=-1204.2790504892296\n",
      "Epoch=67\t Average reward=-1204.2790504892296\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1505.7866785446668\n",
      "EP1 EpisodeReward=-1135.1895461900103\n",
      "EP2 EpisodeReward=-1168.8396976707168\n",
      "EP3 EpisodeReward=-1390.9086212833865\n",
      "EP4 EpisodeReward=-1651.8067711692677\n",
      "Epoch=68\t Average reward=-1651.8067711692677\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1541.5646179919177\n",
      "EP1 EpisodeReward=-1432.4183182769643\n",
      "EP2 EpisodeReward=-1535.0319632291203\n",
      "EP3 EpisodeReward=-1332.1157463658792\n",
      "EP4 EpisodeReward=-1031.4290549258985\n",
      "Epoch=69\t Average reward=-1031.4290549258985\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1192.9931876730263\n",
      "EP1 EpisodeReward=-1497.002791380291\n",
      "EP2 EpisodeReward=-1244.7531827716996\n",
      "EP3 EpisodeReward=-1380.172885357668\n",
      "EP4 EpisodeReward=-1254.1691061161641\n",
      "Epoch=70\t Average reward=-1254.1691061161641\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1527.7191874317887\n",
      "EP1 EpisodeReward=-1216.679530435042\n",
      "EP2 EpisodeReward=-1568.7479907899628\n",
      "EP3 EpisodeReward=-1646.25485599973\n",
      "EP4 EpisodeReward=-1600.687833493553\n",
      "Epoch=71\t Average reward=-1600.687833493553\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1125.6716326976718\n",
      "EP1 EpisodeReward=-1030.988184762498\n",
      "EP2 EpisodeReward=-1641.476305271455\n",
      "EP3 EpisodeReward=-1213.887382756588\n",
      "EP4 EpisodeReward=-1330.1066324576052\n",
      "Epoch=72\t Average reward=-1330.1066324576052\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1329.3359943528337\n",
      "EP1 EpisodeReward=-1344.703793441663\n",
      "EP2 EpisodeReward=-1299.5443684422924\n",
      "EP3 EpisodeReward=-1495.5728094446765\n",
      "EP4 EpisodeReward=-1493.5634203453903\n",
      "Epoch=73\t Average reward=-1493.5634203453903\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1425.7764090036574\n",
      "EP1 EpisodeReward=-1529.873612019464\n",
      "EP2 EpisodeReward=-1023.7762978976863\n",
      "EP3 EpisodeReward=-1319.2816876343713\n",
      "EP4 EpisodeReward=-1488.850362019169\n",
      "Epoch=74\t Average reward=-1488.850362019169\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1550.8611834675924\n",
      "EP1 EpisodeReward=-1513.4284229859377\n",
      "EP2 EpisodeReward=-1456.1809570497633\n",
      "EP3 EpisodeReward=-1380.0508934459276\n",
      "EP4 EpisodeReward=-1404.2858287481606\n",
      "Epoch=75\t Average reward=-1404.2858287481606\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1313.1943980505675\n",
      "EP1 EpisodeReward=-1603.3437053010477\n",
      "EP2 EpisodeReward=-1413.434149872528\n",
      "EP3 EpisodeReward=-1444.8777914049713\n",
      "EP4 EpisodeReward=-1305.944609096386\n",
      "Epoch=76\t Average reward=-1305.944609096386\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1498.6993638402023\n",
      "EP1 EpisodeReward=-1548.0845756649483\n",
      "EP2 EpisodeReward=-1479.1478293768262\n",
      "EP3 EpisodeReward=-1323.6937902892791\n",
      "EP4 EpisodeReward=-1553.5879370091225\n",
      "Epoch=77\t Average reward=-1553.5879370091225\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1546.9289883967006\n",
      "EP1 EpisodeReward=-1520.7471543005252\n",
      "EP2 EpisodeReward=-1314.139344263392\n",
      "EP3 EpisodeReward=-1447.7950023770682\n",
      "EP4 EpisodeReward=-1553.8645052835973\n",
      "Epoch=78\t Average reward=-1553.8645052835973\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1575.8140990287077\n",
      "EP1 EpisodeReward=-1335.3955380807115\n",
      "EP2 EpisodeReward=-1440.4466504445309\n",
      "EP3 EpisodeReward=-1191.0614854674643\n",
      "EP4 EpisodeReward=-1441.794455817906\n",
      "Epoch=79\t Average reward=-1441.794455817906\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1570.1047612266823\n",
      "EP1 EpisodeReward=-1456.05024271408\n",
      "EP2 EpisodeReward=-1461.565533586987\n",
      "EP3 EpisodeReward=-1458.4847090619517\n",
      "EP4 EpisodeReward=-1229.0125431990016\n",
      "Epoch=80\t Average reward=-1229.0125431990016\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1470.9500291895793\n",
      "EP1 EpisodeReward=-1478.7503497599726\n",
      "EP2 EpisodeReward=-1475.656170662949\n",
      "EP3 EpisodeReward=-1475.863824226112\n",
      "EP4 EpisodeReward=-1467.5855040999645\n",
      "Epoch=81\t Average reward=-1467.5855040999645\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1472.225140195178\n",
      "EP1 EpisodeReward=-1503.6085139763609\n",
      "EP2 EpisodeReward=-1474.0071492491643\n",
      "EP3 EpisodeReward=-1485.437526030137\n",
      "EP4 EpisodeReward=-1627.3632354386973\n",
      "Epoch=82\t Average reward=-1627.3632354386973\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1471.0414271548857\n",
      "EP1 EpisodeReward=-1498.3434419256444\n",
      "EP2 EpisodeReward=-1501.1167495424731\n",
      "EP3 EpisodeReward=-1506.174530561215\n",
      "EP4 EpisodeReward=-1386.5912087543325\n",
      "Epoch=83\t Average reward=-1386.5912087543325\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1364.7426500024094\n",
      "EP1 EpisodeReward=-1451.0509215528969\n",
      "EP2 EpisodeReward=-1338.898641643011\n",
      "EP3 EpisodeReward=-1460.9530614451983\n",
      "EP4 EpisodeReward=-1373.1713283419926\n",
      "Epoch=84\t Average reward=-1373.1713283419926\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1650.2384237512213\n",
      "EP1 EpisodeReward=-1478.853215304211\n",
      "EP2 EpisodeReward=-1494.1979720940444\n",
      "EP3 EpisodeReward=-1537.9239177331478\n",
      "EP4 EpisodeReward=-1567.5706060116943\n",
      "Epoch=85\t Average reward=-1567.5706060116943\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1398.799535057457\n",
      "EP1 EpisodeReward=-1407.7019413746705\n",
      "EP2 EpisodeReward=-1505.3241219878507\n",
      "EP3 EpisodeReward=-1441.788833426141\n",
      "EP4 EpisodeReward=-1357.109813376879\n",
      "Epoch=86\t Average reward=-1357.109813376879\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1349.5605775424533\n",
      "EP1 EpisodeReward=-1292.444991035853\n",
      "EP2 EpisodeReward=-1543.8936701335576\n",
      "EP3 EpisodeReward=-1507.6737714798226\n",
      "EP4 EpisodeReward=-1447.0017865370294\n",
      "Epoch=87\t Average reward=-1447.0017865370294\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1634.6158040790883\n",
      "EP1 EpisodeReward=-1501.8648882351247\n",
      "EP2 EpisodeReward=-1483.725026594611\n",
      "EP3 EpisodeReward=-1328.5394003195875\n",
      "EP4 EpisodeReward=-1394.0429813856026\n",
      "Epoch=88\t Average reward=-1394.0429813856026\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1570.8188613455823\n",
      "EP1 EpisodeReward=-1444.7865863345885\n",
      "EP2 EpisodeReward=-1434.6122895224012\n",
      "EP3 EpisodeReward=-1463.7172326203163\n",
      "EP4 EpisodeReward=-1360.2233747032649\n",
      "Epoch=89\t Average reward=-1360.2233747032649\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1486.741229639666\n",
      "EP1 EpisodeReward=-1435.4635783791239\n",
      "EP2 EpisodeReward=-1524.7998674088858\n",
      "EP3 EpisodeReward=-1260.7751972852818\n",
      "EP4 EpisodeReward=-1359.5855508422708\n",
      "Epoch=90\t Average reward=-1359.5855508422708\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1587.1334278412376\n",
      "EP1 EpisodeReward=-1466.3228146005072\n",
      "EP2 EpisodeReward=-1509.1492731649232\n",
      "EP3 EpisodeReward=-1333.4836451625392\n",
      "EP4 EpisodeReward=-1505.536536303339\n",
      "Epoch=91\t Average reward=-1505.536536303339\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1510.7667877044387\n",
      "EP1 EpisodeReward=-1526.962252542681\n",
      "EP2 EpisodeReward=-1514.6245330553493\n",
      "EP3 EpisodeReward=-1498.9603443799313\n",
      "EP4 EpisodeReward=-1518.3872117120386\n",
      "Epoch=92\t Average reward=-1518.3872117120386\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1500.5785983350206\n",
      "EP1 EpisodeReward=-1401.269492433946\n",
      "EP2 EpisodeReward=-1468.566441074549\n",
      "EP3 EpisodeReward=-1636.5853926582297\n",
      "EP4 EpisodeReward=-1458.4722002442456\n",
      "Epoch=93\t Average reward=-1458.4722002442456\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1472.8197375013458\n",
      "EP1 EpisodeReward=-1334.3668663439466\n",
      "EP2 EpisodeReward=-1411.0394561815817\n",
      "EP3 EpisodeReward=-1615.636109696086\n",
      "EP4 EpisodeReward=-1627.1573085154369\n",
      "Epoch=94\t Average reward=-1627.1573085154369\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1565.5302365863013\n",
      "EP1 EpisodeReward=-1503.2540012434474\n",
      "EP2 EpisodeReward=-1481.924916693063\n",
      "EP3 EpisodeReward=-1521.7840155575984\n",
      "EP4 EpisodeReward=-1479.363276528936\n",
      "Epoch=95\t Average reward=-1479.363276528936\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1493.3397876932472\n",
      "EP1 EpisodeReward=-1546.7956991686424\n",
      "EP2 EpisodeReward=-1475.2020672848942\n",
      "EP3 EpisodeReward=-1432.5012766205894\n",
      "EP4 EpisodeReward=-1490.8870772708467\n",
      "Epoch=96\t Average reward=-1490.8870772708467\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1373.6094839569405\n",
      "EP1 EpisodeReward=-1351.1478297173596\n",
      "EP2 EpisodeReward=-1462.550605367764\n",
      "EP3 EpisodeReward=-1365.7095725158877\n",
      "EP4 EpisodeReward=-1034.296745742723\n",
      "Epoch=97\t Average reward=-1034.296745742723\n",
      "Training Agent 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP0 EpisodeReward=-1164.0732897003654\n",
      "EP1 EpisodeReward=-1327.4525601396776\n",
      "EP2 EpisodeReward=-1377.6658913054564\n",
      "EP3 EpisodeReward=-1439.8608305569885\n",
      "EP4 EpisodeReward=-1442.1895995123975\n",
      "Epoch=98\t Average reward=-1442.1895995123975\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1501.3292258478123\n",
      "EP1 EpisodeReward=-1500.2427892282112\n",
      "EP2 EpisodeReward=-1496.8963019718708\n",
      "EP3 EpisodeReward=-1532.2306614261372\n",
      "EP4 EpisodeReward=-1446.6840893971512\n",
      "Epoch=99\t Average reward=-1446.6840893971512\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1450.9712495421188\n",
      "EP1 EpisodeReward=-1394.5773197329872\n",
      "EP2 EpisodeReward=-1386.0276142441298\n",
      "EP3 EpisodeReward=-1479.6606884912644\n",
      "EP4 EpisodeReward=-1331.0278756780494\n",
      "Epoch=100\t Average reward=-1331.0278756780494\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-889.4078448694921\n",
      "EP1 EpisodeReward=-883.2858591438846\n",
      "EP2 EpisodeReward=-1376.0892434750856\n",
      "EP3 EpisodeReward=-1231.0329445792818\n",
      "EP4 EpisodeReward=-1205.7064613612843\n",
      "Epoch=101\t Average reward=-1205.7064613612843\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1375.6025708407255\n",
      "EP1 EpisodeReward=-1459.8645708106874\n",
      "EP2 EpisodeReward=-1453.0764543230125\n",
      "EP3 EpisodeReward=-1357.3443636386482\n",
      "EP4 EpisodeReward=-1395.3179427642626\n",
      "Epoch=102\t Average reward=-1395.3179427642626\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1351.7303705403313\n",
      "EP1 EpisodeReward=-1328.501237075874\n",
      "EP2 EpisodeReward=-1492.1104951777804\n",
      "EP3 EpisodeReward=-1352.4628043772236\n",
      "EP4 EpisodeReward=-1431.6699560851666\n",
      "Epoch=103\t Average reward=-1431.6699560851666\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1613.9895431647549\n",
      "EP1 EpisodeReward=-1634.5332352161429\n",
      "EP2 EpisodeReward=-1470.9185063218533\n",
      "EP3 EpisodeReward=-1505.5830895871522\n",
      "EP4 EpisodeReward=-1487.628386783799\n",
      "Epoch=104\t Average reward=-1487.628386783799\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1478.3564955695954\n",
      "EP1 EpisodeReward=-1562.4509909999142\n",
      "EP2 EpisodeReward=-1636.1530467164205\n",
      "EP3 EpisodeReward=-1377.1391044417846\n",
      "EP4 EpisodeReward=-1451.099364562013\n",
      "Epoch=105\t Average reward=-1451.099364562013\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1551.2184067026312\n",
      "EP1 EpisodeReward=-1592.2953276337332\n",
      "EP2 EpisodeReward=-1538.5903907658515\n",
      "EP3 EpisodeReward=-1631.3245637783914\n",
      "EP4 EpisodeReward=-1527.1745400951306\n",
      "Epoch=106\t Average reward=-1527.1745400951306\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1509.5781007907717\n",
      "EP1 EpisodeReward=-1518.5707123746633\n",
      "EP2 EpisodeReward=-1284.0305316654888\n",
      "EP3 EpisodeReward=-1614.6119404723531\n",
      "EP4 EpisodeReward=-1502.1720012038868\n",
      "Epoch=107\t Average reward=-1502.1720012038868\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1642.312315609051\n",
      "EP1 EpisodeReward=-1543.5326831095208\n",
      "EP2 EpisodeReward=-1550.3229734809497\n",
      "EP3 EpisodeReward=-1420.3928065796463\n",
      "EP4 EpisodeReward=-1503.3921440540291\n",
      "Epoch=108\t Average reward=-1503.3921440540291\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1650.3215968666707\n",
      "EP1 EpisodeReward=-1480.3032785063383\n",
      "EP2 EpisodeReward=-1536.1453096363095\n",
      "EP3 EpisodeReward=-1506.1105728485816\n",
      "EP4 EpisodeReward=-1508.861191933973\n",
      "Epoch=109\t Average reward=-1508.861191933973\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1559.4299419732274\n",
      "EP1 EpisodeReward=-1523.9705953275848\n",
      "EP2 EpisodeReward=-1501.7876331093391\n",
      "EP3 EpisodeReward=-1490.304330385063\n",
      "EP4 EpisodeReward=-1538.7765793562526\n",
      "Epoch=110\t Average reward=-1538.7765793562526\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1552.6999851397761\n",
      "EP1 EpisodeReward=-1484.8958457010935\n",
      "EP2 EpisodeReward=-1604.2431378363544\n",
      "EP3 EpisodeReward=-1403.3545046450984\n",
      "EP4 EpisodeReward=-1493.2606739461091\n",
      "Epoch=111\t Average reward=-1493.2606739461091\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1449.2179563808934\n",
      "EP1 EpisodeReward=-1490.15840238758\n",
      "EP2 EpisodeReward=-1490.6006736669399\n",
      "EP3 EpisodeReward=-1562.5314906442834\n",
      "EP4 EpisodeReward=-1511.527803824893\n",
      "Epoch=112\t Average reward=-1511.527803824893\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1509.5682947934015\n",
      "EP1 EpisodeReward=-1489.2191317777979\n",
      "EP2 EpisodeReward=-1498.6285291097258\n",
      "EP3 EpisodeReward=-1487.2955468741475\n",
      "EP4 EpisodeReward=-1514.9240029686966\n",
      "Epoch=113\t Average reward=-1514.9240029686966\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1494.4682436744095\n",
      "EP1 EpisodeReward=-1533.1366359136653\n",
      "EP2 EpisodeReward=-1487.7265932032901\n",
      "EP3 EpisodeReward=-1533.0863681834028\n",
      "EP4 EpisodeReward=-1636.4152477562081\n",
      "Epoch=114\t Average reward=-1636.4152477562081\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1567.1056771752928\n",
      "EP1 EpisodeReward=-1484.145902717991\n",
      "EP2 EpisodeReward=-1508.0790004833277\n",
      "EP3 EpisodeReward=-1548.4194850079678\n",
      "EP4 EpisodeReward=-1590.005875120623\n",
      "Epoch=115\t Average reward=-1590.005875120623\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1649.1373606619902\n",
      "EP1 EpisodeReward=-1468.1645940589895\n",
      "EP2 EpisodeReward=-1556.9154125311254\n",
      "EP3 EpisodeReward=-1503.3271831545221\n",
      "EP4 EpisodeReward=-1485.1274289997202\n",
      "Epoch=116\t Average reward=-1485.1274289997202\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1532.3992536177484\n",
      "EP1 EpisodeReward=-1622.051108165039\n",
      "EP2 EpisodeReward=-1501.317290655905\n",
      "EP3 EpisodeReward=-1490.9820783389894\n",
      "EP4 EpisodeReward=-1603.093640321313\n",
      "Epoch=117\t Average reward=-1603.093640321313\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1564.769240989455\n",
      "EP1 EpisodeReward=-1473.6760319223831\n",
      "EP2 EpisodeReward=-1528.0202064065138\n",
      "EP3 EpisodeReward=-1552.210382293494\n",
      "EP4 EpisodeReward=-1524.895625984782\n",
      "Epoch=118\t Average reward=-1524.895625984782\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1488.9463900266767\n",
      "EP1 EpisodeReward=-1525.0555002897356\n",
      "EP2 EpisodeReward=-1636.9062118350855\n",
      "EP3 EpisodeReward=-1509.8926639494782\n",
      "EP4 EpisodeReward=-1613.8288462966384\n",
      "Epoch=119\t Average reward=-1613.8288462966384\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1468.3127181354541\n",
      "EP1 EpisodeReward=-1522.840292013849\n",
      "EP2 EpisodeReward=-1569.240918727467\n",
      "EP3 EpisodeReward=-1625.5063512958127\n",
      "EP4 EpisodeReward=-1649.8979882864014\n",
      "Epoch=120\t Average reward=-1649.8979882864014\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1500.4684850359895\n",
      "EP1 EpisodeReward=-1644.6004545540081\n",
      "EP2 EpisodeReward=-1501.0718608321006\n",
      "EP3 EpisodeReward=-1638.7899109013938\n",
      "EP4 EpisodeReward=-1470.9467571038504\n",
      "Epoch=121\t Average reward=-1470.9467571038504\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1581.954642573523\n",
      "EP1 EpisodeReward=-1525.7341571950378\n",
      "EP2 EpisodeReward=-1521.3662026673126\n",
      "EP3 EpisodeReward=-1639.7909879647739\n",
      "EP4 EpisodeReward=-1434.3422738309007\n",
      "Epoch=122\t Average reward=-1434.3422738309007\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1612.585354465204\n",
      "EP1 EpisodeReward=-1554.9626177917442\n",
      "EP2 EpisodeReward=-1531.6965643882997\n",
      "EP3 EpisodeReward=-1482.1077006714647\n",
      "EP4 EpisodeReward=-1608.1272255433491\n",
      "Epoch=123\t Average reward=-1608.1272255433491\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1460.4937518713373\n",
      "EP1 EpisodeReward=-1500.191831363622\n",
      "EP2 EpisodeReward=-1524.2305820349573\n",
      "EP3 EpisodeReward=-1543.733417503329\n",
      "EP4 EpisodeReward=-1641.2776151161538\n",
      "Epoch=124\t Average reward=-1641.2776151161538\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1510.4378708862794\n",
      "EP1 EpisodeReward=-1511.09763263832\n",
      "EP2 EpisodeReward=-1602.8335746575647\n",
      "EP3 EpisodeReward=-1493.8229687527871\n",
      "EP4 EpisodeReward=-1510.6059116014012\n",
      "Epoch=125\t Average reward=-1510.6059116014012\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1620.855855493021\n",
      "EP1 EpisodeReward=-1536.714027839307\n",
      "EP2 EpisodeReward=-1488.5215477073323\n",
      "EP3 EpisodeReward=-1529.5410987378043\n",
      "EP4 EpisodeReward=-1579.1442072358411\n",
      "Epoch=126\t Average reward=-1579.1442072358411\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1537.4329410124492\n",
      "EP1 EpisodeReward=-1637.9035184798258\n",
      "EP2 EpisodeReward=-1511.8944761409875\n",
      "EP3 EpisodeReward=-1488.258927766188\n",
      "EP4 EpisodeReward=-1552.9508975557662\n",
      "Epoch=127\t Average reward=-1552.9508975557662\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1472.3857328542101\n",
      "EP1 EpisodeReward=-1612.9051062884214\n",
      "EP2 EpisodeReward=-1575.668769473475\n",
      "EP3 EpisodeReward=-1513.6223601146714\n",
      "EP4 EpisodeReward=-1520.0531378529188\n",
      "Epoch=128\t Average reward=-1520.0531378529188\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1502.6668865376976\n",
      "EP1 EpisodeReward=-1347.4415930078421\n",
      "EP2 EpisodeReward=-1537.946135081401\n",
      "EP3 EpisodeReward=-1558.7003542394693\n",
      "EP4 EpisodeReward=-1385.5055845969594\n",
      "Epoch=129\t Average reward=-1385.5055845969594\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1441.3689074138342\n",
      "EP1 EpisodeReward=-1515.1367268049078\n",
      "EP2 EpisodeReward=-1538.5933136604644\n",
      "EP3 EpisodeReward=-1513.5237850524818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP4 EpisodeReward=-1435.002268353166\n",
      "Epoch=130\t Average reward=-1435.002268353166\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1531.310336614398\n",
      "EP1 EpisodeReward=-1628.4245735393433\n",
      "EP2 EpisodeReward=-1463.3424503897113\n",
      "EP3 EpisodeReward=-1648.760714871811\n",
      "EP4 EpisodeReward=-1511.1124490568254\n",
      "Epoch=131\t Average reward=-1511.1124490568254\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1616.6621472639838\n",
      "EP1 EpisodeReward=-1436.7923175430492\n",
      "EP2 EpisodeReward=-1546.9494577304063\n",
      "EP3 EpisodeReward=-1556.0655941309803\n",
      "EP4 EpisodeReward=-1546.9713490093436\n",
      "Epoch=132\t Average reward=-1546.9713490093436\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1538.9541096345863\n",
      "EP1 EpisodeReward=-1498.5160746205895\n",
      "EP2 EpisodeReward=-1560.189029309362\n",
      "EP3 EpisodeReward=-1562.0394058928937\n",
      "EP4 EpisodeReward=-1492.2199404813891\n",
      "Epoch=133\t Average reward=-1492.2199404813891\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1491.647083826834\n",
      "EP1 EpisodeReward=-1378.4780096383583\n",
      "EP2 EpisodeReward=-1342.1791026125088\n",
      "EP3 EpisodeReward=-1458.3160917547882\n",
      "EP4 EpisodeReward=-1497.246851556901\n",
      "Epoch=134\t Average reward=-1497.246851556901\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1598.6318290566783\n",
      "EP1 EpisodeReward=-1454.290822209243\n",
      "EP2 EpisodeReward=-1536.1855342433942\n",
      "EP3 EpisodeReward=-1546.046736547578\n",
      "EP4 EpisodeReward=-1634.7741009950682\n",
      "Epoch=135\t Average reward=-1634.7741009950682\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1472.6000255056663\n",
      "EP1 EpisodeReward=-1398.3129852532356\n",
      "EP2 EpisodeReward=-1468.4409597740303\n",
      "EP3 EpisodeReward=-1501.1942064384798\n",
      "EP4 EpisodeReward=-1516.010102485367\n",
      "Epoch=136\t Average reward=-1516.010102485367\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1507.5960350055573\n",
      "EP1 EpisodeReward=-1294.5381754510465\n",
      "EP2 EpisodeReward=-1520.177905844757\n",
      "EP3 EpisodeReward=-1652.8013876494476\n",
      "EP4 EpisodeReward=-1559.990528089661\n",
      "Epoch=137\t Average reward=-1559.990528089661\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1540.3134956001852\n",
      "EP1 EpisodeReward=-1486.1561962294434\n",
      "EP2 EpisodeReward=-1562.983514375058\n",
      "EP3 EpisodeReward=-1615.4890236657438\n",
      "EP4 EpisodeReward=-1652.1762225730504\n",
      "Epoch=138\t Average reward=-1652.1762225730504\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1596.0238489910118\n",
      "EP1 EpisodeReward=-1621.8971854045708\n",
      "EP2 EpisodeReward=-1654.1918486301934\n",
      "EP3 EpisodeReward=-1588.3455992817217\n",
      "EP4 EpisodeReward=-1541.2082869707174\n",
      "Epoch=139\t Average reward=-1541.2082869707174\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1526.3242895316541\n",
      "EP1 EpisodeReward=-1603.657199670716\n",
      "EP2 EpisodeReward=-1492.6046639324495\n",
      "EP3 EpisodeReward=-1626.3020746805146\n",
      "EP4 EpisodeReward=-1650.673673356797\n",
      "Epoch=140\t Average reward=-1650.673673356797\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1474.5232558572213\n",
      "EP1 EpisodeReward=-1511.7936914708393\n",
      "EP2 EpisodeReward=-1444.52810422331\n",
      "EP3 EpisodeReward=-1576.4638955458115\n",
      "EP4 EpisodeReward=-1130.1210313857862\n",
      "Epoch=141\t Average reward=-1130.1210313857862\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1641.5832497837262\n",
      "EP1 EpisodeReward=-1608.1981464432185\n",
      "EP2 EpisodeReward=-1479.2813260509906\n",
      "EP3 EpisodeReward=-1398.30365654688\n",
      "EP4 EpisodeReward=-1619.7909500044027\n",
      "Epoch=142\t Average reward=-1619.7909500044027\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1497.4065307979238\n",
      "EP1 EpisodeReward=-1589.8727109378633\n",
      "EP2 EpisodeReward=-1454.1095419657752\n",
      "EP3 EpisodeReward=-1480.5509565501757\n",
      "EP4 EpisodeReward=-1522.186535863319\n",
      "Epoch=143\t Average reward=-1522.186535863319\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1558.0642207700719\n",
      "EP1 EpisodeReward=-1485.397465840054\n",
      "EP2 EpisodeReward=-1626.6495208311276\n",
      "EP3 EpisodeReward=-1492.3594692378817\n",
      "EP4 EpisodeReward=-1508.1827831393673\n",
      "Epoch=144\t Average reward=-1508.1827831393673\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1519.8953642427164\n",
      "EP1 EpisodeReward=-1503.8805829434473\n",
      "EP2 EpisodeReward=-1533.65525575577\n",
      "EP3 EpisodeReward=-1490.9989127589458\n",
      "EP4 EpisodeReward=-1528.226809887185\n",
      "Epoch=145\t Average reward=-1528.226809887185\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1582.0070135846192\n",
      "EP1 EpisodeReward=-1489.5025899261725\n",
      "EP2 EpisodeReward=-1491.9744327344206\n",
      "EP3 EpisodeReward=-1622.1047015490801\n",
      "EP4 EpisodeReward=-1573.1301056915026\n",
      "Epoch=146\t Average reward=-1573.1301056915026\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1386.0170704197506\n",
      "EP1 EpisodeReward=-1382.3391279976852\n",
      "EP2 EpisodeReward=-1633.637429557446\n",
      "EP3 EpisodeReward=-1606.4119967084187\n",
      "EP4 EpisodeReward=-1622.6102797173103\n",
      "Epoch=147\t Average reward=-1622.6102797173103\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1599.6728131229588\n",
      "EP1 EpisodeReward=-1430.513650615926\n",
      "EP2 EpisodeReward=-1610.428370511351\n",
      "EP3 EpisodeReward=-1334.7487362650954\n",
      "EP4 EpisodeReward=-1587.9003679417685\n",
      "Epoch=148\t Average reward=-1587.9003679417685\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1368.9296382061389\n",
      "EP1 EpisodeReward=-1586.411441247681\n",
      "EP2 EpisodeReward=-1281.8462374272815\n",
      "EP3 EpisodeReward=-1615.0596557658646\n",
      "EP4 EpisodeReward=-1509.0044229759542\n",
      "Epoch=149\t Average reward=-1509.0044229759542\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1394.103982883474\n",
      "EP1 EpisodeReward=-1265.5690610901827\n",
      "EP2 EpisodeReward=-1557.0413731055526\n",
      "EP3 EpisodeReward=-1404.7469230624254\n",
      "EP4 EpisodeReward=-1478.7091577540195\n",
      "Epoch=150\t Average reward=-1478.7091577540195\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1631.9321760822195\n",
      "EP1 EpisodeReward=-1365.9408863127574\n",
      "EP2 EpisodeReward=-1471.0563516313555\n",
      "EP3 EpisodeReward=-1336.3999187927416\n",
      "EP4 EpisodeReward=-1560.8638703822544\n",
      "Epoch=151\t Average reward=-1560.8638703822544\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1479.146708846852\n",
      "EP1 EpisodeReward=-1535.969116668094\n",
      "EP2 EpisodeReward=-1594.1077814087682\n",
      "EP3 EpisodeReward=-1610.1408089142817\n",
      "EP4 EpisodeReward=-1416.6579683496816\n",
      "Epoch=152\t Average reward=-1416.6579683496816\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1640.8540509310455\n",
      "EP1 EpisodeReward=-1434.0259955068398\n",
      "EP2 EpisodeReward=-1639.0686087397378\n",
      "EP3 EpisodeReward=-1518.118276468281\n",
      "EP4 EpisodeReward=-1470.0476592205268\n",
      "Epoch=153\t Average reward=-1470.0476592205268\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1621.6903977734546\n",
      "EP1 EpisodeReward=-1534.7455895613448\n",
      "EP2 EpisodeReward=-1203.1865477505112\n",
      "EP3 EpisodeReward=-1443.0737769587295\n",
      "EP4 EpisodeReward=-1474.8740817064242\n",
      "Epoch=154\t Average reward=-1474.8740817064242\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1467.3008508982105\n",
      "EP1 EpisodeReward=-1476.0640059235095\n",
      "EP2 EpisodeReward=-1470.45402220402\n",
      "EP3 EpisodeReward=-1471.7119702843454\n",
      "EP4 EpisodeReward=-1497.6131771599196\n",
      "Epoch=155\t Average reward=-1497.6131771599196\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1289.8818057955996\n",
      "EP1 EpisodeReward=-1648.3023207856907\n",
      "EP2 EpisodeReward=-1628.0080143836844\n",
      "EP3 EpisodeReward=-1518.6365469247958\n",
      "EP4 EpisodeReward=-1506.8890967941397\n",
      "Epoch=156\t Average reward=-1506.8890967941397\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1521.5665175047857\n",
      "EP1 EpisodeReward=-1451.2776511339462\n",
      "EP2 EpisodeReward=-1510.2598703474414\n",
      "EP3 EpisodeReward=-1480.028250471069\n",
      "EP4 EpisodeReward=-1539.267544042864\n",
      "Epoch=157\t Average reward=-1539.267544042864\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1477.7364343474558\n",
      "EP1 EpisodeReward=-1520.0026771152277\n",
      "EP2 EpisodeReward=-1628.8238346751336\n",
      "EP3 EpisodeReward=-1645.115873556095\n",
      "EP4 EpisodeReward=-1586.162892052292\n",
      "Epoch=158\t Average reward=-1586.162892052292\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1551.8048104378283\n",
      "EP1 EpisodeReward=-1510.9574606615975\n",
      "EP2 EpisodeReward=-1530.137036401912\n",
      "EP3 EpisodeReward=-1215.394621487154\n",
      "EP4 EpisodeReward=-1510.6661822906472\n",
      "Epoch=159\t Average reward=-1510.6661822906472\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1477.022072858078\n",
      "EP1 EpisodeReward=-1546.5299408587723\n",
      "EP2 EpisodeReward=-1510.050681418013\n",
      "EP3 EpisodeReward=-1466.0311423843807\n",
      "EP4 EpisodeReward=-1499.3578479170862\n",
      "Epoch=160\t Average reward=-1499.3578479170862\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1617.1487398982733\n",
      "EP1 EpisodeReward=-1421.3469280033432\n",
      "EP2 EpisodeReward=-1495.4204817283653\n",
      "EP3 EpisodeReward=-1524.4080811199997\n",
      "EP4 EpisodeReward=-1514.6280587090253\n",
      "Epoch=161\t Average reward=-1514.6280587090253\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1487.0447618064738\n",
      "EP1 EpisodeReward=-1652.7004279794146\n",
      "EP2 EpisodeReward=-1628.7388991296204\n",
      "EP3 EpisodeReward=-1511.22181199646\n",
      "EP4 EpisodeReward=-1652.008903426893\n",
      "Epoch=162\t Average reward=-1652.008903426893\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1504.4547486924314\n",
      "EP1 EpisodeReward=-1448.637183104845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP2 EpisodeReward=-1435.464981822563\n",
      "EP3 EpisodeReward=-1422.1433953932456\n",
      "EP4 EpisodeReward=-1501.180246420052\n",
      "Epoch=163\t Average reward=-1501.180246420052\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1518.263390497734\n",
      "EP1 EpisodeReward=-1457.851878097258\n",
      "EP2 EpisodeReward=-1447.145718424395\n",
      "EP3 EpisodeReward=-1526.8538977685064\n",
      "EP4 EpisodeReward=-1555.230040762797\n",
      "Epoch=164\t Average reward=-1555.230040762797\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1474.505730248403\n",
      "EP1 EpisodeReward=-1464.500830882294\n",
      "EP2 EpisodeReward=-1635.2329645397438\n",
      "EP3 EpisodeReward=-1385.0652627758527\n",
      "EP4 EpisodeReward=-1561.737359131706\n",
      "Epoch=165\t Average reward=-1561.737359131706\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1516.666567367618\n",
      "EP1 EpisodeReward=-1652.9524100402346\n",
      "EP2 EpisodeReward=-1489.5129692017588\n",
      "EP3 EpisodeReward=-1470.8800327502017\n",
      "EP4 EpisodeReward=-1518.0210537840667\n",
      "Epoch=166\t Average reward=-1518.0210537840667\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1519.9925389160894\n",
      "EP1 EpisodeReward=-1614.9745040475834\n",
      "EP2 EpisodeReward=-1639.3785076612855\n",
      "EP3 EpisodeReward=-1476.1953250351278\n",
      "EP4 EpisodeReward=-1489.5286860224421\n",
      "Epoch=167\t Average reward=-1489.5286860224421\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1550.4533815362204\n",
      "EP1 EpisodeReward=-1607.5496272996597\n",
      "EP2 EpisodeReward=-1413.0671808137474\n",
      "EP3 EpisodeReward=-1525.0531128856921\n",
      "EP4 EpisodeReward=-1571.2086558731341\n",
      "Epoch=168\t Average reward=-1571.2086558731341\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1431.7821515791763\n",
      "EP1 EpisodeReward=-1501.3931204450205\n",
      "EP2 EpisodeReward=-1462.9192079608622\n",
      "EP3 EpisodeReward=-1383.6796591269135\n",
      "EP4 EpisodeReward=-1491.5232514622119\n",
      "Epoch=169\t Average reward=-1491.5232514622119\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1563.5696907947586\n",
      "EP1 EpisodeReward=-1432.2873432543718\n",
      "EP2 EpisodeReward=-1616.1324313611865\n",
      "EP3 EpisodeReward=-1558.7678286747798\n",
      "EP4 EpisodeReward=-1524.2878293491308\n",
      "Epoch=170\t Average reward=-1524.2878293491308\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1349.4458714312648\n",
      "EP1 EpisodeReward=-1538.9362930956238\n",
      "EP2 EpisodeReward=-1550.0153294147995\n",
      "EP3 EpisodeReward=-1237.1966292736163\n",
      "EP4 EpisodeReward=-1355.5028865775887\n",
      "Epoch=171\t Average reward=-1355.5028865775887\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1531.2718337890967\n",
      "EP1 EpisodeReward=-1522.9685842134259\n",
      "EP2 EpisodeReward=-1464.1459276446792\n",
      "EP3 EpisodeReward=-1412.0724199628683\n",
      "EP4 EpisodeReward=-1466.9574263865936\n",
      "Epoch=172\t Average reward=-1466.9574263865936\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1538.4967963881732\n",
      "EP1 EpisodeReward=-1562.0726938145597\n",
      "EP2 EpisodeReward=-1614.9774409029403\n",
      "EP3 EpisodeReward=-1651.4060271906767\n",
      "EP4 EpisodeReward=-1461.528116252096\n",
      "Epoch=173\t Average reward=-1461.528116252096\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1529.4120048545672\n",
      "EP1 EpisodeReward=-1551.1323909217033\n",
      "EP2 EpisodeReward=-1531.3248019229316\n",
      "EP3 EpisodeReward=-1630.705999248696\n",
      "EP4 EpisodeReward=-1341.8579136324756\n",
      "Epoch=174\t Average reward=-1341.8579136324756\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1519.5174589016876\n",
      "EP1 EpisodeReward=-1357.5052588884141\n",
      "EP2 EpisodeReward=-914.1130672810066\n",
      "EP3 EpisodeReward=-1330.4545800903916\n",
      "EP4 EpisodeReward=-1491.6715771902834\n",
      "Epoch=175\t Average reward=-1491.6715771902834\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1501.0083908544543\n",
      "EP1 EpisodeReward=-1500.764624819621\n",
      "EP2 EpisodeReward=-1586.3403575479376\n",
      "EP3 EpisodeReward=-1633.583889185697\n",
      "EP4 EpisodeReward=-1631.632507720017\n",
      "Epoch=176\t Average reward=-1631.632507720017\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1499.1371570861695\n",
      "EP1 EpisodeReward=-1351.3505817828652\n",
      "EP2 EpisodeReward=-1566.4619662087337\n",
      "EP3 EpisodeReward=-1568.6858804597316\n",
      "EP4 EpisodeReward=-1569.113450041678\n",
      "Epoch=177\t Average reward=-1569.113450041678\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1614.2697007076174\n",
      "EP1 EpisodeReward=-1583.5231032266502\n",
      "EP2 EpisodeReward=-1506.7657608496256\n",
      "EP3 EpisodeReward=-1538.581090508166\n",
      "EP4 EpisodeReward=-1542.397250773555\n",
      "Epoch=178\t Average reward=-1542.397250773555\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1461.6248353501228\n",
      "EP1 EpisodeReward=-1453.0353380177826\n",
      "EP2 EpisodeReward=-1516.8427561589995\n",
      "EP3 EpisodeReward=-1645.3505742716548\n",
      "EP4 EpisodeReward=-931.6156322224535\n",
      "Epoch=179\t Average reward=-931.6156322224535\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1520.2304904972607\n",
      "EP1 EpisodeReward=-1500.2144006954354\n",
      "EP2 EpisodeReward=-1640.5280738465779\n",
      "EP3 EpisodeReward=-1608.2278400584744\n",
      "EP4 EpisodeReward=-1577.3364272360327\n",
      "Epoch=180\t Average reward=-1577.3364272360327\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1605.575549494451\n",
      "EP1 EpisodeReward=-1647.900763048439\n",
      "EP2 EpisodeReward=-1516.1446767780214\n",
      "EP3 EpisodeReward=-1545.51679334889\n",
      "EP4 EpisodeReward=-1171.9759375787858\n",
      "Epoch=181\t Average reward=-1171.9759375787858\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1477.071717183547\n",
      "EP1 EpisodeReward=-1533.5838736757694\n",
      "EP2 EpisodeReward=-1588.8386516900669\n",
      "EP3 EpisodeReward=-1479.8976090899405\n",
      "EP4 EpisodeReward=-1561.243110242558\n",
      "Epoch=182\t Average reward=-1561.243110242558\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1521.2443002888847\n",
      "EP1 EpisodeReward=-1522.1689762953679\n",
      "EP2 EpisodeReward=-1564.7343701002653\n",
      "EP3 EpisodeReward=-1611.63794598575\n",
      "EP4 EpisodeReward=-1583.101341231214\n",
      "Epoch=183\t Average reward=-1583.101341231214\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1488.9018070467885\n",
      "EP1 EpisodeReward=-1336.460828555208\n",
      "EP2 EpisodeReward=-1541.6501675481022\n",
      "EP3 EpisodeReward=-1642.76736781995\n",
      "EP4 EpisodeReward=-1630.1645507929823\n",
      "Epoch=184\t Average reward=-1630.1645507929823\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1542.3067306723676\n",
      "EP1 EpisodeReward=-1531.9382707470731\n",
      "EP2 EpisodeReward=-1515.817946076758\n",
      "EP3 EpisodeReward=-1594.444146552673\n",
      "EP4 EpisodeReward=-1569.1255401586634\n",
      "Epoch=185\t Average reward=-1569.1255401586634\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1613.194891077341\n",
      "EP1 EpisodeReward=-1574.7686807999146\n",
      "EP2 EpisodeReward=-1486.536130043548\n",
      "EP3 EpisodeReward=-1584.0524892382334\n",
      "EP4 EpisodeReward=-1512.4250716461556\n",
      "Epoch=186\t Average reward=-1512.4250716461556\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1550.4452634760405\n",
      "EP1 EpisodeReward=-1527.826271449229\n",
      "EP2 EpisodeReward=-1612.5536854103789\n",
      "EP3 EpisodeReward=-1562.8450703460833\n",
      "EP4 EpisodeReward=-1548.390240579813\n",
      "Epoch=187\t Average reward=-1548.390240579813\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1467.2264847495662\n",
      "EP1 EpisodeReward=-1535.8898259456246\n",
      "EP2 EpisodeReward=-1650.9171423781015\n",
      "EP3 EpisodeReward=-1559.782481777362\n",
      "EP4 EpisodeReward=-1338.592585742385\n",
      "Epoch=188\t Average reward=-1338.592585742385\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1589.7023676720137\n",
      "EP1 EpisodeReward=-1523.5287150433883\n",
      "EP2 EpisodeReward=-1529.8487886967766\n",
      "EP3 EpisodeReward=-1441.0511771264353\n",
      "EP4 EpisodeReward=-1505.183339422275\n",
      "Epoch=189\t Average reward=-1505.183339422275\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1639.048086577417\n",
      "EP1 EpisodeReward=-1547.5933711094924\n",
      "EP2 EpisodeReward=-1471.2566296684483\n",
      "EP3 EpisodeReward=-1551.5032275210128\n",
      "EP4 EpisodeReward=-1570.1068395879345\n",
      "Epoch=190\t Average reward=-1570.1068395879345\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1537.383636876686\n",
      "EP1 EpisodeReward=-1493.9675463980723\n",
      "EP2 EpisodeReward=-1541.6997479989677\n",
      "EP3 EpisodeReward=-1598.0850676608711\n",
      "EP4 EpisodeReward=-1269.8236963679249\n",
      "Epoch=191\t Average reward=-1269.8236963679249\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1646.804122425395\n",
      "EP1 EpisodeReward=-1529.983988887779\n",
      "EP2 EpisodeReward=-1476.2407309698976\n",
      "EP3 EpisodeReward=-1510.9471422038494\n",
      "EP4 EpisodeReward=-1529.6352033308247\n",
      "Epoch=192\t Average reward=-1529.6352033308247\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1597.6702806219596\n",
      "EP1 EpisodeReward=-1544.9337610269542\n",
      "EP2 EpisodeReward=-1522.969628818761\n",
      "EP3 EpisodeReward=-1398.7961539646617\n",
      "EP4 EpisodeReward=-1518.5781825234317\n",
      "Epoch=193\t Average reward=-1518.5781825234317\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1536.0743205533959\n",
      "EP1 EpisodeReward=-1559.8679399980545\n",
      "EP2 EpisodeReward=-1623.0763500747719\n",
      "EP3 EpisodeReward=-1605.6113301773676\n",
      "EP4 EpisodeReward=-1329.7830331612324\n",
      "Epoch=194\t Average reward=-1329.7830331612324\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1552.4730537435648\n",
      "EP1 EpisodeReward=-1439.8389152601605\n",
      "EP2 EpisodeReward=-1495.017988752812\n",
      "EP3 EpisodeReward=-1609.2799461123293\n",
      "EP4 EpisodeReward=-1514.3455411110065\n",
      "Epoch=195\t Average reward=-1514.3455411110065\n",
      "Training Agent 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP0 EpisodeReward=-1528.3005759660389\n",
      "EP1 EpisodeReward=-1527.1124983747459\n",
      "EP2 EpisodeReward=-1503.3122661737964\n",
      "EP3 EpisodeReward=-1199.0681960146796\n",
      "EP4 EpisodeReward=-1223.4798154718562\n",
      "Epoch=196\t Average reward=-1223.4798154718562\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1509.1391380847497\n",
      "EP1 EpisodeReward=-1589.8859207679927\n",
      "EP2 EpisodeReward=-1203.069437080587\n",
      "EP3 EpisodeReward=-1455.442991218841\n",
      "EP4 EpisodeReward=-1634.3634034413599\n",
      "Epoch=197\t Average reward=-1634.3634034413599\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1580.4281665335286\n",
      "EP1 EpisodeReward=-1568.1379216782775\n",
      "EP2 EpisodeReward=-1160.47869575608\n",
      "EP3 EpisodeReward=-1543.6386560815056\n",
      "EP4 EpisodeReward=-1524.496858561221\n",
      "Epoch=198\t Average reward=-1524.496858561221\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1452.2057774009354\n",
      "EP1 EpisodeReward=-1545.912842699872\n",
      "EP2 EpisodeReward=-1507.0758644998489\n",
      "EP3 EpisodeReward=-1414.1748460976828\n",
      "EP4 EpisodeReward=-1374.496612724475\n",
      "Epoch=199\t Average reward=-1374.496612724475\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 169897<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/n/home05/vqin/fasrc/es100_workspace/wandb/run-20201118_174936-96irns88/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/n/home05/vqin/fasrc/es100_workspace/wandb/run-20201118_174936-96irns88/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>Reward0</td><td>-1374.49661</td></tr><tr><td>_step</td><td>1199</td></tr><tr><td>_runtime</td><td>8324</td></tr><tr><td>_timestamp</td><td>1605730101</td></tr><tr><td>batch</td><td>199</td></tr><tr><td>Epoch</td><td>-1374.49661</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>Reward0</td><td>▂▁▄▂▅▃▃▂█▂▃▄▃█▂▄▃▄▃▄▄▃▃▃▁▂▂▃▃▆▁▃▃▃▂▄▂▂▃▂</td></tr><tr><td>_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>_runtime</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>batch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>Epoch</td><td>▃▃▃▃▄▁▃▅▅▃▃▄█▆▂▃▄▄▃█▆▃▃▂▄▃▃▃▃▂▄▃▃▃▄▃▃▃▃▅</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">DDPG-long-single</strong>: <a href=\"https://wandb.ai/victor-qin/deep-rl-tf2/runs/96irns88\" target=\"_blank\">https://wandb.ai/victor-qin/deep-rl-tf2/runs/96irns88</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# def main():\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    try: wandb.finish()\n",
    "    except: pass\n",
    "    \n",
    "    ####configurations\n",
    "    wandb.init(name='DDPG-long-single', project=\"deep-rl-tf2\")\n",
    "    env_name = 'Pendulum-v0'\n",
    "\n",
    "    \n",
    "    wandb.config.gamma = 0.99\n",
    "    wandb.config.actor_lr = 0.001\n",
    "    wandb.config.critic_lr = 0.0001\n",
    "    wandb.config.batch_size = 64\n",
    "    wandb.config.tau = 0.005\n",
    "    wandb.config.train_start = 400\n",
    "    wandb.config.episodes = 5\n",
    "    wandb.config.num = 1\n",
    "    wandb.config.epochs = 400\n",
    "\n",
    "    wandb.config.actor = {'layer1': 128, 'layer2' : 128}\n",
    "    wandb.config.critic = {'state1': 256, 'state2': 128, 'actor1': 128, 'cat1': 64}\n",
    "    \n",
    "    print(wandb.config)\n",
    "    \n",
    "    # main run    \n",
    "    N = wandb.config.num\n",
    "    agents = []\n",
    "    \n",
    "    # set up the agent\n",
    "    for i in range(N):\n",
    "        env_t = gym.make(env_name)\n",
    "        agents.append(Agent(env_t, i))\n",
    "\n",
    "    # start the training\n",
    "    for z in range(wandb.config.epochs):\n",
    "\n",
    "        reward = 0\n",
    "        # train the agent\n",
    "        for j in range(len(agents)):\n",
    "            print('Training Agent {}'.format(agents[j].iden))\n",
    "            reward += agents[j].train(wandb.config.episodes)\n",
    "    \n",
    "        reward = reward / N\n",
    "        print('Epoch={}\\t Average reward={}'.format(z, reward))\n",
    "        wandb.log({'batch': z, 'Epoch': reward})\n",
    "\n",
    "\n",
    "        # get the average - actor and critic\n",
    "        critic_avg = []\n",
    "        actor_avg = []\n",
    "\n",
    "        for i in range(len(agents[0].actor.model.get_weights())):\n",
    "            \n",
    "            actor_t = agents[0].actor.model.get_weights()[i]\n",
    "\n",
    "            for j in range(1, N):\n",
    "                actor_t += agents[j].actor.model.get_weights()[i]\n",
    "\n",
    "            actor_t = actor_t / N\n",
    "            actor_avg.append(actor_t)\n",
    "\n",
    "\n",
    "        for i in range(len(agents[0].critic.model.get_weights())):\n",
    "            critic_t = agents[0].critic.model.get_weights()[i]\n",
    "\n",
    "            for j in range(1, N):\n",
    "                critic_t += agents[j].critic.model.get_weights()[i]\n",
    "\n",
    "            critic_t = critic_t / N\n",
    "            critic_avg.append(critic_t)\n",
    "\n",
    "\n",
    "        # set the average\n",
    "        for j in range(N):\n",
    "            agents[j].actor.model.set_weights(actor_avg)\n",
    "            agents[j].critic.model.set_weights(critic_avg)\n",
    "\n",
    "\n",
    "    # wrtie things out\n",
    "    for j in range(N):\n",
    "        with open(\"agent{}-actor.txt\".format(j), \"w\") as f:\n",
    "            f.write(str(agents[j].actor.model.get_weights()))\n",
    "            f.close()\n",
    "        wandb.save(\"agent{}-actor.txt\".format(j))\n",
    "        \n",
    "        \n",
    "        with open(\"agent{}-critic.txt\".format(j), \"w\") as f:\n",
    "            f.write(str(agents[j].critic.model.get_weights()))\n",
    "            f.close()\n",
    "        wandb.save(\"agent{}-critic.txt\".format(j))\n",
    "\n",
    "    \n",
    "    wandb.finish()\n",
    "    \n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Agent' object has no attribute 'saver'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-68f225e89bdf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcrit_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"agent{}-critic.h5\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0magents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcrit_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# def saver(model, na):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Agent' object has no attribute 'saver'"
     ]
    }
   ],
   "source": [
    "act_name = \"agent{}-actor.h5\".format(0)\n",
    "crit_name = \"agent{}-critic.h5\".format(0)\n",
    "\n",
    "agents[0].saver(act_name, crit_name)\n",
    "\n",
    "# def saver(model, na):\n",
    "#     model.save(na)\n",
    "    \n",
    "# agents[0].actor.model.get_weights()\n",
    "# saver(agents[0].actor.model, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas\n",
    "wandb.finish()\n",
    "api = wandb.Api()\n",
    "run = api.run(\"victor-qin/deep-rl-tf2/1s1ac3wo\")\n",
    "temp = run.history()\n",
    "# print(run.scan_history())\n",
    "# enumerate(run.history())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204 \t -1397.4979261457054\n",
      "411 \t -1314.97677473863\n",
      "616 \t -1675.7039031101483\n",
      "819 \t -1728.7964805138092\n",
      "1026 \t -1527.2705949310778\n",
      "1237 \t -1578.1663064042016\n",
      "1447 \t -1562.6616137080212\n",
      "1656 \t -1570.4740155175275\n",
      "1866 \t -1766.325876451435\n",
      "2070 \t -1645.5299593648642\n",
      "2285 \t -1418.3971006402744\n",
      "2502 \t -1555.6624956007024\n",
      "2718 \t -1214.3700774544943\n",
      "2932 \t -1561.5437283128983\n",
      "3144 \t -1403.9980968619263\n",
      "3407 \t -1367.411475604592\n",
      "3623 \t -1393.962258191116\n",
      "3830 \t -1406.6523690831484\n",
      "4029 \t -1101.6098018066943\n",
      "4231 \t -1269.6366791527905\n",
      "4449 \t -1498.5555164487466\n",
      "4673 \t -1298.3502109839903\n",
      "4894 \t -1638.4867399099965\n",
      "5118 \t -1412.7765626408752\n",
      "5334 \t -994.9669943700595\n",
      "5534 \t -1320.8340664813616\n",
      "5742 \t -1543.0258378183478\n",
      "5951 \t -1445.0463899960896\n",
      "6160 \t -1340.442215837614\n",
      "6363 \t -1392.9779538323928\n",
      "6585 \t -1430.2436505694286\n",
      "6800 \t -1292.2180494001896\n",
      "7001 \t -1243.6286137615236\n",
      "7201 \t -1505.2007945599473\n",
      "7407 \t -1393.522384897824\n",
      "7618 \t -1316.374957483535\n",
      "7821 \t -1227.821696099155\n",
      "8018 \t -1256.3030727925864\n",
      "8233 \t -1475.9687666689742\n",
      "8458 \t -1448.129700424585\n",
      "8691 \t -1589.2101814466962\n",
      "8917 \t -1554.4246337220395\n",
      "9138 \t -1354.0511439462396\n",
      "9368 \t -1483.7708450767216\n",
      "9585 \t -1552.8099394424605\n",
      "9792 \t -1590.763048342363\n",
      "9997 \t -1563.410586560709\n",
      "10202 \t -1571.663311560197\n",
      "10412 \t -1524.646201417664\n",
      "10619 \t -1565.1473699133305\n",
      "10825 \t -1430.25103398209\n",
      "11036 \t -1430.898161358348\n",
      "11246 \t -1567.7661947933484\n",
      "11458 \t -1439.7195566953417\n",
      "11670 \t -1367.7280438567038\n",
      "11882 \t -1233.4157188561635\n",
      "12095 \t -1313.3987942214123\n",
      "12301 \t -988.6851924529747\n",
      "12511 \t -1322.787519071342\n",
      "12724 \t -1142.0564715300918\n",
      "12930 \t -1404.7334101295576\n",
      "13125 \t -1563.9482078835965\n",
      "13336 \t -1317.679937362023\n",
      "13547 \t -1652.7577969825845\n",
      "13751 \t -1367.7524524357098\n",
      "13964 \t -1438.2750032889073\n",
      "14166 \t -1332.6479814298384\n",
      "14375 \t -1314.5392830365247\n",
      "14578 \t -1403.5709815928083\n",
      "14793 \t -1291.5085919747962\n",
      "15006 \t -1352.0295432585706\n",
      "15213 \t -1438.7699369651937\n",
      "15426 \t -1499.1296663572755\n",
      "15641 \t -1496.3781862511057\n",
      "15848 \t -1371.3112961218956\n",
      "16053 \t -1301.509713760541\n",
      "16255 \t -1363.4955050042392\n",
      "16460 \t -1437.0198252885002\n",
      "16666 \t -1086.705557834666\n",
      "16876 \t -1198.4762621403286\n",
      "17083 \t -1285.0994428844763\n",
      "17293 \t -949.6872004066419\n",
      "17500 \t -1040.0539293076038\n",
      "17707 \t -1196.0356590664621\n",
      "17913 \t -1240.428323284336\n",
      "18122 \t -978.9539550671219\n",
      "18326 \t -1505.0273427826821\n",
      "18529 \t -1406.9912177469398\n",
      "18732 \t -1130.9318124475742\n",
      "18938 \t -1071.4994566679757\n",
      "19146 \t -1179.2771363449187\n",
      "19355 \t -1211.6421737082821\n",
      "19564 \t -1312.9479933628381\n",
      "19771 \t -1429.1659107989447\n",
      "19979 \t -1476.0478246654584\n",
      "20185 \t -1337.540776287461\n",
      "20393 \t -1080.4496709679318\n",
      "20601 \t -1116.4012973180927\n",
      "20808 \t -1112.765818102435\n",
      "21016 \t -1073.2144882216273\n"
     ]
    }
   ],
   "source": [
    "if run.state == \"finished\":\n",
    "    for i, row in enumerate(run.scan_history()):\n",
    "        try: print(row[\"_runtime\"],'\\t', row[\"Epoch\"])\n",
    "        except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP0 EpisodeReward=-1652.69530928303\n",
      "EP0 EpisodeReward=-1455.6688225531002\n"
     ]
    }
   ],
   "source": [
    "# critic_avg = []\n",
    "# actor_avg = []\n",
    "# for i in range(len(agent1.actor.model.get_weights())):\n",
    "#     critic_avg.append(agent1.critic.model.get_weights()[i] + agent2.critic.model.get_weights()[i])\n",
    "#     actor_avg.append(agent1.actor.model.get_weights()[i] + agent2.actor.model.get_weights()[i])\n",
    "    \n",
    "#     agent1.critic.model.set_weights = critic_avg[i]\n",
    "#     agent2.critic.model.set_weights = critic_avg[i]\n",
    "    \n",
    "#     agent1.actor.model.set_weights = actor_avg[i]\n",
    "#     agent2.actor.model.set_weights = actor_avg[i]\n",
    "    \n",
    "# agent1.train(1)\n",
    "# agent2.train(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to query for notebook name, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvictor-qin\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "/n/home05/vqin/.conda/envs/es100/lib/python3.7/site-packages/IPython/html.py:14: ShimWarning: The `IPython.html` package has been deprecated since IPython 4.0. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  \"`IPython.html.widgets` has moved to `ipywidgets`.\", ShimWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.10<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">DDPG</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/victor-qin/deep-rl-tf2\" target=\"_blank\">https://wandb.ai/victor-qin/deep-rl-tf2</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/victor-qin/deep-rl-tf2/runs/2p28sqeb\" target=\"_blank\">https://wandb.ai/victor-qin/deep-rl-tf2/runs/2p28sqeb</a><br/>\n",
       "                Run data is saved locally in <code>/n/home05/vqin/fasrc/es100_workspace/wandb/run-20201113_051816-2p28sqeb</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NoSuchDisplayException",
     "evalue": "Cannot connect to \"None\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoSuchDisplayException\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-4025fe2d68fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-4025fe2d68fa>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-082b43f12cd4>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, max_episodes)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mbg_noise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mou_noise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbg_noise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/es100/lib/python3.7/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/es100/lib/python3.7/site-packages/gym/envs/classic_control/pendulum.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassic_control\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mViewer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_bounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/es100/lib/python3.7/site-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     raise ImportError('''\n",
      "\u001b[0;32m~/.conda/envs/es100/lib/python3.7/site-packages/pyglet/gl/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;31m# trickery is for circular import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0m_pyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/es100/lib/python3.7/site-packages/pyglet/window/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   1878\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_pyglet_doc_run\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m     \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m     \u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_shadow_window\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/es100/lib/python3.7/site-packages/pyglet/gl/__init__.py\u001b[0m in \u001b[0;36m_create_shadow_window\u001b[0;34m()\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWindow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m     \u001b[0m_shadow_window\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWindow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisible\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m     \u001b[0m_shadow_window\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswitch_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/es100/lib/python3.7/site-packages/pyglet/window/xlib/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_handlers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXlibWindow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;32mglobal\u001b[0m \u001b[0m_can_detect_autorepeat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/es100/lib/python3.7/site-packages/pyglet/window/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, width, height, caption, resizable, style, fullscreen, visible, vsync, display, screen, config, context, mode)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mdisplay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_display\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/es100/lib/python3.7/site-packages/pyglet/canvas/__init__.py\u001b[0m in \u001b[0;36mget_display\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;31m# Otherwise, create a new display and return it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mDisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/es100/lib/python3.7/site-packages/pyglet/canvas/xlib.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, x_screen)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_display\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXOpenDisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_display\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mNoSuchDisplayException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot connect to \"%s\"'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0mscreen_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXScreenCount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_display\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNoSuchDisplayException\u001b[0m: Cannot connect to \"None\""
     ]
    }
   ],
   "source": [
    "# def main():\n",
    "#     wandb.init(name='DDPG', project=\"deep-rl-tf2\")\n",
    "#     env_name = 'Pendulum-v0'\n",
    "#     env = gym.make(env_name)\n",
    "#     agent = Agent(env)\n",
    "#     agent.train()\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-es100]",
   "language": "python",
   "name": "conda-env-.conda-es100-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc-autonumbering": true,
  "toc-showcode": true,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
