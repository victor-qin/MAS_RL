{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'wandb.sdk' has no attribute 'lib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-df860229af3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## Taken from https://github.com/marload/DeepRL-TensorFlow2 ##\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLambda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/es100/lib/python3.7/site-packages/wandb/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwandb_lib\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwandb_sdk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0minit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwandb_sdk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'wandb.sdk' has no attribute 'lib'"
     ]
    }
   ],
   "source": [
    "## Taken from https://github.com/marload/DeepRL-TensorFlow2 ##\n",
    "\n",
    "import wandb\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, concatenate\n",
    "# import tensorflow_federated as tff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import argparse\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--gamma', type=float, default=0.99)\n",
    "# parser.add_argument('--actor_lr', type=float, default=0.0005)\n",
    "# parser.add_argument('--critic_lr', type=float, default=0.001)\n",
    "# parser.add_argument('--batch_size', type=int, default=64)\n",
    "# parser.add_argument('--tau', type=float, default=0.05)\n",
    "# parser.add_argument('--train_start', type=int, default=2000)\n",
    "\n",
    "# args = parser.parse_args()\n",
    "\n",
    "\n",
    "\n",
    "# class Args:\n",
    "#     gamma = 0.99\n",
    "#     actor_lr = 0.0005\n",
    "#     critic_lr = 0.0005\n",
    "#     batch_size = 64\n",
    "#     tau = 0.05\n",
    "#     train_start = 400\n",
    "#     episodes = 10\n",
    "#     N = 3\n",
    "#     epochs = 100\n",
    "\n",
    "# args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=20000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def put(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append([state, action, reward, next_state, done])\n",
    "    \n",
    "    def sample(self):\n",
    "        sample = random.sample(self.buffer, wandb.config.batch_size)\n",
    "        states, actions, rewards, next_states, done = map(np.asarray, zip(*sample))\n",
    "        states = np.array(states).reshape(wandb.config.batch_size, -1)\n",
    "        next_states = np.array(next_states).reshape(wandb.config.batch_size, -1)\n",
    "        return states, actions, rewards, next_states, done\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor:\n",
    "    def __init__(self, state_dim, action_dim, action_bound):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.action_bound = action_bound\n",
    "        self.model = self.create_model()\n",
    "        self.opt = tf.keras.optimizers.Adam(wandb.config.actor_lr)\n",
    "\n",
    "    def create_model(self):\n",
    "        return tf.keras.Sequential([\n",
    "            Input((self.state_dim,)),\n",
    "            Dense(wandb.config.actor['layer1'], activation='relu'),\n",
    "            Dense(wandb.config.actor['layer2'], activation='relu'),\n",
    "            Dense(self.action_dim, activation='tanh'),\n",
    "            Lambda(lambda x: x * self.action_bound)\n",
    "        ])\n",
    "\n",
    "    def train(self, states, q_grads):\n",
    "        with tf.GradientTape() as tape:\n",
    "            grads = tape.gradient(self.model(states), self.model.trainable_variables, -q_grads)\n",
    "        self.opt.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "    \n",
    "    def predict(self, state):\n",
    "        return self.model.predict(state)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        state = np.reshape(state, [1, self.state_dim])\n",
    "        return self.model.predict(state)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic:\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.model = self.create_model()\n",
    "        self.opt = tf.keras.optimizers.Adam(wandb.config.critic_lr)\n",
    "\n",
    "    def create_model(self):\n",
    "        state_input = Input((self.state_dim,))\n",
    "        s1 = Dense(wandb.config.critic['state1'], activation='relu')(state_input)\n",
    "        s2 = Dense(wandb.config.critic['state2'], activation='relu')(s1)\n",
    "        action_input = Input((self.action_dim,))\n",
    "        a1 = Dense(wandb.config.critic['actor1'], activation='relu')(action_input)\n",
    "        c1 = concatenate([s2, a1], axis=-1)\n",
    "        c2 = Dense(wandb.config.critic['cat1'], activation='relu')(c1)\n",
    "        output = Dense(1, activation='linear')(c2)\n",
    "        return tf.keras.Model([state_input, action_input], output)\n",
    "    \n",
    "    def predict(self, inputs):\n",
    "        return self.model.predict(inputs)\n",
    "    \n",
    "    def q_grads(self, states, actions):\n",
    "        actions = tf.convert_to_tensor(actions)\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(actions)\n",
    "            q_values = self.model([states, actions])\n",
    "            q_values = tf.squeeze(q_values)\n",
    "        return tape.gradient(q_values, actions)\n",
    "\n",
    "    def compute_loss(self, v_pred, td_targets):\n",
    "        mse = tf.keras.losses.MeanSquaredError()\n",
    "        return mse(td_targets, v_pred)\n",
    "\n",
    "    def train(self, states, actions, td_targets):\n",
    "        with tf.GradientTape() as tape:\n",
    "            v_pred = self.model([states, actions], training=True)\n",
    "            assert v_pred.shape == td_targets.shape\n",
    "            loss = self.compute_loss(v_pred, tf.stop_gradient(td_targets))\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.opt.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env, iden = 0):\n",
    "        self.env = env\n",
    "        self.state_dim = self.env.observation_space.shape[0]\n",
    "        self.action_dim = self.env.action_space.shape[0]\n",
    "        self.action_bound = self.env.action_space.high[0]\n",
    "\n",
    "        self.buffer = ReplayBuffer()\n",
    "\n",
    "        self.actor = Actor(self.state_dim, self.action_dim, self.action_bound)\n",
    "        self.critic = Critic(self.state_dim, self.action_dim)\n",
    "        \n",
    "        self.target_actor = Actor(self.state_dim, self.action_dim, self.action_bound)\n",
    "        self.target_critic = Critic(self.state_dim, self.action_dim)\n",
    "\n",
    "        actor_weights = self.actor.model.get_weights()\n",
    "        critic_weights = self.critic.model.get_weights()\n",
    "        self.target_actor.model.set_weights(actor_weights)\n",
    "        self.target_critic.model.set_weights(critic_weights)\n",
    "        \n",
    "        self.iden = iden\n",
    "        \n",
    "    \n",
    "    def target_update(self):\n",
    "        actor_weights = self.actor.model.get_weights()\n",
    "        t_actor_weights = self.target_actor.model.get_weights()\n",
    "        critic_weights = self.critic.model.get_weights()\n",
    "        t_critic_weights = self.target_critic.model.get_weights()\n",
    "\n",
    "        for i in range(len(actor_weights)):\n",
    "            t_actor_weights[i] = wandb.config.tau * actor_weights[i] + (1 - wandb.config.tau) * t_actor_weights[i]\n",
    "\n",
    "        for i in range(len(critic_weights)):\n",
    "            t_critic_weights[i] = wandb.config.tau * critic_weights[i] + (1 - wandb.config.tau) * t_critic_weights[i]\n",
    "        \n",
    "        self.target_actor.model.set_weights(t_actor_weights)\n",
    "        self.target_critic.model.set_weights(t_critic_weights)\n",
    "\n",
    "\n",
    "    def td_target(self, rewards, q_values, dones):\n",
    "        targets = np.asarray(q_values)\n",
    "        for i in range(q_values.shape[0]):\n",
    "            if dones[i]:\n",
    "                targets[i] = rewards[i]\n",
    "            else:\n",
    "                targets[i] = wandb.config.gamma * q_values[i]\n",
    "        return targets\n",
    "\n",
    "    def list_to_batch(self, list):\n",
    "        batch = list[0]\n",
    "        for elem in list[1:]:\n",
    "            batch = np.append(batch, elem, axis=0)\n",
    "        return batch\n",
    "    \n",
    "    def ou_noise(self, x, rho=0.15, mu=0, dt=1e-1, sigma=0.2, dim=1):\n",
    "        return x + rho * (mu-x) * dt + sigma * np.sqrt(dt) * np.random.normal(size=dim)\n",
    "    \n",
    "    def replay(self):\n",
    "        for _ in range(10):\n",
    "            states, actions, rewards, next_states, dones = self.buffer.sample()\n",
    "            target_q_values = self.target_critic.predict([next_states, self.target_actor.predict(next_states)])\n",
    "            td_targets = self.td_target(rewards, target_q_values, dones)\n",
    "            \n",
    "            self.critic.train(states, actions, td_targets)\n",
    "            \n",
    "            s_actions = self.actor.predict(states)\n",
    "            s_grads = self.critic.q_grads(states, s_actions)\n",
    "            grads = np.array(s_grads).reshape((-1, self.action_dim))\n",
    "            self.actor.train(states, grads)\n",
    "            self.target_update()\n",
    "\n",
    "    def train(self, max_episodes=1000):\n",
    "        for ep in range(max_episodes):      # train a bunch of episodes\n",
    "            episode_reward, done = 0, False\n",
    "\n",
    "            state = self.env.reset()\n",
    "            bg_noise = np.zeros(self.action_dim)\n",
    "            while not done:    # run till done by hitting the action that's done\n",
    "#                 self.env.render()\n",
    "                action = self.actor.get_action(state)   # pick an action, add noise, clip the action\n",
    "                noise = self.ou_noise(bg_noise, dim=self.action_dim)\n",
    "                action = np.clip(action + noise, -self.action_bound, self.action_bound)\n",
    "\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                self.buffer.put(state, action, (reward+8)/8, next_state, done)\n",
    "                bg_noise = noise     # why does the noise wander in such a weird way\n",
    "                episode_reward += reward\n",
    "                state = next_state\n",
    "                \n",
    "            if self.buffer.size() >= wandb.config.batch_size and self.buffer.size() >= wandb.config.train_start:    # update the states if enough\n",
    "                self.replay()                \n",
    "            print('EP{} EpisodeReward={}'.format(ep, episode_reward))\n",
    "            wandb.log({'Reward' + str(self.iden): episode_reward})\n",
    "            \n",
    "        return episode_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def main():\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    try: wandb.finish()\n",
    "    except: pass\n",
    "    \n",
    "    ####configurations\n",
    "    wandb.init(name='DDPG-multiple-test', project=\"deep-rl-tf2\")\n",
    "    env_name = 'Pendulum-v0'\n",
    "\n",
    "    \n",
    "    wandb.config.gamma = 0.99\n",
    "    wandb.config.actor_lr = 0.001\n",
    "    wandb.config.critic_lr = 0.0001\n",
    "    wandb.config.batch_size = 64\n",
    "    wandb.config.tau = 0.005\n",
    "    wandb.config.train_start = 400\n",
    "    wandb.config.episodes = 5\n",
    "    wandb.config.num = 3\n",
    "    wandb.config.epochs = 2\n",
    "\n",
    "    wandb.config.actor = {'layer1': 128, 'layer2' : 128}\n",
    "    wandb.config.critic = {'state1': 256, 'state2': 128, 'actor1': 128, 'cat1': 64}\n",
    "    \n",
    "    print(wandb.config)\n",
    "    \n",
    "    # main run    \n",
    "    N = wandb.config.num\n",
    "    agents = []\n",
    "    \n",
    "    # set up the agent\n",
    "    for i in range(N):\n",
    "        env_t = gym.make(env_name)\n",
    "        agents.append(Agent(env_t, i))\n",
    "\n",
    "    # start the training\n",
    "    for z in range(wandb.config.epochs):\n",
    "\n",
    "        reward = 0\n",
    "        # train the agent\n",
    "        for j in range(len(agents)):\n",
    "            print('Training Agent {}'.format(agents[j].iden))\n",
    "            reward += agents[j].train(wandb.config.episodes)\n",
    "    \n",
    "        reward = reward / N\n",
    "        print('Epoch={}\\t Average reward={}'.format(z, reward))\n",
    "        wandb.log({'batch': z, 'Epoch': reward})\n",
    "\n",
    "\n",
    "        # get the average - actor and critic\n",
    "        critic_avg = []\n",
    "        actor_avg = []\n",
    "\n",
    "        for i in range(len(agents[0].actor.model.get_weights())):\n",
    "            \n",
    "            actor_t = agents[0].actor.model.get_weights()[i]\n",
    "\n",
    "            for j in range(1, N):\n",
    "                actor_t += agents[j].actor.model.get_weights()[i]\n",
    "\n",
    "            actor_t = actor_t / N\n",
    "            actor_avg.append(actor_t)\n",
    "\n",
    "\n",
    "        for i in range(len(agents[0].critic.model.get_weights())):\n",
    "            critic_t = agents[0].critic.model.get_weights()[i]\n",
    "\n",
    "            for j in range(1, N):\n",
    "                critic_t += agents[j].critic.model.get_weights()[i]\n",
    "\n",
    "            critic_t = critic_t / N\n",
    "            critic_avg.append(critic_t)\n",
    "\n",
    "\n",
    "        # set the average\n",
    "        for j in range(N):\n",
    "            agents[j].actor.model.set_weights(actor_avg)\n",
    "            agents[j].critic.model.set_weights(critic_avg)\n",
    "\n",
    "\n",
    "    # wrtie things out\n",
    "    for j in range(N):\n",
    "        with open(wandb.run.dir + \"agent{}-actor.txt\".format(j), \"w\") as f:\n",
    "            f.write(str(agents[j].actor.model.get_weights()))\n",
    "            f.close()\n",
    "        wandb.save(wandb.run.dir + \"agent{}-actor.txt\".format(j))\n",
    "        \n",
    "        \n",
    "        with open(wandb.run.dir + \"agent{}-critic.txt\".format(j), \"w\") as f:\n",
    "            f.write(str(agents[j].critic.model.get_weights()))\n",
    "            f.close()\n",
    "        wandb.save(wandb.run.dir + \"agent{}-critic.txt\".format(j))\n",
    "\n",
    "    \n",
    "    wandb.finish()\n",
    "    \n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas\n",
    "wandb.finish()\n",
    "api = wandb.Api()\n",
    "run = api.run(\"victor-qin/deep-rl-tf2/1s1ac3wo\")\n",
    "temp = run.history()\n",
    "# print(run.scan_history())\n",
    "# enumerate(run.history())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run.state == \"finished\":\n",
    "    for i, row in enumerate(run.scan_history()):\n",
    "        try: print(row[\"_runtime\"],'\\t', row[\"Epoch\"])\n",
    "        except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# critic_avg = []\n",
    "# actor_avg = []\n",
    "# for i in range(len(agent1.actor.model.get_weights())):\n",
    "#     critic_avg.append(agent1.critic.model.get_weights()[i] + agent2.critic.model.get_weights()[i])\n",
    "#     actor_avg.append(agent1.actor.model.get_weights()[i] + agent2.actor.model.get_weights()[i])\n",
    "    \n",
    "#     agent1.critic.model.set_weights = critic_avg[i]\n",
    "#     agent2.critic.model.set_weights = critic_avg[i]\n",
    "    \n",
    "#     agent1.actor.model.set_weights = actor_avg[i]\n",
    "#     agent2.actor.model.set_weights = actor_avg[i]\n",
    "    \n",
    "# agent1.train(1)\n",
    "# agent2.train(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def main():\n",
    "#     wandb.init(name='DDPG', project=\"deep-rl-tf2\")\n",
    "#     env_name = 'Pendulum-v0'\n",
    "#     env = gym.make(env_name)\n",
    "#     agent = Agent(env)\n",
    "#     agent.train()\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-es100]",
   "language": "python",
   "name": "conda-env-.conda-es100-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc-autonumbering": true,
  "toc-showcode": true,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
