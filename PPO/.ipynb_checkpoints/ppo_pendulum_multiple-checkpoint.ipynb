{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda\n",
    "\n",
    "import gym\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--gamma', type=float, default=0.99)\n",
    "# parser.add_argument('--update_interval', type=int, default=5)\n",
    "# parser.add_argument('--actor_lr', type=float, default=0.0005)\n",
    "# parser.add_argument('--critic_lr', type=float, default=0.001)\n",
    "# parser.add_argument('--clip_ratio', type=float, default=0.1)\n",
    "# parser.add_argument('--lmbda', type=float, default=0.95)\n",
    "# parser.add_argument('--epochs', type=int, default=3)\n",
    "\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# class Args:\n",
    "#     gamma = 0.99\n",
    "#     update_interval = 5\n",
    "#     actor_lr = 0.0005\n",
    "#     critic_lr = 0.001\n",
    "#     batch_size = 64\n",
    "#     clip_ratio = 0.1\n",
    "#     lmbda = 0.95\n",
    "#     intervals = 3\n",
    "    \n",
    "#     episodes = 10\n",
    "#     N = 3\n",
    "#     epochs = 100\n",
    "\n",
    "# args = Args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor:\n",
    "    def __init__(self, state_dim, action_dim, action_bound, std_bound):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.action_bound = action_bound\n",
    "        self.std_bound = std_bound\n",
    "        self.model = self.create_model()\n",
    "        self.opt = tf.keras.optimizers.Adam(wandb.config.actor_lr)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        state = np.reshape(state, [1, self.state_dim])\n",
    "        mu, std = self.model.predict(state)\n",
    "        action = np.random.normal(mu[0], std[0], size=self.action_dim)\n",
    "        action = np.clip(action, -self.action_bound, self.action_bound)\n",
    "        log_policy = self.log_pdf(mu, std, action)\n",
    "\n",
    "        return log_policy, action\n",
    "\n",
    "    def log_pdf(self, mu, std, action):\n",
    "        std = tf.clip_by_value(std, self.std_bound[0], self.std_bound[1])\n",
    "        var = std ** 2\n",
    "        log_policy_pdf = -0.5 * (action - mu) ** 2 / \\\n",
    "            var - 0.5 * tf.math.log(var * 2 * np.pi)\n",
    "        return tf.reduce_sum(log_policy_pdf, 1, keepdims=True)\n",
    "\n",
    "    def create_model(self):\n",
    "        state_input = Input((self.state_dim,))\n",
    "        dense_1 = Dense(wandb.config.actor['layer1'], activation='relu')(state_input)\n",
    "        dense_2 = Dense(wandb.config.actor['layer2'], activation='relu')(dense_1)\n",
    "        out_mu = Dense(self.action_dim, activation='tanh')(dense_2)\n",
    "        mu_output = Lambda(lambda x: x * self.action_bound)(out_mu)\n",
    "        std_output = Dense(self.action_dim, activation='softplus')(dense_2)\n",
    "        return tf.keras.models.Model(state_input, [mu_output, std_output])\n",
    "\n",
    "    def compute_loss(self, log_old_policy, log_new_policy, actions, gaes):\n",
    "        ratio = tf.exp(log_new_policy - tf.stop_gradient(log_old_policy))\n",
    "        gaes = tf.stop_gradient(gaes)\n",
    "        clipped_ratio = tf.clip_by_value(\n",
    "            ratio, 1.0-wandb.config.clip_ratio, 1.0+wandb.config.clip_ratio)\n",
    "        surrogate = -tf.minimum(ratio * gaes, clipped_ratio * gaes)\n",
    "        return tf.reduce_mean(surrogate)\n",
    "\n",
    "    def train(self, log_old_policy, states, actions, gaes):\n",
    "        with tf.GradientTape() as tape:\n",
    "            mu, std = self.model(states, training=True)\n",
    "            log_new_policy = self.log_pdf(mu, std, actions)\n",
    "            loss = self.compute_loss(\n",
    "                log_old_policy, log_new_policy, actions, gaes)\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.opt.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic:\n",
    "    def __init__(self, state_dim):\n",
    "        self.state_dim = state_dim\n",
    "        self.model = self.create_model()\n",
    "        self.opt = tf.keras.optimizers.Adam(wandb.config.critic_lr)\n",
    "\n",
    "    def create_model(self):\n",
    "        return tf.keras.Sequential([\n",
    "            Input((self.state_dim,)),\n",
    "            Dense(wandb.config.critic['layer1'], activation='relu'),\n",
    "            Dense(wandb.config.critic['layer2'], activation='relu'),\n",
    "            Dense(wandb.config.critic['layer3'], activation='relu'),\n",
    "            Dense(1, activation='linear')\n",
    "        ])\n",
    "\n",
    "    def compute_loss(self, v_pred, td_targets):\n",
    "        mse = tf.keras.losses.MeanSquaredError()\n",
    "        return mse(td_targets, v_pred)\n",
    "\n",
    "    def train(self, states, td_targets):\n",
    "        with tf.GradientTape() as tape:\n",
    "            v_pred = self.model(states, training=True)\n",
    "            assert v_pred.shape == td_targets.shape\n",
    "            loss = self.compute_loss(v_pred, tf.stop_gradient(td_targets))\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.opt.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env, iden = 0):\n",
    "        self.env = env\n",
    "        self.state_dim = self.env.observation_space.shape[0]\n",
    "        self.action_dim = self.env.action_space.shape[0]\n",
    "        self.action_bound = self.env.action_space.high[0]\n",
    "        self.std_bound = [1e-2, 1.0]\n",
    "\n",
    "        self.actor_opt = tf.keras.optimizers.Adam(wandb.config.actor_lr)\n",
    "        self.critic_opt = tf.keras.optimizers.Adam(wandb.config.critic_lr)\n",
    "        self.actor = Actor(self.state_dim, self.action_dim,\n",
    "                           self.action_bound, self.std_bound)\n",
    "        self.critic = Critic(self.state_dim)\n",
    "        \n",
    "        self.iden = iden\n",
    "\n",
    "    def gae_target(self, rewards, v_values, next_v_value, done):\n",
    "        n_step_targets = np.zeros_like(rewards)\n",
    "        gae = np.zeros_like(rewards)\n",
    "        gae_cumulative = 0\n",
    "        forward_val = 0\n",
    "\n",
    "        if not done:\n",
    "            forward_val = next_v_value\n",
    "\n",
    "        for k in reversed(range(0, len(rewards))):\n",
    "            delta = rewards[k] + wandb.config.gamma * forward_val - v_values[k]\n",
    "            gae_cumulative = wandb.config.gamma * wandb.config.lmbda * gae_cumulative + delta\n",
    "            gae[k] = gae_cumulative\n",
    "            forward_val = v_values[k]\n",
    "            n_step_targets[k] = gae[k] + v_values[k]\n",
    "        return gae, n_step_targets\n",
    "\n",
    "    def list_to_batch(self, list):\n",
    "        batch = list[0]\n",
    "        for elem in list[1:]:\n",
    "            batch = np.append(batch, elem, axis=0)\n",
    "        return batch\n",
    "\n",
    "    def train(self, max_episodes=1000):\n",
    "        for ep in range(max_episodes):\n",
    "            state_batch = []\n",
    "            action_batch = []\n",
    "            reward_batch = []\n",
    "            old_policy_batch = []\n",
    "\n",
    "            episode_reward, done = 0, False\n",
    "\n",
    "            state = self.env.reset()\n",
    "\n",
    "            while not done:\n",
    "                # self.env.render()\n",
    "                log_old_policy, action = self.actor.get_action(state)\n",
    "\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "\n",
    "                state = np.reshape(state, [1, self.state_dim])\n",
    "                action = np.reshape(action, [1, 1])\n",
    "                next_state = np.reshape(next_state, [1, self.state_dim])\n",
    "                reward = np.reshape(reward, [1, 1])\n",
    "                log_old_policy = np.reshape(log_old_policy, [1, 1])\n",
    "\n",
    "                state_batch.append(state)\n",
    "                action_batch.append(action)\n",
    "                reward_batch.append((reward+8)/8)\n",
    "                old_policy_batch.append(log_old_policy)\n",
    "\n",
    "                if len(state_batch) >= wandb.config.update_interval or done:\n",
    "                    states = self.list_to_batch(state_batch)\n",
    "                    actions = self.list_to_batch(action_batch)\n",
    "                    rewards = self.list_to_batch(reward_batch)\n",
    "                    old_policys = self.list_to_batch(old_policy_batch)\n",
    "\n",
    "                    v_values = self.critic.model.predict(states)\n",
    "                    next_v_value = self.critic.model.predict(next_state)\n",
    "\n",
    "                    gaes, td_targets = self.gae_target(\n",
    "                        rewards, v_values, next_v_value, done)\n",
    "\n",
    "                    for epoch in range(wandb.config.intervals):\n",
    "                        actor_loss = self.actor.train(\n",
    "                            old_policys, states, actions, gaes)\n",
    "                        critic_loss = self.critic.train(states, td_targets)\n",
    "\n",
    "                    state_batch = []\n",
    "                    action_batch = []\n",
    "                    reward_batch = []\n",
    "                    old_policy_batch = []\n",
    "\n",
    "                episode_reward += reward[0][0]\n",
    "                state = next_state[0]\n",
    "\n",
    "            print('EP{} EpisodeReward={}'.format(ep, episode_reward))\n",
    "            wandb.log({'Reward' + str(self.iden): episode_reward})\n",
    "        \n",
    "        return episode_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.10<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">PPO-single-long</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/victor-qin/deep-rl-tf2\" target=\"_blank\">https://wandb.ai/victor-qin/deep-rl-tf2</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/victor-qin/deep-rl-tf2/runs/3fnxst17\" target=\"_blank\">https://wandb.ai/victor-qin/deep-rl-tf2/runs/3fnxst17</a><br/>\n",
       "                Run data is saved locally in <code>/n/home05/vqin/fasrc/es100_workspace/wandb/run-20201119_232717-3fnxst17</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.99, 'update_interval': 5, 'actor_lr': 0.0005, 'critic_lr': 0.001, 'batch_size': 64, 'clip_ratio': 0.1, 'lmbda': 0.95, 'intervals': 3, 'episodes': 5, 'num': 1, 'epochs': 200, 'actor': {'layer1': 32, 'layer2': 32}, 'critic': {'layer1': 32, 'layer2': 32, 'layer3': 16}}\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1604.323542328736\n",
      "EP1 EpisodeReward=-1764.8738976768589\n",
      "EP2 EpisodeReward=-1453.0758846940448\n",
      "EP3 EpisodeReward=-1567.0979561135896\n",
      "EP4 EpisodeReward=-1574.3512868836403\n",
      "Epoch=0\t Average reward=-1574.3512868836403\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1351.4573219404592\n",
      "EP1 EpisodeReward=-1166.0763621272197\n",
      "EP2 EpisodeReward=-1507.028712534297\n",
      "EP3 EpisodeReward=-1167.1190951940046\n",
      "EP4 EpisodeReward=-1591.3551537959083\n",
      "Epoch=1\t Average reward=-1591.3551537959083\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1447.0965414388672\n",
      "EP1 EpisodeReward=-1504.6176026852056\n",
      "EP2 EpisodeReward=-1204.0246770107337\n",
      "EP3 EpisodeReward=-1522.700035715799\n",
      "EP4 EpisodeReward=-1313.6323422939813\n",
      "Epoch=2\t Average reward=-1313.6323422939813\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1440.8661905502943\n",
      "EP1 EpisodeReward=-1421.2057027865583\n",
      "EP2 EpisodeReward=-1504.1619936892869\n",
      "EP3 EpisodeReward=-1459.8996289229763\n",
      "EP4 EpisodeReward=-1478.6577326287845\n",
      "Epoch=3\t Average reward=-1478.6577326287845\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1275.558214673268\n",
      "EP1 EpisodeReward=-1126.2924878429064\n",
      "EP2 EpisodeReward=-1393.54289890827\n",
      "EP3 EpisodeReward=-1560.7907095922826\n",
      "EP4 EpisodeReward=-1432.2535761715615\n",
      "Epoch=4\t Average reward=-1432.2535761715615\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1435.960650307892\n",
      "EP1 EpisodeReward=-1292.2227947391384\n",
      "EP2 EpisodeReward=-1462.4108028059063\n",
      "EP3 EpisodeReward=-1386.9543088310422\n",
      "EP4 EpisodeReward=-1470.6588887661571\n",
      "Epoch=5\t Average reward=-1470.6588887661571\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1199.0365123894471\n",
      "EP1 EpisodeReward=-1500.99002149147\n",
      "EP2 EpisodeReward=-1418.6070530371887\n",
      "EP3 EpisodeReward=-1002.2096089667864\n",
      "EP4 EpisodeReward=-1394.9511887317067\n",
      "Epoch=6\t Average reward=-1394.9511887317067\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1499.197768448227\n",
      "EP1 EpisodeReward=-1435.1425531929174\n",
      "EP2 EpisodeReward=-1445.2946798159537\n",
      "EP3 EpisodeReward=-1302.6509109396081\n",
      "EP4 EpisodeReward=-1255.772046377169\n",
      "Epoch=7\t Average reward=-1255.772046377169\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1284.1884419516748\n",
      "EP1 EpisodeReward=-1574.0551545661533\n",
      "EP2 EpisodeReward=-1322.1387165731105\n",
      "EP3 EpisodeReward=-1524.9032215968855\n",
      "EP4 EpisodeReward=-1266.027475025864\n",
      "Epoch=8\t Average reward=-1266.027475025864\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1313.5978133239205\n",
      "EP1 EpisodeReward=-1452.8326668593936\n",
      "EP2 EpisodeReward=-1370.1242951449708\n",
      "EP3 EpisodeReward=-1539.8307125872045\n",
      "EP4 EpisodeReward=-1331.4682838774888\n",
      "Epoch=9\t Average reward=-1331.4682838774888\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1537.3280908297081\n",
      "EP1 EpisodeReward=-1547.7470950190686\n",
      "EP2 EpisodeReward=-1510.2458736827466\n",
      "EP3 EpisodeReward=-1374.3458698714878\n",
      "EP4 EpisodeReward=-1462.4789446774719\n",
      "Epoch=10\t Average reward=-1462.4789446774719\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1465.3953209255424\n",
      "EP1 EpisodeReward=-1554.8745771999372\n",
      "EP2 EpisodeReward=-1544.7009655335194\n",
      "EP3 EpisodeReward=-1396.5875835887105\n",
      "EP4 EpisodeReward=-1464.64758991453\n",
      "Epoch=11\t Average reward=-1464.64758991453\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1332.9996545276974\n",
      "EP1 EpisodeReward=-1152.6047603779493\n",
      "EP2 EpisodeReward=-1306.4518524086966\n",
      "EP3 EpisodeReward=-1533.873529217927\n",
      "EP4 EpisodeReward=-1620.1604902150077\n",
      "Epoch=12\t Average reward=-1620.1604902150077\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1329.7186611676636\n",
      "EP1 EpisodeReward=-1644.5374582176357\n",
      "EP2 EpisodeReward=-1528.184610790064\n",
      "EP3 EpisodeReward=-1511.5896301420314\n",
      "EP4 EpisodeReward=-1253.584701321973\n",
      "Epoch=13\t Average reward=-1253.584701321973\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1188.900177389159\n",
      "EP1 EpisodeReward=-1478.8594143643184\n",
      "EP2 EpisodeReward=-1452.3229201757185\n",
      "EP3 EpisodeReward=-1516.047768369527\n",
      "EP4 EpisodeReward=-1536.252622660479\n",
      "Epoch=14\t Average reward=-1536.252622660479\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1628.463733118623\n",
      "EP1 EpisodeReward=-1189.8867264635333\n",
      "EP2 EpisodeReward=-1168.7415101098063\n",
      "EP3 EpisodeReward=-1457.612343424921\n",
      "EP4 EpisodeReward=-1458.0425681422323\n",
      "Epoch=15\t Average reward=-1458.0425681422323\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1335.810446590953\n",
      "EP1 EpisodeReward=-1256.759439903171\n",
      "EP2 EpisodeReward=-1532.5008880618232\n",
      "EP3 EpisodeReward=-1639.2813487770838\n",
      "EP4 EpisodeReward=-1556.5966538956436\n",
      "Epoch=16\t Average reward=-1556.5966538956436\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1550.394485064473\n",
      "EP1 EpisodeReward=-1652.3893864043844\n",
      "EP2 EpisodeReward=-1302.4212803997768\n",
      "EP3 EpisodeReward=-1205.885108770075\n",
      "EP4 EpisodeReward=-1525.4880552986717\n",
      "Epoch=17\t Average reward=-1525.4880552986717\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1462.1201486418122\n",
      "EP1 EpisodeReward=-1452.1184735213826\n",
      "EP2 EpisodeReward=-1500.4031846607497\n",
      "EP3 EpisodeReward=-1325.8226889202083\n",
      "EP4 EpisodeReward=-1530.0483721031399\n",
      "Epoch=18\t Average reward=-1530.0483721031399\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1162.1661463503765\n",
      "EP1 EpisodeReward=-1128.949037973175\n",
      "EP2 EpisodeReward=-1207.2375376974067\n",
      "EP3 EpisodeReward=-1019.2751671898083\n",
      "EP4 EpisodeReward=-1195.818876665007\n",
      "Epoch=19\t Average reward=-1195.818876665007\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1169.8489548872828\n",
      "EP1 EpisodeReward=-1361.9828635326242\n",
      "EP2 EpisodeReward=-1155.5237763730468\n",
      "EP3 EpisodeReward=-1178.0421236958427\n",
      "EP4 EpisodeReward=-1165.9029350567143\n",
      "Epoch=20\t Average reward=-1165.9029350567143\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-927.5782117838098\n",
      "EP1 EpisodeReward=-1249.170490026476\n",
      "EP2 EpisodeReward=-1517.1813783749371\n",
      "EP3 EpisodeReward=-1383.2482641489755\n",
      "EP4 EpisodeReward=-1354.708023533308\n",
      "Epoch=21\t Average reward=-1354.708023533308\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1308.902638495959\n",
      "EP1 EpisodeReward=-1389.665626166039\n",
      "EP2 EpisodeReward=-1327.248804668577\n",
      "EP3 EpisodeReward=-1170.1801365572621\n",
      "EP4 EpisodeReward=-1345.059301090283\n",
      "Epoch=22\t Average reward=-1345.059301090283\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1293.5490953861056\n",
      "EP1 EpisodeReward=-1428.174536016296\n",
      "EP2 EpisodeReward=-1518.2080685872513\n",
      "EP3 EpisodeReward=-1341.7336896839677\n",
      "EP4 EpisodeReward=-1163.096237724807\n",
      "Epoch=23\t Average reward=-1163.096237724807\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1517.7809555675838\n",
      "EP1 EpisodeReward=-1410.7034796389046\n",
      "EP2 EpisodeReward=-1148.0365416023587\n",
      "EP3 EpisodeReward=-1506.5062095410387\n",
      "EP4 EpisodeReward=-1066.1438223747723\n",
      "Epoch=24\t Average reward=-1066.1438223747723\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1588.1426618370708\n",
      "EP1 EpisodeReward=-945.9797328488033\n",
      "EP2 EpisodeReward=-1507.6374447692015\n",
      "EP3 EpisodeReward=-1187.113825312912\n",
      "EP4 EpisodeReward=-1618.892507218165\n",
      "Epoch=25\t Average reward=-1618.892507218165\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1284.0629031805656\n",
      "EP1 EpisodeReward=-1500.0506492886589\n",
      "EP2 EpisodeReward=-1618.641303926588\n",
      "EP3 EpisodeReward=-1402.285206148121\n",
      "EP4 EpisodeReward=-1441.217412187337\n",
      "Epoch=26\t Average reward=-1441.217412187337\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1498.4956438828597\n",
      "EP1 EpisodeReward=-1414.0806009137943\n",
      "EP2 EpisodeReward=-1616.2888695848565\n",
      "EP3 EpisodeReward=-1496.0459554071087\n",
      "EP4 EpisodeReward=-1504.4682347377486\n",
      "Epoch=27\t Average reward=-1504.4682347377486\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1628.9753185543448\n",
      "EP1 EpisodeReward=-1481.905660709853\n",
      "EP2 EpisodeReward=-1212.5760227214894\n",
      "EP3 EpisodeReward=-1495.3128930916282\n",
      "EP4 EpisodeReward=-1164.4338537582892\n",
      "Epoch=28\t Average reward=-1164.4338537582892\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1497.2853642904654\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    try: wandb.finish()\n",
    "    except: pass\n",
    "    \n",
    "    ####configurations\n",
    "    wandb.init(name='PPO-single-long', project=\"deep-rl-tf2\")\n",
    "    env_name = 'Pendulum-v0'\n",
    "\n",
    "    \n",
    "    wandb.config.gamma = 0.99\n",
    "    wandb.config.update_interval = 5\n",
    "    wandb.config.actor_lr = 0.0005\n",
    "    wandb.config.critic_lr = 0.001\n",
    "    wandb.config.batch_size = 64\n",
    "    wandb.config.clip_ratio = 0.1\n",
    "    wandb.config.lmbda = 0.95\n",
    "    wandb.config.intervals = 3\n",
    "    \n",
    "    wandb.config.episodes = 5\n",
    "    wandb.config.num = 1\n",
    "    wandb.config.epochs = 200\n",
    "\n",
    "    wandb.config.actor = {'layer1': 32, 'layer2' : 32}\n",
    "    wandb.config.critic = {'layer1': 32, 'layer2' : 32, 'layer3': 16}\n",
    "    \n",
    "    print(wandb.config)\n",
    "    \n",
    "    # main run    \n",
    "    N = wandb.config.num\n",
    "    agents = []\n",
    "    \n",
    "    # set up the agent\n",
    "    for i in range(N):\n",
    "        env_t = gym.make(env_name)\n",
    "        agents.append(Agent(env_t, i))\n",
    "\n",
    "    # start the training\n",
    "    for z in range(wandb.config.epochs):\n",
    "\n",
    "        reward = 0\n",
    "        # train the agent\n",
    "        for j in range(len(agents)):\n",
    "            print('Training Agent {}'.format(agents[j].iden))\n",
    "            reward += agents[j].train(wandb.config.episodes)\n",
    "    \n",
    "        reward = reward / N\n",
    "        print('Epoch={}\\t Average reward={}'.format(z, reward))\n",
    "        wandb.log({'batch': z, 'Epoch': reward})\n",
    "\n",
    "\n",
    "        # get the average - actor and critic\n",
    "        critic_avg = []\n",
    "        actor_avg = []\n",
    "\n",
    "        for i in range(len(agents[0].actor.model.get_weights())):\n",
    "            \n",
    "            actor_t = agents[0].actor.model.get_weights()[i]\n",
    "\n",
    "            for j in range(1, N):\n",
    "                actor_t += agents[j].actor.model.get_weights()[i]\n",
    "\n",
    "            actor_t = actor_t / N\n",
    "            actor_avg.append(actor_t)\n",
    "\n",
    "\n",
    "        for i in range(len(agents[0].critic.model.get_weights())):\n",
    "            critic_t = agents[0].critic.model.get_weights()[i]\n",
    "\n",
    "            for j in range(1, N):\n",
    "                critic_t += agents[j].critic.model.get_weights()[i]\n",
    "\n",
    "            critic_t = critic_t / N\n",
    "            critic_avg.append(critic_t)\n",
    "\n",
    "\n",
    "        # set the average\n",
    "        for j in range(N):\n",
    "            agents[j].actor.model.set_weights(actor_avg)\n",
    "            agents[j].critic.model.set_weights(critic_avg)\n",
    "\n",
    "\n",
    "    # wrtie things out\n",
    "    for j in range(N):\n",
    "        with open(\"agent{}-actor.txt\".format(j), \"w\") as f:\n",
    "            f.write(str(agents[j].actor.model.get_weights()))\n",
    "            f.close()\n",
    "        wandb.save(\"agent{}-actor.txt\".format(j))\n",
    "        \n",
    "        \n",
    "        with open(\"agent{}-critic.txt\".format(j), \"w\") as f:\n",
    "            f.write(str(agents[j].critic.model.get_weights()))\n",
    "            f.close()\n",
    "        wandb.save(\"agent{}-critic.txt\".format(j))\n",
    "\n",
    "    \n",
    "    wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-es100]",
   "language": "python",
   "name": "conda-env-.conda-es100-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
