{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda\n",
    "\n",
    "import gym\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--gamma', type=float, default=0.99)\n",
    "# parser.add_argument('--update_interval', type=int, default=5)\n",
    "# parser.add_argument('--actor_lr', type=float, default=0.0005)\n",
    "# parser.add_argument('--critic_lr', type=float, default=0.001)\n",
    "# parser.add_argument('--clip_ratio', type=float, default=0.1)\n",
    "# parser.add_argument('--lmbda', type=float, default=0.95)\n",
    "# parser.add_argument('--epochs', type=int, default=3)\n",
    "\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# class Args:\n",
    "#     gamma = 0.99\n",
    "#     update_interval = 5\n",
    "#     actor_lr = 0.0005\n",
    "#     critic_lr = 0.001\n",
    "#     batch_size = 64\n",
    "#     clip_ratio = 0.1\n",
    "#     lmbda = 0.95\n",
    "#     intervals = 3\n",
    "    \n",
    "#     episodes = 10\n",
    "#     N = 3\n",
    "#     epochs = 100\n",
    "\n",
    "# args = Args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor:\n",
    "    def __init__(self, state_dim, action_dim, action_bound, std_bound):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.action_bound = action_bound\n",
    "        self.std_bound = std_bound\n",
    "        self.model = self.create_model()\n",
    "        self.opt = tf.keras.optimizers.Adam(wandb.config.actor_lr)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        state = np.reshape(state, [1, self.state_dim])\n",
    "        mu, std = self.model.predict(state)\n",
    "        action = np.random.normal(mu[0], std[0], size=self.action_dim)\n",
    "        action = np.clip(action, -self.action_bound, self.action_bound)\n",
    "        log_policy = self.log_pdf(mu, std, action)\n",
    "\n",
    "        return log_policy, action\n",
    "\n",
    "    def log_pdf(self, mu, std, action):\n",
    "        std = tf.clip_by_value(std, self.std_bound[0], self.std_bound[1])\n",
    "        var = std ** 2\n",
    "        log_policy_pdf = -0.5 * (action - mu) ** 2 / \\\n",
    "            var - 0.5 * tf.math.log(var * 2 * np.pi)\n",
    "        return tf.reduce_sum(log_policy_pdf, 1, keepdims=True)\n",
    "\n",
    "    def create_model(self):\n",
    "        state_input = Input((self.state_dim,))\n",
    "        dense_1 = Dense(wandb.config.actor['layer1'], activation='relu')(state_input)\n",
    "        dense_2 = Dense(wandb.config.actor['layer2'], activation='relu')(dense_1)\n",
    "        out_mu = Dense(self.action_dim, activation='tanh')(dense_2)\n",
    "        mu_output = Lambda(lambda x: x * self.action_bound)(out_mu)\n",
    "        std_output = Dense(self.action_dim, activation='softplus')(dense_2)\n",
    "        return tf.keras.models.Model(state_input, [mu_output, std_output])\n",
    "\n",
    "    def compute_loss(self, log_old_policy, log_new_policy, actions, gaes):\n",
    "        ratio = tf.exp(log_new_policy - tf.stop_gradient(log_old_policy))\n",
    "        gaes = tf.stop_gradient(gaes)\n",
    "        clipped_ratio = tf.clip_by_value(\n",
    "            ratio, 1.0-wandb.config.clip_ratio, 1.0+wandb.config.clip_ratio)\n",
    "        surrogate = -tf.minimum(ratio * gaes, clipped_ratio * gaes)\n",
    "        return tf.reduce_mean(surrogate)\n",
    "\n",
    "    def train(self, log_old_policy, states, actions, gaes):\n",
    "        with tf.GradientTape() as tape:\n",
    "            mu, std = self.model(states, training=True)\n",
    "            log_new_policy = self.log_pdf(mu, std, actions)\n",
    "            loss = self.compute_loss(\n",
    "                log_old_policy, log_new_policy, actions, gaes)\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.opt.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic:\n",
    "    def __init__(self, state_dim):\n",
    "        self.state_dim = state_dim\n",
    "        self.model = self.create_model()\n",
    "        self.opt = tf.keras.optimizers.Adam(wandb.config.critic_lr)\n",
    "\n",
    "    def create_model(self):\n",
    "        return tf.keras.Sequential([\n",
    "            Input((self.state_dim,)),\n",
    "            Dense(wandb.config.critic['layer1'], activation='relu'),\n",
    "            Dense(wandb.config.critic['layer2'], activation='relu'),\n",
    "            Dense(wandb.config.critic['layer3'], activation='relu'),\n",
    "            Dense(1, activation='linear')\n",
    "        ])\n",
    "\n",
    "    def compute_loss(self, v_pred, td_targets):\n",
    "        mse = tf.keras.losses.MeanSquaredError()\n",
    "        return mse(td_targets, v_pred)\n",
    "\n",
    "    def train(self, states, td_targets):\n",
    "        with tf.GradientTape() as tape:\n",
    "            v_pred = self.model(states, training=True)\n",
    "            assert v_pred.shape == td_targets.shape\n",
    "            loss = self.compute_loss(v_pred, tf.stop_gradient(td_targets))\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.opt.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env, iden = 0):\n",
    "        self.env = env\n",
    "        self.state_dim = self.env.observation_space.shape[0]\n",
    "        self.action_dim = self.env.action_space.shape[0]\n",
    "        self.action_bound = self.env.action_space.high[0]\n",
    "        self.std_bound = [1e-2, 1.0]\n",
    "\n",
    "        self.actor_opt = tf.keras.optimizers.Adam(wandb.config.actor_lr)\n",
    "        self.critic_opt = tf.keras.optimizers.Adam(wandb.config.critic_lr)\n",
    "        self.actor = Actor(self.state_dim, self.action_dim,\n",
    "                           self.action_bound, self.std_bound)\n",
    "        self.critic = Critic(self.state_dim)\n",
    "        \n",
    "        self.iden = iden\n",
    "\n",
    "    def gae_target(self, rewards, v_values, next_v_value, done):\n",
    "        n_step_targets = np.zeros_like(rewards)\n",
    "        gae = np.zeros_like(rewards)\n",
    "        gae_cumulative = 0\n",
    "        forward_val = 0\n",
    "\n",
    "        if not done:\n",
    "            forward_val = next_v_value\n",
    "\n",
    "        for k in reversed(range(0, len(rewards))):\n",
    "            delta = rewards[k] + wandb.config.gamma * forward_val - v_values[k]\n",
    "            gae_cumulative = wandb.config.gamma * wandb.config.lmbda * gae_cumulative + delta\n",
    "            gae[k] = gae_cumulative\n",
    "            forward_val = v_values[k]\n",
    "            n_step_targets[k] = gae[k] + v_values[k]\n",
    "        return gae, n_step_targets\n",
    "\n",
    "    def list_to_batch(self, list):\n",
    "        batch = list[0]\n",
    "        for elem in list[1:]:\n",
    "            batch = np.append(batch, elem, axis=0)\n",
    "        return batch\n",
    "\n",
    "    def train(self, max_episodes=1000):\n",
    "        for ep in range(max_episodes):\n",
    "            state_batch = []\n",
    "            action_batch = []\n",
    "            reward_batch = []\n",
    "            old_policy_batch = []\n",
    "\n",
    "            episode_reward, done = 0, False\n",
    "\n",
    "            state = self.env.reset()\n",
    "\n",
    "            while not done:\n",
    "                # self.env.render()\n",
    "                log_old_policy, action = self.actor.get_action(state)\n",
    "\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "\n",
    "                state = np.reshape(state, [1, self.state_dim])\n",
    "                action = np.reshape(action, [1, 1])\n",
    "                next_state = np.reshape(next_state, [1, self.state_dim])\n",
    "                reward = np.reshape(reward, [1, 1])\n",
    "                log_old_policy = np.reshape(log_old_policy, [1, 1])\n",
    "\n",
    "                state_batch.append(state)\n",
    "                action_batch.append(action)\n",
    "                reward_batch.append((reward+8)/8)\n",
    "                old_policy_batch.append(log_old_policy)\n",
    "\n",
    "                if len(state_batch) >= wandb.config.update_interval or done:\n",
    "                    states = self.list_to_batch(state_batch)\n",
    "                    actions = self.list_to_batch(action_batch)\n",
    "                    rewards = self.list_to_batch(reward_batch)\n",
    "                    old_policys = self.list_to_batch(old_policy_batch)\n",
    "\n",
    "                    v_values = self.critic.model.predict(states)\n",
    "                    next_v_value = self.critic.model.predict(next_state)\n",
    "\n",
    "                    gaes, td_targets = self.gae_target(\n",
    "                        rewards, v_values, next_v_value, done)\n",
    "\n",
    "                    for epoch in range(wandb.config.intervals):\n",
    "                        actor_loss = self.actor.train(\n",
    "                            old_policys, states, actions, gaes)\n",
    "                        critic_loss = self.critic.train(states, td_targets)\n",
    "\n",
    "                    state_batch = []\n",
    "                    action_batch = []\n",
    "                    reward_batch = []\n",
    "                    old_policy_batch = []\n",
    "\n",
    "                episode_reward += reward[0][0]\n",
    "                state = next_state[0]\n",
    "\n",
    "            print('EP{} EpisodeReward={}'.format(ep, episode_reward))\n",
    "            wandb.log({'Reward' + str(self.iden): episode_reward})\n",
    "        \n",
    "        return episode_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.10<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">PPO-single-long</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/victor-qin/deep-rl-tf2\" target=\"_blank\">https://wandb.ai/victor-qin/deep-rl-tf2</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/victor-qin/deep-rl-tf2/runs/3fnxst17\" target=\"_blank\">https://wandb.ai/victor-qin/deep-rl-tf2/runs/3fnxst17</a><br/>\n",
       "                Run data is saved locally in <code>/n/home05/vqin/fasrc/es100_workspace/wandb/run-20201119_232717-3fnxst17</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.99, 'update_interval': 5, 'actor_lr': 0.0005, 'critic_lr': 0.001, 'batch_size': 64, 'clip_ratio': 0.1, 'lmbda': 0.95, 'intervals': 3, 'episodes': 5, 'num': 1, 'epochs': 200, 'actor': {'layer1': 32, 'layer2': 32}, 'critic': {'layer1': 32, 'layer2': 32, 'layer3': 16}}\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1604.323542328736\n",
      "EP1 EpisodeReward=-1764.8738976768589\n",
      "EP2 EpisodeReward=-1453.0758846940448\n",
      "EP3 EpisodeReward=-1567.0979561135896\n",
      "EP4 EpisodeReward=-1574.3512868836403\n",
      "Epoch=0\t Average reward=-1574.3512868836403\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1351.4573219404592\n",
      "EP1 EpisodeReward=-1166.0763621272197\n",
      "EP2 EpisodeReward=-1507.028712534297\n",
      "EP3 EpisodeReward=-1167.1190951940046\n",
      "EP4 EpisodeReward=-1591.3551537959083\n",
      "Epoch=1\t Average reward=-1591.3551537959083\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1447.0965414388672\n",
      "EP1 EpisodeReward=-1504.6176026852056\n",
      "EP2 EpisodeReward=-1204.0246770107337\n",
      "EP3 EpisodeReward=-1522.700035715799\n",
      "EP4 EpisodeReward=-1313.6323422939813\n",
      "Epoch=2\t Average reward=-1313.6323422939813\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1440.8661905502943\n",
      "EP1 EpisodeReward=-1421.2057027865583\n",
      "EP2 EpisodeReward=-1504.1619936892869\n",
      "EP3 EpisodeReward=-1459.8996289229763\n",
      "EP4 EpisodeReward=-1478.6577326287845\n",
      "Epoch=3\t Average reward=-1478.6577326287845\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1275.558214673268\n",
      "EP1 EpisodeReward=-1126.2924878429064\n",
      "EP2 EpisodeReward=-1393.54289890827\n",
      "EP3 EpisodeReward=-1560.7907095922826\n",
      "EP4 EpisodeReward=-1432.2535761715615\n",
      "Epoch=4\t Average reward=-1432.2535761715615\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1435.960650307892\n",
      "EP1 EpisodeReward=-1292.2227947391384\n",
      "EP2 EpisodeReward=-1462.4108028059063\n",
      "EP3 EpisodeReward=-1386.9543088310422\n",
      "EP4 EpisodeReward=-1470.6588887661571\n",
      "Epoch=5\t Average reward=-1470.6588887661571\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1199.0365123894471\n",
      "EP1 EpisodeReward=-1500.99002149147\n",
      "EP2 EpisodeReward=-1418.6070530371887\n",
      "EP3 EpisodeReward=-1002.2096089667864\n",
      "EP4 EpisodeReward=-1394.9511887317067\n",
      "Epoch=6\t Average reward=-1394.9511887317067\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1499.197768448227\n",
      "EP1 EpisodeReward=-1435.1425531929174\n",
      "EP2 EpisodeReward=-1445.2946798159537\n",
      "EP3 EpisodeReward=-1302.6509109396081\n",
      "EP4 EpisodeReward=-1255.772046377169\n",
      "Epoch=7\t Average reward=-1255.772046377169\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1284.1884419516748\n",
      "EP1 EpisodeReward=-1574.0551545661533\n",
      "EP2 EpisodeReward=-1322.1387165731105\n",
      "EP3 EpisodeReward=-1524.9032215968855\n",
      "EP4 EpisodeReward=-1266.027475025864\n",
      "Epoch=8\t Average reward=-1266.027475025864\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1313.5978133239205\n",
      "EP1 EpisodeReward=-1452.8326668593936\n",
      "EP2 EpisodeReward=-1370.1242951449708\n",
      "EP3 EpisodeReward=-1539.8307125872045\n",
      "EP4 EpisodeReward=-1331.4682838774888\n",
      "Epoch=9\t Average reward=-1331.4682838774888\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1537.3280908297081\n",
      "EP1 EpisodeReward=-1547.7470950190686\n",
      "EP2 EpisodeReward=-1510.2458736827466\n",
      "EP3 EpisodeReward=-1374.3458698714878\n",
      "EP4 EpisodeReward=-1462.4789446774719\n",
      "Epoch=10\t Average reward=-1462.4789446774719\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1465.3953209255424\n",
      "EP1 EpisodeReward=-1554.8745771999372\n",
      "EP2 EpisodeReward=-1544.7009655335194\n",
      "EP3 EpisodeReward=-1396.5875835887105\n",
      "EP4 EpisodeReward=-1464.64758991453\n",
      "Epoch=11\t Average reward=-1464.64758991453\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1332.9996545276974\n",
      "EP1 EpisodeReward=-1152.6047603779493\n",
      "EP2 EpisodeReward=-1306.4518524086966\n",
      "EP3 EpisodeReward=-1533.873529217927\n",
      "EP4 EpisodeReward=-1620.1604902150077\n",
      "Epoch=12\t Average reward=-1620.1604902150077\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1329.7186611676636\n",
      "EP1 EpisodeReward=-1644.5374582176357\n",
      "EP2 EpisodeReward=-1528.184610790064\n",
      "EP3 EpisodeReward=-1511.5896301420314\n",
      "EP4 EpisodeReward=-1253.584701321973\n",
      "Epoch=13\t Average reward=-1253.584701321973\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1188.900177389159\n",
      "EP1 EpisodeReward=-1478.8594143643184\n",
      "EP2 EpisodeReward=-1452.3229201757185\n",
      "EP3 EpisodeReward=-1516.047768369527\n",
      "EP4 EpisodeReward=-1536.252622660479\n",
      "Epoch=14\t Average reward=-1536.252622660479\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1628.463733118623\n",
      "EP1 EpisodeReward=-1189.8867264635333\n",
      "EP2 EpisodeReward=-1168.7415101098063\n",
      "EP3 EpisodeReward=-1457.612343424921\n",
      "EP4 EpisodeReward=-1458.0425681422323\n",
      "Epoch=15\t Average reward=-1458.0425681422323\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1335.810446590953\n",
      "EP1 EpisodeReward=-1256.759439903171\n",
      "EP2 EpisodeReward=-1532.5008880618232\n",
      "EP3 EpisodeReward=-1639.2813487770838\n",
      "EP4 EpisodeReward=-1556.5966538956436\n",
      "Epoch=16\t Average reward=-1556.5966538956436\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1550.394485064473\n",
      "EP1 EpisodeReward=-1652.3893864043844\n",
      "EP2 EpisodeReward=-1302.4212803997768\n",
      "EP3 EpisodeReward=-1205.885108770075\n",
      "EP4 EpisodeReward=-1525.4880552986717\n",
      "Epoch=17\t Average reward=-1525.4880552986717\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1462.1201486418122\n",
      "EP1 EpisodeReward=-1452.1184735213826\n",
      "EP2 EpisodeReward=-1500.4031846607497\n",
      "EP3 EpisodeReward=-1325.8226889202083\n",
      "EP4 EpisodeReward=-1530.0483721031399\n",
      "Epoch=18\t Average reward=-1530.0483721031399\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1162.1661463503765\n",
      "EP1 EpisodeReward=-1128.949037973175\n",
      "EP2 EpisodeReward=-1207.2375376974067\n",
      "EP3 EpisodeReward=-1019.2751671898083\n",
      "EP4 EpisodeReward=-1195.818876665007\n",
      "Epoch=19\t Average reward=-1195.818876665007\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1169.8489548872828\n",
      "EP1 EpisodeReward=-1361.9828635326242\n",
      "EP2 EpisodeReward=-1155.5237763730468\n",
      "EP3 EpisodeReward=-1178.0421236958427\n",
      "EP4 EpisodeReward=-1165.9029350567143\n",
      "Epoch=20\t Average reward=-1165.9029350567143\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-927.5782117838098\n",
      "EP1 EpisodeReward=-1249.170490026476\n",
      "EP2 EpisodeReward=-1517.1813783749371\n",
      "EP3 EpisodeReward=-1383.2482641489755\n",
      "EP4 EpisodeReward=-1354.708023533308\n",
      "Epoch=21\t Average reward=-1354.708023533308\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1308.902638495959\n",
      "EP1 EpisodeReward=-1389.665626166039\n",
      "EP2 EpisodeReward=-1327.248804668577\n",
      "EP3 EpisodeReward=-1170.1801365572621\n",
      "EP4 EpisodeReward=-1345.059301090283\n",
      "Epoch=22\t Average reward=-1345.059301090283\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1293.5490953861056\n",
      "EP1 EpisodeReward=-1428.174536016296\n",
      "EP2 EpisodeReward=-1518.2080685872513\n",
      "EP3 EpisodeReward=-1341.7336896839677\n",
      "EP4 EpisodeReward=-1163.096237724807\n",
      "Epoch=23\t Average reward=-1163.096237724807\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1517.7809555675838\n",
      "EP1 EpisodeReward=-1410.7034796389046\n",
      "EP2 EpisodeReward=-1148.0365416023587\n",
      "EP3 EpisodeReward=-1506.5062095410387\n",
      "EP4 EpisodeReward=-1066.1438223747723\n",
      "Epoch=24\t Average reward=-1066.1438223747723\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1588.1426618370708\n",
      "EP1 EpisodeReward=-945.9797328488033\n",
      "EP2 EpisodeReward=-1507.6374447692015\n",
      "EP3 EpisodeReward=-1187.113825312912\n",
      "EP4 EpisodeReward=-1618.892507218165\n",
      "Epoch=25\t Average reward=-1618.892507218165\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1284.0629031805656\n",
      "EP1 EpisodeReward=-1500.0506492886589\n",
      "EP2 EpisodeReward=-1618.641303926588\n",
      "EP3 EpisodeReward=-1402.285206148121\n",
      "EP4 EpisodeReward=-1441.217412187337\n",
      "Epoch=26\t Average reward=-1441.217412187337\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1498.4956438828597\n",
      "EP1 EpisodeReward=-1414.0806009137943\n",
      "EP2 EpisodeReward=-1616.2888695848565\n",
      "EP3 EpisodeReward=-1496.0459554071087\n",
      "EP4 EpisodeReward=-1504.4682347377486\n",
      "Epoch=27\t Average reward=-1504.4682347377486\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1628.9753185543448\n",
      "EP1 EpisodeReward=-1481.905660709853\n",
      "EP2 EpisodeReward=-1212.5760227214894\n",
      "EP3 EpisodeReward=-1495.3128930916282\n",
      "EP4 EpisodeReward=-1164.4338537582892\n",
      "Epoch=28\t Average reward=-1164.4338537582892\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1497.2853642904654\n",
      "EP1 EpisodeReward=-1493.2927940893035\n",
      "EP2 EpisodeReward=-1139.9746740813318\n",
      "EP3 EpisodeReward=-1173.3759494307178\n",
      "EP4 EpisodeReward=-1093.1040932241954\n",
      "Epoch=29\t Average reward=-1093.1040932241954\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1003.8360386044983\n",
      "EP1 EpisodeReward=-1479.0384574391937\n",
      "EP2 EpisodeReward=-1638.95471453353\n",
      "EP3 EpisodeReward=-1606.7771461046948\n",
      "EP4 EpisodeReward=-1644.0874356179065\n",
      "Epoch=30\t Average reward=-1644.0874356179065\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1485.370475208935\n",
      "EP1 EpisodeReward=-1356.1472597593902\n",
      "EP2 EpisodeReward=-1524.7080545849747\n",
      "EP3 EpisodeReward=-1655.3526822004253\n",
      "EP4 EpisodeReward=-1652.0465990460775\n",
      "Epoch=31\t Average reward=-1652.0465990460775\n",
      "Training Agent 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP0 EpisodeReward=-1182.9311168156446\n",
      "EP1 EpisodeReward=-1454.1287607421423\n",
      "EP2 EpisodeReward=-1636.8647431486334\n",
      "EP3 EpisodeReward=-1053.713762417968\n",
      "EP4 EpisodeReward=-1640.231465543899\n",
      "Epoch=32\t Average reward=-1640.231465543899\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1505.3062844307055\n",
      "EP1 EpisodeReward=-1035.144828920091\n",
      "EP2 EpisodeReward=-1620.1072749974508\n",
      "EP3 EpisodeReward=-1655.1229541260973\n",
      "EP4 EpisodeReward=-1336.234417813011\n",
      "Epoch=33\t Average reward=-1336.234417813011\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1214.7538703747578\n",
      "EP1 EpisodeReward=-1247.2556329420358\n",
      "EP2 EpisodeReward=-1050.1352479481095\n",
      "EP3 EpisodeReward=-1487.8134269457678\n",
      "EP4 EpisodeReward=-1483.9670030914924\n",
      "Epoch=34\t Average reward=-1483.9670030914924\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1414.1248447521114\n",
      "EP1 EpisodeReward=-1175.284304530833\n",
      "EP2 EpisodeReward=-961.8884125569509\n",
      "EP3 EpisodeReward=-1573.9500418838877\n",
      "EP4 EpisodeReward=-1586.2460182048467\n",
      "Epoch=35\t Average reward=-1586.2460182048467\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1178.3073233775528\n",
      "EP1 EpisodeReward=-1201.4121400354697\n",
      "EP2 EpisodeReward=-1506.284252190996\n",
      "EP3 EpisodeReward=-1186.9118521658402\n",
      "EP4 EpisodeReward=-1499.6593228396202\n",
      "Epoch=36\t Average reward=-1499.6593228396202\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1634.1252029998836\n",
      "EP1 EpisodeReward=-1493.6145061920051\n",
      "EP2 EpisodeReward=-1507.1503416503976\n",
      "EP3 EpisodeReward=-1494.1066147389636\n",
      "EP4 EpisodeReward=-1514.7780124579058\n",
      "Epoch=37\t Average reward=-1514.7780124579058\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1362.2305139062985\n",
      "EP1 EpisodeReward=-1491.0731826838512\n",
      "EP2 EpisodeReward=-1411.3188986794212\n",
      "EP3 EpisodeReward=-1207.1624751332365\n",
      "EP4 EpisodeReward=-1331.2185786985872\n",
      "Epoch=38\t Average reward=-1331.2185786985872\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1282.4579531698905\n",
      "EP1 EpisodeReward=-1311.1596588020216\n",
      "EP2 EpisodeReward=-1415.0303704642026\n",
      "EP3 EpisodeReward=-972.0877507218344\n",
      "EP4 EpisodeReward=-948.7582297601701\n",
      "Epoch=39\t Average reward=-948.7582297601701\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1419.5852482128475\n",
      "EP1 EpisodeReward=-1026.0170836545992\n",
      "EP2 EpisodeReward=-1550.8279693220509\n",
      "EP3 EpisodeReward=-1296.1966547350505\n",
      "EP4 EpisodeReward=-1502.147136417237\n",
      "Epoch=40\t Average reward=-1502.147136417237\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1607.7363406933207\n",
      "EP1 EpisodeReward=-1319.204833709025\n",
      "EP2 EpisodeReward=-1496.195971312862\n",
      "EP3 EpisodeReward=-1655.4126798005495\n",
      "EP4 EpisodeReward=-1644.519177241802\n",
      "Epoch=41\t Average reward=-1644.519177241802\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1050.6643799839114\n",
      "EP1 EpisodeReward=-1656.967880553265\n",
      "EP2 EpisodeReward=-1620.033251654866\n",
      "EP3 EpisodeReward=-1373.5592919875191\n",
      "EP4 EpisodeReward=-1577.8064810067556\n",
      "Epoch=42\t Average reward=-1577.8064810067556\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1644.1712702772597\n",
      "EP1 EpisodeReward=-1654.4010018915328\n",
      "EP2 EpisodeReward=-1617.219281167445\n",
      "EP3 EpisodeReward=-1251.761420144707\n",
      "EP4 EpisodeReward=-1464.9072401922624\n",
      "Epoch=43\t Average reward=-1464.9072401922624\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1642.575673841505\n",
      "EP1 EpisodeReward=-1240.390633318402\n",
      "EP2 EpisodeReward=-1239.5524000490436\n",
      "EP3 EpisodeReward=-1504.8517497117723\n",
      "EP4 EpisodeReward=-1208.9531144074701\n",
      "Epoch=44\t Average reward=-1208.9531144074701\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1653.7679308747065\n",
      "EP1 EpisodeReward=-1654.451622406666\n",
      "EP2 EpisodeReward=-1651.2196068532926\n",
      "EP3 EpisodeReward=-1655.021216430698\n",
      "EP4 EpisodeReward=-1626.8757071522575\n",
      "Epoch=45\t Average reward=-1626.8757071522575\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1581.2501401074353\n",
      "EP1 EpisodeReward=-1367.9343226493488\n",
      "EP2 EpisodeReward=-1049.724708371175\n",
      "EP3 EpisodeReward=-1470.6285285434617\n",
      "EP4 EpisodeReward=-1631.8864869723184\n",
      "Epoch=46\t Average reward=-1631.8864869723184\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1638.8766352440807\n",
      "EP1 EpisodeReward=-954.2810927085285\n",
      "EP2 EpisodeReward=-1605.824557888809\n",
      "EP3 EpisodeReward=-1043.4002721665279\n",
      "EP4 EpisodeReward=-1657.9152761669482\n",
      "Epoch=47\t Average reward=-1657.9152761669482\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1513.5995525088827\n",
      "EP1 EpisodeReward=-1592.479513636565\n",
      "EP2 EpisodeReward=-1051.9320893145482\n",
      "EP3 EpisodeReward=-1641.0136738159686\n",
      "EP4 EpisodeReward=-1141.018310466622\n",
      "Epoch=48\t Average reward=-1141.018310466622\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1073.0028383248439\n",
      "EP1 EpisodeReward=-1646.2532285656803\n",
      "EP2 EpisodeReward=-1213.3441020483574\n",
      "EP3 EpisodeReward=-1636.9895978579818\n",
      "EP4 EpisodeReward=-1633.7445391821109\n",
      "Epoch=49\t Average reward=-1633.7445391821109\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1328.4869560905775\n",
      "EP1 EpisodeReward=-860.8087766451122\n",
      "EP2 EpisodeReward=-1190.8448794129204\n",
      "EP3 EpisodeReward=-1654.957293775333\n",
      "EP4 EpisodeReward=-1281.561938124622\n",
      "Epoch=50\t Average reward=-1281.561938124622\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1513.0009971282009\n",
      "EP1 EpisodeReward=-1652.5914122286872\n",
      "EP2 EpisodeReward=-1643.66075586297\n",
      "EP3 EpisodeReward=-1429.742418084961\n",
      "EP4 EpisodeReward=-1650.2321988616216\n",
      "Epoch=51\t Average reward=-1650.2321988616216\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1655.3796884747057\n",
      "EP1 EpisodeReward=-1063.7541750904886\n",
      "EP2 EpisodeReward=-1631.2419689551957\n",
      "EP3 EpisodeReward=-1047.8019498373333\n",
      "EP4 EpisodeReward=-964.0912642547828\n",
      "Epoch=52\t Average reward=-964.0912642547828\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1521.7790098086082\n",
      "EP1 EpisodeReward=-1469.079346954095\n",
      "EP2 EpisodeReward=-1497.3282376585114\n",
      "EP3 EpisodeReward=-1624.261407863135\n",
      "EP4 EpisodeReward=-1169.453347438746\n",
      "Epoch=53\t Average reward=-1169.453347438746\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1499.7629452575338\n",
      "EP1 EpisodeReward=-1062.277667597259\n",
      "EP2 EpisodeReward=-724.0609195028485\n",
      "EP3 EpisodeReward=-1272.6464412600528\n",
      "EP4 EpisodeReward=-1458.6730920813418\n",
      "Epoch=54\t Average reward=-1458.6730920813418\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1300.4113190172407\n",
      "EP1 EpisodeReward=-1384.3396899833995\n",
      "EP2 EpisodeReward=-1383.8425970411854\n",
      "EP3 EpisodeReward=-1528.7221008453018\n",
      "EP4 EpisodeReward=-1652.7292007012338\n",
      "Epoch=55\t Average reward=-1652.7292007012338\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1527.8510265989603\n",
      "EP1 EpisodeReward=-1530.9028441440682\n",
      "EP2 EpisodeReward=-835.74931889927\n",
      "EP3 EpisodeReward=-1599.9276941004118\n",
      "EP4 EpisodeReward=-1656.3205873060144\n",
      "Epoch=56\t Average reward=-1656.3205873060144\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1655.0830623670752\n",
      "EP1 EpisodeReward=-1462.9690489518216\n",
      "EP2 EpisodeReward=-1182.0944094680824\n",
      "EP3 EpisodeReward=-1379.1559696193838\n",
      "EP4 EpisodeReward=-1655.4362846430533\n",
      "Epoch=57\t Average reward=-1655.4362846430533\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-942.0513416052127\n",
      "EP1 EpisodeReward=-1503.3087950412212\n",
      "EP2 EpisodeReward=-1051.6764739969442\n",
      "EP3 EpisodeReward=-1164.0114065878051\n",
      "EP4 EpisodeReward=-1357.2147517796545\n",
      "Epoch=58\t Average reward=-1357.2147517796545\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1483.2145728323724\n",
      "EP1 EpisodeReward=-1153.3239993446057\n",
      "EP2 EpisodeReward=-1653.947443771283\n",
      "EP3 EpisodeReward=-1482.967887018634\n",
      "EP4 EpisodeReward=-1206.069867663935\n",
      "Epoch=59\t Average reward=-1206.069867663935\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-951.6891301874147\n",
      "EP1 EpisodeReward=-1255.2694365747209\n",
      "EP2 EpisodeReward=-1386.8938873137593\n",
      "EP3 EpisodeReward=-1656.835052479415\n",
      "EP4 EpisodeReward=-1059.5585037786054\n",
      "Epoch=60\t Average reward=-1059.5585037786054\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1188.1891030484805\n",
      "EP1 EpisodeReward=-1647.5370048134996\n",
      "EP2 EpisodeReward=-1441.4389211874707\n",
      "EP3 EpisodeReward=-1469.3732388508906\n",
      "EP4 EpisodeReward=-1055.3494048353075\n",
      "Epoch=61\t Average reward=-1055.3494048353075\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1503.8503781731115\n",
      "EP1 EpisodeReward=-1651.9505296924183\n",
      "EP2 EpisodeReward=-1498.202301136759\n",
      "EP3 EpisodeReward=-1518.1583443247866\n",
      "EP4 EpisodeReward=-1177.611049261867\n",
      "Epoch=62\t Average reward=-1177.611049261867\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1334.094522753939\n",
      "EP1 EpisodeReward=-1523.450346032048\n",
      "EP2 EpisodeReward=-1648.000699036983\n",
      "EP3 EpisodeReward=-1616.2446197646182\n",
      "EP4 EpisodeReward=-1225.652111880002\n",
      "Epoch=63\t Average reward=-1225.652111880002\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1646.5962536955017\n",
      "EP1 EpisodeReward=-1652.1318207507231\n",
      "EP2 EpisodeReward=-1189.9957694332672\n",
      "EP3 EpisodeReward=-1301.3439584643754\n",
      "EP4 EpisodeReward=-1601.5110062220292\n",
      "Epoch=64\t Average reward=-1601.5110062220292\n",
      "Training Agent 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP0 EpisodeReward=-1644.8578350669875\n",
      "EP1 EpisodeReward=-1495.3176148492623\n",
      "EP2 EpisodeReward=-1494.8635967847604\n",
      "EP3 EpisodeReward=-1505.692886933457\n",
      "EP4 EpisodeReward=-1626.9483827933284\n",
      "Epoch=65\t Average reward=-1626.9483827933284\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1158.5290668561324\n",
      "EP1 EpisodeReward=-1493.7238997303261\n",
      "EP2 EpisodeReward=-1641.7696212780793\n",
      "EP3 EpisodeReward=-1627.3284955485485\n",
      "EP4 EpisodeReward=-1585.4976105531568\n",
      "Epoch=66\t Average reward=-1585.4976105531568\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1493.5507085466486\n",
      "EP1 EpisodeReward=-1515.918555809698\n",
      "EP2 EpisodeReward=-1654.4947006519476\n",
      "EP3 EpisodeReward=-1497.1618781698237\n",
      "EP4 EpisodeReward=-972.5273339183657\n",
      "Epoch=67\t Average reward=-972.5273339183657\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1567.362940635568\n",
      "EP1 EpisodeReward=-1459.149750821978\n",
      "EP2 EpisodeReward=-1621.4048984939466\n",
      "EP3 EpisodeReward=-1354.1468194266022\n",
      "EP4 EpisodeReward=-1644.1728254131801\n",
      "Epoch=68\t Average reward=-1644.1728254131801\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1612.6246141512488\n",
      "EP1 EpisodeReward=-1195.346550104762\n",
      "EP2 EpisodeReward=-1495.1395084448582\n",
      "EP3 EpisodeReward=-1229.8846447671556\n",
      "EP4 EpisodeReward=-1388.9241871273346\n",
      "Epoch=69\t Average reward=-1388.9241871273346\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1423.5772482042091\n",
      "EP1 EpisodeReward=-1498.5138540833861\n",
      "EP2 EpisodeReward=-1651.8681512659314\n",
      "EP3 EpisodeReward=-1619.9741529569287\n",
      "EP4 EpisodeReward=-980.3967168759885\n",
      "Epoch=70\t Average reward=-980.3967168759885\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1644.8348961705399\n",
      "EP1 EpisodeReward=-989.4475035636726\n",
      "EP2 EpisodeReward=-1416.645208248092\n",
      "EP3 EpisodeReward=-1525.8171966719951\n",
      "EP4 EpisodeReward=-1646.1366404516907\n",
      "Epoch=71\t Average reward=-1646.1366404516907\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1271.8602793894336\n",
      "EP1 EpisodeReward=-1643.9322966637694\n",
      "EP2 EpisodeReward=-1618.0920554080215\n",
      "EP3 EpisodeReward=-1384.1720333206442\n",
      "EP4 EpisodeReward=-1043.096728468703\n",
      "Epoch=72\t Average reward=-1043.096728468703\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1170.7293850031335\n",
      "EP1 EpisodeReward=-1284.5780658468864\n",
      "EP2 EpisodeReward=-1651.9983023327225\n",
      "EP3 EpisodeReward=-1523.6093601016219\n",
      "EP4 EpisodeReward=-1647.2543832646093\n",
      "Epoch=73\t Average reward=-1647.2543832646093\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1247.014900552752\n",
      "EP1 EpisodeReward=-1568.7656414954588\n",
      "EP2 EpisodeReward=-1496.0169837557983\n",
      "EP3 EpisodeReward=-1350.618155044724\n",
      "EP4 EpisodeReward=-1080.252826775691\n",
      "Epoch=74\t Average reward=-1080.252826775691\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1552.532237943133\n",
      "EP1 EpisodeReward=-1653.4277089379298\n",
      "EP2 EpisodeReward=-1384.4631604182935\n",
      "EP3 EpisodeReward=-1519.0555672199903\n",
      "EP4 EpisodeReward=-1227.9934386360258\n",
      "Epoch=75\t Average reward=-1227.9934386360258\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1640.7973904633766\n",
      "EP1 EpisodeReward=-1645.6353829445384\n",
      "EP2 EpisodeReward=-1297.8665429024281\n",
      "EP3 EpisodeReward=-932.224769133927\n",
      "EP4 EpisodeReward=-1512.7056997473678\n",
      "Epoch=76\t Average reward=-1512.7056997473678\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1497.9906731299484\n",
      "EP1 EpisodeReward=-1059.9833410945232\n",
      "EP2 EpisodeReward=-1172.434789788147\n",
      "EP3 EpisodeReward=-1065.5207212562057\n",
      "EP4 EpisodeReward=-1156.7580096398808\n",
      "Epoch=77\t Average reward=-1156.7580096398808\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1490.366564035578\n",
      "EP1 EpisodeReward=-1367.5233339631686\n",
      "EP2 EpisodeReward=-1259.0061931346659\n",
      "EP3 EpisodeReward=-1015.4333812433647\n",
      "EP4 EpisodeReward=-1477.9788215179512\n",
      "Epoch=78\t Average reward=-1477.9788215179512\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1646.1598275633983\n",
      "EP1 EpisodeReward=-1639.549483604986\n",
      "EP2 EpisodeReward=-1284.2532000527422\n",
      "EP3 EpisodeReward=-1645.4248213596024\n",
      "EP4 EpisodeReward=-1638.4375980028915\n",
      "Epoch=79\t Average reward=-1638.4375980028915\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1515.766384442876\n",
      "EP1 EpisodeReward=-959.7766399760282\n",
      "EP2 EpisodeReward=-1366.0561875702415\n",
      "EP3 EpisodeReward=-1185.453638967708\n",
      "EP4 EpisodeReward=-1188.5818915224988\n",
      "Epoch=80\t Average reward=-1188.5818915224988\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1290.774503373508\n",
      "EP1 EpisodeReward=-1514.7807105563604\n",
      "EP2 EpisodeReward=-1293.794212808194\n",
      "EP3 EpisodeReward=-1510.7703976571736\n",
      "EP4 EpisodeReward=-1601.8442139070014\n",
      "Epoch=81\t Average reward=-1601.8442139070014\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1495.968774994009\n",
      "EP1 EpisodeReward=-1360.1014234205823\n",
      "EP2 EpisodeReward=-1647.6787764737524\n",
      "EP3 EpisodeReward=-1495.9433986460822\n",
      "EP4 EpisodeReward=-1515.8341515522814\n",
      "Epoch=82\t Average reward=-1515.8341515522814\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1638.051989951528\n",
      "EP1 EpisodeReward=-1657.4136749585782\n",
      "EP2 EpisodeReward=-997.3323182553974\n",
      "EP3 EpisodeReward=-1324.1211768285427\n",
      "EP4 EpisodeReward=-1640.629988255367\n",
      "Epoch=83\t Average reward=-1640.629988255367\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1634.5488835043816\n",
      "EP1 EpisodeReward=-1640.1733355888275\n",
      "EP2 EpisodeReward=-1230.6930547222762\n",
      "EP3 EpisodeReward=-1619.7017791451287\n",
      "EP4 EpisodeReward=-1061.5950940084201\n",
      "Epoch=84\t Average reward=-1061.5950940084201\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-946.9423202331847\n",
      "EP1 EpisodeReward=-832.2138101828924\n",
      "EP2 EpisodeReward=-1314.24458811409\n",
      "EP3 EpisodeReward=-1258.8288482775013\n",
      "EP4 EpisodeReward=-1276.513659160835\n",
      "Epoch=85\t Average reward=-1276.513659160835\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1162.9096326549336\n",
      "EP1 EpisodeReward=-1491.223697137042\n",
      "EP2 EpisodeReward=-940.2888342421102\n",
      "EP3 EpisodeReward=-1246.439517224893\n",
      "EP4 EpisodeReward=-960.3248801307667\n",
      "Epoch=86\t Average reward=-960.3248801307667\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1514.2775982817982\n",
      "EP1 EpisodeReward=-1232.2429047679489\n",
      "EP2 EpisodeReward=-1225.5048129621869\n",
      "EP3 EpisodeReward=-1501.8150155087178\n",
      "EP4 EpisodeReward=-1350.924701144864\n",
      "Epoch=87\t Average reward=-1350.924701144864\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1409.628453670844\n",
      "EP1 EpisodeReward=-1376.4942088087662\n",
      "EP2 EpisodeReward=-1161.5785045231341\n",
      "EP3 EpisodeReward=-1173.3137974741167\n",
      "EP4 EpisodeReward=-1503.6857566019194\n",
      "Epoch=88\t Average reward=-1503.6857566019194\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1047.5010419722478\n",
      "EP1 EpisodeReward=-1490.6995756352403\n",
      "EP2 EpisodeReward=-1168.7074042912081\n",
      "EP3 EpisodeReward=-1599.9097898194289\n",
      "EP4 EpisodeReward=-1514.4328476691544\n",
      "Epoch=89\t Average reward=-1514.4328476691544\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-966.1488355713968\n",
      "EP1 EpisodeReward=-1635.1056547615367\n",
      "EP2 EpisodeReward=-1144.8996766728033\n",
      "EP3 EpisodeReward=-1486.5705178741257\n",
      "EP4 EpisodeReward=-1570.335334894174\n",
      "Epoch=90\t Average reward=-1570.335334894174\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1316.3278610494094\n",
      "EP1 EpisodeReward=-1377.8477706500937\n",
      "EP2 EpisodeReward=-1258.0155000528434\n",
      "EP3 EpisodeReward=-1381.2890402779515\n",
      "EP4 EpisodeReward=-1386.8256883524164\n",
      "Epoch=91\t Average reward=-1386.8256883524164\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1519.8609760763657\n",
      "EP1 EpisodeReward=-946.0488382453022\n",
      "EP2 EpisodeReward=-1522.3150468578065\n",
      "EP3 EpisodeReward=-1646.0121860471568\n",
      "EP4 EpisodeReward=-941.3153695288354\n",
      "Epoch=92\t Average reward=-941.3153695288354\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1346.2726922791517\n",
      "EP1 EpisodeReward=-984.9476474380788\n",
      "EP2 EpisodeReward=-1494.880161564837\n",
      "EP3 EpisodeReward=-1221.0662925677432\n",
      "EP4 EpisodeReward=-1649.777455225442\n",
      "Epoch=93\t Average reward=-1649.777455225442\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1567.8195955572257\n",
      "EP1 EpisodeReward=-1516.6582868954076\n",
      "EP2 EpisodeReward=-1182.5739285291672\n",
      "EP3 EpisodeReward=-1523.0817726414587\n",
      "EP4 EpisodeReward=-949.0088145011474\n",
      "Epoch=94\t Average reward=-949.0088145011474\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1521.4640046815175\n",
      "EP1 EpisodeReward=-966.5396619201667\n",
      "EP2 EpisodeReward=-946.8132331855994\n",
      "EP3 EpisodeReward=-1497.0913427035657\n",
      "EP4 EpisodeReward=-1323.1712204349249\n",
      "Epoch=95\t Average reward=-1323.1712204349249\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1291.7246425281514\n",
      "EP1 EpisodeReward=-1109.3273169844394\n",
      "EP2 EpisodeReward=-1503.141297732057\n",
      "EP3 EpisodeReward=-1497.0741257407701\n",
      "EP4 EpisodeReward=-1257.393909991059\n",
      "Epoch=96\t Average reward=-1257.393909991059\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1185.097835983297\n",
      "EP1 EpisodeReward=-1023.6673870046787\n",
      "EP2 EpisodeReward=-1030.5128227089938\n",
      "EP3 EpisodeReward=-1259.21596534332\n",
      "EP4 EpisodeReward=-1534.199891974562\n",
      "Epoch=97\t Average reward=-1534.199891974562\n",
      "Training Agent 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP0 EpisodeReward=-945.6973279776801\n",
      "EP1 EpisodeReward=-1419.4920076657784\n",
      "EP2 EpisodeReward=-1373.6615417569349\n",
      "EP3 EpisodeReward=-1496.593649081974\n",
      "EP4 EpisodeReward=-988.8738288516751\n",
      "Epoch=98\t Average reward=-988.8738288516751\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1556.7815596327075\n",
      "EP1 EpisodeReward=-1492.9129868157092\n",
      "EP2 EpisodeReward=-1175.522051807821\n",
      "EP3 EpisodeReward=-960.3938420533602\n",
      "EP4 EpisodeReward=-1267.515658960753\n",
      "Epoch=99\t Average reward=-1267.515658960753\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-959.2866756063029\n",
      "EP1 EpisodeReward=-1645.5506873027064\n",
      "EP2 EpisodeReward=-1201.7088477433113\n",
      "EP3 EpisodeReward=-1592.1164819513658\n",
      "EP4 EpisodeReward=-1651.7524956041398\n",
      "Epoch=100\t Average reward=-1651.7524956041398\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1079.9189621238834\n",
      "EP1 EpisodeReward=-1497.9184965009988\n",
      "EP2 EpisodeReward=-1251.1047614224342\n",
      "EP3 EpisodeReward=-1646.9602126976104\n",
      "EP4 EpisodeReward=-1649.4307191046694\n",
      "Epoch=101\t Average reward=-1649.4307191046694\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1501.685501027974\n",
      "EP1 EpisodeReward=-950.8550436743888\n",
      "EP2 EpisodeReward=-1634.4127102222117\n",
      "EP3 EpisodeReward=-1513.342596030442\n",
      "EP4 EpisodeReward=-1597.3854825392116\n",
      "Epoch=102\t Average reward=-1597.3854825392116\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1328.5146150458836\n",
      "EP1 EpisodeReward=-1183.956809319277\n",
      "EP2 EpisodeReward=-968.7555181084102\n",
      "EP3 EpisodeReward=-1502.2235673684495\n",
      "EP4 EpisodeReward=-1129.8085608936306\n",
      "Epoch=103\t Average reward=-1129.8085608936306\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1438.066966499108\n",
      "EP1 EpisodeReward=-1309.8872400112084\n",
      "EP2 EpisodeReward=-1495.6045790366745\n",
      "EP3 EpisodeReward=-759.6089746378826\n",
      "EP4 EpisodeReward=-999.2563955769331\n",
      "Epoch=104\t Average reward=-999.2563955769331\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1481.6060002850115\n",
      "EP1 EpisodeReward=-1636.3957791253083\n",
      "EP2 EpisodeReward=-1648.007004171268\n",
      "EP3 EpisodeReward=-1418.4393279653311\n",
      "EP4 EpisodeReward=-1214.3110019712974\n",
      "Epoch=105\t Average reward=-1214.3110019712974\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-945.3221744006261\n",
      "EP1 EpisodeReward=-1504.1149860363766\n",
      "EP2 EpisodeReward=-1513.9351295094211\n",
      "EP3 EpisodeReward=-1207.231013303209\n",
      "EP4 EpisodeReward=-1187.4008182119821\n",
      "Epoch=106\t Average reward=-1187.4008182119821\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1183.0748450748636\n",
      "EP1 EpisodeReward=-1462.0191591771381\n",
      "EP2 EpisodeReward=-1416.7153060193027\n",
      "EP3 EpisodeReward=-933.371574815185\n",
      "EP4 EpisodeReward=-1654.2239592108685\n",
      "Epoch=107\t Average reward=-1654.2239592108685\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1187.4636236733718\n",
      "EP1 EpisodeReward=-951.3819708386721\n",
      "EP2 EpisodeReward=-956.296690166117\n",
      "EP3 EpisodeReward=-1645.8073051389842\n",
      "EP4 EpisodeReward=-952.3992820804705\n",
      "Epoch=108\t Average reward=-952.3992820804705\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1494.550919317444\n",
      "EP1 EpisodeReward=-1631.5901234051953\n",
      "EP2 EpisodeReward=-953.9389441698842\n",
      "EP3 EpisodeReward=-1190.4749703026669\n",
      "EP4 EpisodeReward=-953.9923150849921\n",
      "Epoch=109\t Average reward=-953.9923150849921\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-948.2613996237501\n",
      "EP1 EpisodeReward=-1628.0536735339213\n",
      "EP2 EpisodeReward=-944.7210629633026\n",
      "EP3 EpisodeReward=-1515.2723765650246\n",
      "EP4 EpisodeReward=-1639.7039231051792\n",
      "Epoch=110\t Average reward=-1639.7039231051792\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-949.6273843496526\n",
      "EP1 EpisodeReward=-1495.6545594933084\n",
      "EP2 EpisodeReward=-1517.301155699474\n",
      "EP3 EpisodeReward=-1358.759783784345\n",
      "EP4 EpisodeReward=-1363.3074887111145\n",
      "Epoch=111\t Average reward=-1363.3074887111145\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1432.848032142778\n",
      "EP1 EpisodeReward=-1493.2254985813122\n",
      "EP2 EpisodeReward=-1165.3238264647068\n",
      "EP3 EpisodeReward=-1657.0056230521857\n",
      "EP4 EpisodeReward=-1650.6026211977435\n",
      "Epoch=112\t Average reward=-1650.6026211977435\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1498.7218590310822\n",
      "EP1 EpisodeReward=-1546.002023247902\n",
      "EP2 EpisodeReward=-1648.7104114322813\n",
      "EP3 EpisodeReward=-1410.2565876129968\n",
      "EP4 EpisodeReward=-1654.1208942784128\n",
      "Epoch=113\t Average reward=-1654.1208942784128\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1492.7469328743503\n",
      "EP1 EpisodeReward=-1337.6505504030326\n",
      "EP2 EpisodeReward=-1348.5284178675633\n",
      "EP3 EpisodeReward=-944.9797993194114\n",
      "EP4 EpisodeReward=-1492.905395441491\n",
      "Epoch=114\t Average reward=-1492.905395441491\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1503.0299915973706\n",
      "EP1 EpisodeReward=-958.4114382828515\n",
      "EP2 EpisodeReward=-1653.386258971706\n",
      "EP3 EpisodeReward=-1638.0421954339902\n",
      "EP4 EpisodeReward=-1044.9307037196131\n",
      "Epoch=115\t Average reward=-1044.9307037196131\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1224.7550969742058\n",
      "EP1 EpisodeReward=-1636.5146122147737\n",
      "EP2 EpisodeReward=-1193.9926291025631\n",
      "EP3 EpisodeReward=-1511.934906383865\n",
      "EP4 EpisodeReward=-1096.9870923398628\n",
      "Epoch=116\t Average reward=-1096.9870923398628\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1351.5667480983122\n",
      "EP1 EpisodeReward=-1492.4261296348793\n",
      "EP2 EpisodeReward=-1194.961599054949\n",
      "EP3 EpisodeReward=-943.2765990797766\n",
      "EP4 EpisodeReward=-1106.9325049557053\n",
      "Epoch=117\t Average reward=-1106.9325049557053\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1342.284166137027\n",
      "EP1 EpisodeReward=-1384.0948750643208\n",
      "EP2 EpisodeReward=-936.0152982639424\n",
      "EP3 EpisodeReward=-1640.0313680062866\n",
      "EP4 EpisodeReward=-1327.2150925189114\n",
      "Epoch=118\t Average reward=-1327.2150925189114\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-947.6856396741221\n",
      "EP1 EpisodeReward=-1499.7748794574632\n",
      "EP2 EpisodeReward=-1620.712666788193\n",
      "EP3 EpisodeReward=-1354.9104341464586\n",
      "EP4 EpisodeReward=-1529.6284008477285\n",
      "Epoch=119\t Average reward=-1529.6284008477285\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-973.4974620409504\n",
      "EP1 EpisodeReward=-1079.047046676833\n",
      "EP2 EpisodeReward=-944.9520815339957\n",
      "EP3 EpisodeReward=-1632.240692212467\n",
      "EP4 EpisodeReward=-1653.593500876727\n",
      "Epoch=120\t Average reward=-1653.593500876727\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1582.761101672499\n",
      "EP1 EpisodeReward=-1503.3536266763308\n",
      "EP2 EpisodeReward=-1516.8072186514319\n",
      "EP3 EpisodeReward=-1078.5463988097617\n",
      "EP4 EpisodeReward=-1508.5891537192124\n",
      "Epoch=121\t Average reward=-1508.5891537192124\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1102.3406721835943\n",
      "EP1 EpisodeReward=-1571.7007530360609\n",
      "EP2 EpisodeReward=-1635.7374795328292\n",
      "EP3 EpisodeReward=-1504.5273837749157\n",
      "EP4 EpisodeReward=-1435.6229157302905\n",
      "Epoch=122\t Average reward=-1435.6229157302905\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1421.1416441397441\n",
      "EP1 EpisodeReward=-1466.7482972070077\n",
      "EP2 EpisodeReward=-1572.1476073510378\n",
      "EP3 EpisodeReward=-1652.461801697138\n",
      "EP4 EpisodeReward=-1080.3499862409626\n",
      "Epoch=123\t Average reward=-1080.3499862409626\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1392.2500638467177\n",
      "EP1 EpisodeReward=-1354.0525816630786\n",
      "EP2 EpisodeReward=-1606.8029066929814\n",
      "EP3 EpisodeReward=-1407.2361177920982\n",
      "EP4 EpisodeReward=-956.9503190599287\n",
      "Epoch=124\t Average reward=-956.9503190599287\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1492.5109287600108\n",
      "EP1 EpisodeReward=-1494.8038363311352\n",
      "EP2 EpisodeReward=-1092.4205584536821\n",
      "EP3 EpisodeReward=-1657.8846087415513\n",
      "EP4 EpisodeReward=-1415.683223877568\n",
      "Epoch=125\t Average reward=-1415.683223877568\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1645.6651551530053\n",
      "EP1 EpisodeReward=-1655.760944727628\n",
      "EP2 EpisodeReward=-1456.510367446992\n",
      "EP3 EpisodeReward=-1606.897174478162\n",
      "EP4 EpisodeReward=-707.7683613720691\n",
      "Epoch=126\t Average reward=-707.7683613720691\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1281.3237985904154\n",
      "EP1 EpisodeReward=-1484.9288241803454\n",
      "EP2 EpisodeReward=-1094.1453338540814\n",
      "EP3 EpisodeReward=-1653.7703681907367\n",
      "EP4 EpisodeReward=-1106.5443479430137\n",
      "Epoch=127\t Average reward=-1106.5443479430137\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-938.382865235603\n",
      "EP1 EpisodeReward=-1561.415545920763\n",
      "EP2 EpisodeReward=-1517.8608107685793\n",
      "EP3 EpisodeReward=-1494.7432353089953\n",
      "EP4 EpisodeReward=-1641.4110122940654\n",
      "Epoch=128\t Average reward=-1641.4110122940654\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1261.9943276119732\n",
      "EP1 EpisodeReward=-1356.5496663597523\n",
      "EP2 EpisodeReward=-1644.6293370972412\n",
      "EP3 EpisodeReward=-1635.9893892069654\n",
      "EP4 EpisodeReward=-1478.6276489798581\n",
      "Epoch=129\t Average reward=-1478.6276489798581\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1244.0801598898433\n",
      "EP1 EpisodeReward=-1650.9037010027125\n",
      "EP2 EpisodeReward=-1561.0731849712552\n",
      "EP3 EpisodeReward=-1630.4673510283606\n",
      "EP4 EpisodeReward=-1630.2164212474358\n",
      "Epoch=130\t Average reward=-1630.2164212474358\n",
      "Training Agent 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP0 EpisodeReward=-1650.1811031311315\n",
      "EP1 EpisodeReward=-1635.272195990819\n",
      "EP2 EpisodeReward=-823.5640747342078\n",
      "EP3 EpisodeReward=-1619.8388053387716\n",
      "EP4 EpisodeReward=-831.3645909131121\n",
      "Epoch=131\t Average reward=-831.3645909131121\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1279.188171131542\n",
      "EP1 EpisodeReward=-1615.616249684792\n",
      "EP2 EpisodeReward=-1409.9552413474926\n",
      "EP3 EpisodeReward=-1627.4101275882044\n",
      "EP4 EpisodeReward=-1339.3613990148065\n",
      "Epoch=132\t Average reward=-1339.3613990148065\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1504.026678353535\n",
      "EP1 EpisodeReward=-1391.1695146618918\n",
      "EP2 EpisodeReward=-1393.6832066482482\n",
      "EP3 EpisodeReward=-1633.1625469205767\n",
      "EP4 EpisodeReward=-1657.246842615955\n",
      "Epoch=133\t Average reward=-1657.246842615955\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1240.4820676077288\n",
      "EP1 EpisodeReward=-1655.1667547097052\n",
      "EP2 EpisodeReward=-1334.317767430322\n",
      "EP3 EpisodeReward=-1519.503088778884\n",
      "EP4 EpisodeReward=-1498.5391944949565\n",
      "Epoch=134\t Average reward=-1498.5391944949565\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1340.2262157852049\n",
      "EP1 EpisodeReward=-1190.2254838933047\n",
      "EP2 EpisodeReward=-1499.7091487222312\n",
      "EP3 EpisodeReward=-1491.627311527966\n",
      "EP4 EpisodeReward=-1650.4133412442554\n",
      "Epoch=135\t Average reward=-1650.4133412442554\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1633.0048144261366\n",
      "EP1 EpisodeReward=-1047.263254653074\n",
      "EP2 EpisodeReward=-1509.285839207181\n",
      "EP3 EpisodeReward=-1646.7828417515686\n",
      "EP4 EpisodeReward=-1549.8669163377433\n",
      "Epoch=136\t Average reward=-1549.8669163377433\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1471.6889817706945\n",
      "EP1 EpisodeReward=-1193.405049849014\n",
      "EP2 EpisodeReward=-953.5135825725179\n",
      "EP3 EpisodeReward=-1493.462596253631\n",
      "EP4 EpisodeReward=-1642.1752384518193\n",
      "Epoch=137\t Average reward=-1642.1752384518193\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1334.8165570513306\n",
      "EP1 EpisodeReward=-1326.1221608697244\n",
      "EP2 EpisodeReward=-1504.7017952317074\n",
      "EP3 EpisodeReward=-1647.5582025122208\n",
      "EP4 EpisodeReward=-1613.3112084310574\n",
      "Epoch=138\t Average reward=-1613.3112084310574\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1648.8869427892678\n",
      "EP1 EpisodeReward=-1369.8219540660857\n",
      "EP2 EpisodeReward=-731.3363895671201\n",
      "EP3 EpisodeReward=-1156.113141171185\n",
      "EP4 EpisodeReward=-1656.3351914085192\n",
      "Epoch=139\t Average reward=-1656.3351914085192\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1643.279267939092\n",
      "EP1 EpisodeReward=-1456.9184055764108\n",
      "EP2 EpisodeReward=-1518.7195191116302\n",
      "EP3 EpisodeReward=-1652.8990531745715\n",
      "EP4 EpisodeReward=-1657.197624252227\n",
      "Epoch=140\t Average reward=-1657.197624252227\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1641.1877236453865\n",
      "EP1 EpisodeReward=-1394.7823225567931\n",
      "EP2 EpisodeReward=-1650.760248268072\n",
      "EP3 EpisodeReward=-1381.1587256288553\n",
      "EP4 EpisodeReward=-1266.7753857951598\n",
      "Epoch=141\t Average reward=-1266.7753857951598\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1442.8002954813153\n",
      "EP1 EpisodeReward=-1650.350737337745\n",
      "EP2 EpisodeReward=-1495.8324569323288\n",
      "EP3 EpisodeReward=-951.3315560004409\n",
      "EP4 EpisodeReward=-1629.0998328944634\n",
      "Epoch=142\t Average reward=-1629.0998328944634\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1431.8343972605678\n",
      "EP1 EpisodeReward=-1552.2169643115892\n",
      "EP2 EpisodeReward=-1649.9222696764552\n",
      "EP3 EpisodeReward=-1270.8944565263475\n",
      "EP4 EpisodeReward=-1291.913628303806\n",
      "Epoch=143\t Average reward=-1291.913628303806\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1501.642731296268\n",
      "EP1 EpisodeReward=-1460.034409582241\n",
      "EP2 EpisodeReward=-1495.6340043737264\n",
      "EP3 EpisodeReward=-1646.919342182475\n",
      "EP4 EpisodeReward=-1492.2631025354142\n",
      "Epoch=144\t Average reward=-1492.2631025354142\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1643.7554447145735\n",
      "EP1 EpisodeReward=-1498.1556951781788\n",
      "EP2 EpisodeReward=-1390.7227327141582\n",
      "EP3 EpisodeReward=-1263.282648369815\n",
      "EP4 EpisodeReward=-1598.2629707103833\n",
      "Epoch=145\t Average reward=-1598.2629707103833\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1642.4125865989422\n",
      "EP1 EpisodeReward=-1423.4973228950355\n",
      "EP2 EpisodeReward=-1500.4464110236377\n",
      "EP3 EpisodeReward=-1650.6507877450038\n",
      "EP4 EpisodeReward=-1493.091146168507\n",
      "Epoch=146\t Average reward=-1493.091146168507\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1492.8444823005166\n",
      "EP1 EpisodeReward=-1315.753305886449\n",
      "EP2 EpisodeReward=-947.229017493639\n",
      "EP3 EpisodeReward=-1258.6799433313122\n",
      "EP4 EpisodeReward=-1646.5135600456058\n",
      "Epoch=147\t Average reward=-1646.5135600456058\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1570.4148242067681\n",
      "EP1 EpisodeReward=-1566.581948148806\n",
      "EP2 EpisodeReward=-1132.8619138878666\n",
      "EP3 EpisodeReward=-1333.9534481013613\n",
      "EP4 EpisodeReward=-1498.6683938563276\n",
      "Epoch=148\t Average reward=-1498.6683938563276\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1544.747270988939\n",
      "EP1 EpisodeReward=-1639.1045302360305\n",
      "EP2 EpisodeReward=-1346.2945268197836\n",
      "EP3 EpisodeReward=-1464.8249911787009\n",
      "EP4 EpisodeReward=-1437.5975322145628\n",
      "Epoch=149\t Average reward=-1437.5975322145628\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1405.8730176984209\n",
      "EP1 EpisodeReward=-937.1281085057846\n",
      "EP2 EpisodeReward=-1191.1964335294863\n",
      "EP3 EpisodeReward=-1238.6950003499855\n",
      "EP4 EpisodeReward=-954.062935584798\n",
      "Epoch=150\t Average reward=-954.062935584798\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1521.6439610243738\n",
      "EP1 EpisodeReward=-1502.393041036945\n",
      "EP2 EpisodeReward=-1271.5424707608306\n",
      "EP3 EpisodeReward=-1218.8030578084706\n",
      "EP4 EpisodeReward=-1621.9102970636063\n",
      "Epoch=151\t Average reward=-1621.9102970636063\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-721.5876683451645\n",
      "EP1 EpisodeReward=-1007.8302166196406\n",
      "EP2 EpisodeReward=-1515.8418475703154\n",
      "EP3 EpisodeReward=-1593.3491377429407\n",
      "EP4 EpisodeReward=-1185.2898462829396\n",
      "Epoch=152\t Average reward=-1185.2898462829396\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1480.9374800028738\n",
      "EP1 EpisodeReward=-1461.5361655845954\n",
      "EP2 EpisodeReward=-1000.6458999293993\n",
      "EP3 EpisodeReward=-1653.1960251978107\n",
      "EP4 EpisodeReward=-1652.5731711729536\n",
      "Epoch=153\t Average reward=-1652.5731711729536\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1656.5504287060282\n",
      "EP1 EpisodeReward=-1014.2050157077725\n",
      "EP2 EpisodeReward=-1493.9738676119418\n",
      "EP3 EpisodeReward=-1656.8767339587553\n",
      "EP4 EpisodeReward=-1393.0825163500263\n",
      "Epoch=154\t Average reward=-1393.0825163500263\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1021.8624567355785\n",
      "EP1 EpisodeReward=-1491.181434099075\n",
      "EP2 EpisodeReward=-1191.4182787337465\n",
      "EP3 EpisodeReward=-1040.7741555369425\n",
      "EP4 EpisodeReward=-1358.3498117226588\n",
      "Epoch=155\t Average reward=-1358.3498117226588\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1404.221303833363\n",
      "EP1 EpisodeReward=-1502.2234742895455\n",
      "EP2 EpisodeReward=-1492.7089393065082\n",
      "EP3 EpisodeReward=-1656.6300885713654\n",
      "EP4 EpisodeReward=-1495.747242860366\n",
      "Epoch=156\t Average reward=-1495.747242860366\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1570.4015853492022\n",
      "EP1 EpisodeReward=-1651.2832834944663\n",
      "EP2 EpisodeReward=-1642.676114853091\n",
      "EP3 EpisodeReward=-1239.7461236254205\n",
      "EP4 EpisodeReward=-1330.350347495608\n",
      "Epoch=157\t Average reward=-1330.350347495608\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1641.1387095467057\n",
      "EP1 EpisodeReward=-1459.522349946148\n",
      "EP2 EpisodeReward=-1652.6225678641104\n",
      "EP3 EpisodeReward=-1656.4922263993085\n",
      "EP4 EpisodeReward=-1377.0483043846116\n",
      "Epoch=158\t Average reward=-1377.0483043846116\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-824.5811137360652\n",
      "EP1 EpisodeReward=-1593.020982442716\n",
      "EP2 EpisodeReward=-1475.3712301562089\n",
      "EP3 EpisodeReward=-1202.3636532676005\n",
      "EP4 EpisodeReward=-1234.137609015941\n",
      "Epoch=159\t Average reward=-1234.137609015941\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1481.6946960231205\n",
      "EP1 EpisodeReward=-1232.841587425714\n",
      "EP2 EpisodeReward=-1075.273393922286\n",
      "EP3 EpisodeReward=-1636.81049037192\n",
      "EP4 EpisodeReward=-1640.0787954043483\n",
      "Epoch=160\t Average reward=-1640.0787954043483\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1413.902795018086\n",
      "EP1 EpisodeReward=-1468.7441835847212\n",
      "EP2 EpisodeReward=-955.1670719340242\n",
      "EP3 EpisodeReward=-1218.5019275816642\n",
      "EP4 EpisodeReward=-1498.1070429576448\n",
      "Epoch=161\t Average reward=-1498.1070429576448\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1495.555990666677\n",
      "EP1 EpisodeReward=-1649.7171511306472\n",
      "EP2 EpisodeReward=-1064.702802879235\n",
      "EP3 EpisodeReward=-1064.713510596311\n",
      "EP4 EpisodeReward=-1617.7199792653853\n",
      "Epoch=162\t Average reward=-1617.7199792653853\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1655.979590712606\n",
      "EP1 EpisodeReward=-1095.3332785459697\n",
      "EP2 EpisodeReward=-1404.8628457136651\n",
      "EP3 EpisodeReward=-721.0015661393417\n",
      "EP4 EpisodeReward=-1522.8295926142312\n",
      "Epoch=163\t Average reward=-1522.8295926142312\n",
      "Training Agent 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP0 EpisodeReward=-1265.8384804667905\n",
      "EP1 EpisodeReward=-1459.0097886871572\n",
      "EP2 EpisodeReward=-1551.0783660659667\n",
      "EP3 EpisodeReward=-1490.9796059815644\n",
      "EP4 EpisodeReward=-1197.774197836602\n",
      "Epoch=164\t Average reward=-1197.774197836602\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1571.7377870430457\n",
      "EP1 EpisodeReward=-1502.1113248148463\n",
      "EP2 EpisodeReward=-1124.580260689107\n",
      "EP3 EpisodeReward=-1497.327454622587\n",
      "EP4 EpisodeReward=-1649.080181753957\n",
      "Epoch=165\t Average reward=-1649.080181753957\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1167.3766467664116\n",
      "EP1 EpisodeReward=-1639.7962567591846\n",
      "EP2 EpisodeReward=-1019.2268912164259\n",
      "EP3 EpisodeReward=-1165.621334663276\n",
      "EP4 EpisodeReward=-1649.0390207041867\n",
      "Epoch=166\t Average reward=-1649.0390207041867\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1657.5383374776698\n",
      "EP1 EpisodeReward=-1442.1279597661583\n",
      "EP2 EpisodeReward=-1424.969982213314\n",
      "EP3 EpisodeReward=-1535.9870813343516\n",
      "EP4 EpisodeReward=-948.3177185259486\n",
      "Epoch=167\t Average reward=-948.3177185259486\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1048.0626143249192\n",
      "EP1 EpisodeReward=-1492.8978085584633\n",
      "EP2 EpisodeReward=-1491.4330286821614\n",
      "EP3 EpisodeReward=-1650.1712150929945\n",
      "EP4 EpisodeReward=-718.5002535698216\n",
      "Epoch=168\t Average reward=-718.5002535698216\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1078.7008482723065\n",
      "EP1 EpisodeReward=-1654.511449911085\n",
      "EP2 EpisodeReward=-1652.5595694761266\n",
      "EP3 EpisodeReward=-1503.5767096329394\n",
      "EP4 EpisodeReward=-1351.1483228517163\n",
      "Epoch=169\t Average reward=-1351.1483228517163\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1524.3773771271685\n",
      "EP1 EpisodeReward=-1232.1348134778632\n",
      "EP2 EpisodeReward=-1240.5428232146921\n",
      "EP3 EpisodeReward=-1628.0940819266796\n",
      "EP4 EpisodeReward=-1652.426917238216\n",
      "Epoch=170\t Average reward=-1652.426917238216\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1656.8031388481122\n",
      "EP1 EpisodeReward=-1485.6907045130433\n",
      "EP2 EpisodeReward=-1593.556583100979\n",
      "EP3 EpisodeReward=-1168.147770790253\n",
      "EP4 EpisodeReward=-1104.6705331382145\n",
      "Epoch=171\t Average reward=-1104.6705331382145\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1648.8285782738255\n",
      "EP1 EpisodeReward=-1657.0432720242616\n",
      "EP2 EpisodeReward=-1655.5655425738319\n",
      "EP3 EpisodeReward=-1438.6622861370747\n",
      "EP4 EpisodeReward=-1649.6085052367082\n",
      "Epoch=172\t Average reward=-1649.6085052367082\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1326.6577846035877\n",
      "EP1 EpisodeReward=-1111.4420871703567\n",
      "EP2 EpisodeReward=-1654.321284548273\n",
      "EP3 EpisodeReward=-1654.321460749091\n",
      "EP4 EpisodeReward=-1343.9693162105098\n",
      "Epoch=173\t Average reward=-1343.9693162105098\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1131.3786757731632\n",
      "EP1 EpisodeReward=-1499.7099620402905\n",
      "EP2 EpisodeReward=-1352.8646772613793\n",
      "EP3 EpisodeReward=-1497.9628778078786\n",
      "EP4 EpisodeReward=-1522.21472696681\n",
      "Epoch=174\t Average reward=-1522.21472696681\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1633.9347384931543\n",
      "EP1 EpisodeReward=-1381.715813932384\n",
      "EP2 EpisodeReward=-1652.8148008055991\n",
      "EP3 EpisodeReward=-1628.4862898049644\n",
      "EP4 EpisodeReward=-1172.2169187498419\n",
      "Epoch=175\t Average reward=-1172.2169187498419\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1644.3747669352258\n",
      "EP1 EpisodeReward=-1655.4554506950794\n",
      "EP2 EpisodeReward=-1656.9642147677541\n",
      "EP3 EpisodeReward=-1654.4520032413816\n",
      "EP4 EpisodeReward=-1359.814855029171\n",
      "Epoch=176\t Average reward=-1359.814855029171\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1649.2647610632457\n",
      "EP1 EpisodeReward=-1176.7231861213822\n",
      "EP2 EpisodeReward=-1210.0528800240068\n",
      "EP3 EpisodeReward=-1508.7166361939755\n",
      "EP4 EpisodeReward=-1652.9758628499787\n",
      "Epoch=177\t Average reward=-1652.9758628499787\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1441.481363102507\n",
      "EP1 EpisodeReward=-1647.4566883089065\n",
      "EP2 EpisodeReward=-1278.8401464038775\n",
      "EP3 EpisodeReward=-1219.2786989655456\n",
      "EP4 EpisodeReward=-1644.9067107798803\n",
      "Epoch=178\t Average reward=-1644.9067107798803\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1327.0506611780156\n",
      "EP1 EpisodeReward=-1658.2912392007502\n",
      "EP2 EpisodeReward=-1511.6267429121706\n",
      "EP3 EpisodeReward=-1653.1642817296888\n",
      "EP4 EpisodeReward=-1409.9793092255927\n",
      "Epoch=179\t Average reward=-1409.9793092255927\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1269.8267037140195\n",
      "EP1 EpisodeReward=-1308.7784590852093\n",
      "EP2 EpisodeReward=-1496.078967983223\n",
      "EP3 EpisodeReward=-1437.275572354054\n",
      "EP4 EpisodeReward=-1622.765172612552\n",
      "Epoch=180\t Average reward=-1622.765172612552\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1496.1969509511491\n",
      "EP1 EpisodeReward=-1460.122651759516\n",
      "EP2 EpisodeReward=-1127.4728388208334\n",
      "EP3 EpisodeReward=-1643.925364446935\n",
      "EP4 EpisodeReward=-1630.6181428117327\n",
      "Epoch=181\t Average reward=-1630.6181428117327\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1428.2151721599614\n",
      "EP1 EpisodeReward=-1633.6370365115727\n",
      "EP2 EpisodeReward=-1512.7325435977077\n",
      "EP3 EpisodeReward=-1649.7753175088058\n",
      "EP4 EpisodeReward=-953.5406195850253\n",
      "Epoch=182\t Average reward=-953.5406195850253\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1624.0238529102626\n",
      "EP1 EpisodeReward=-1062.8841140493612\n",
      "EP2 EpisodeReward=-1226.5720137386288\n",
      "EP3 EpisodeReward=-1292.3158943733065\n",
      "EP4 EpisodeReward=-1537.0072425928843\n",
      "Epoch=183\t Average reward=-1537.0072425928843\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1543.6526331493305\n",
      "EP1 EpisodeReward=-1656.2888926056414\n",
      "EP2 EpisodeReward=-1497.9455687896839\n",
      "EP3 EpisodeReward=-1365.6000371695136\n",
      "EP4 EpisodeReward=-1339.1721146955824\n",
      "Epoch=184\t Average reward=-1339.1721146955824\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1313.7956580530324\n",
      "EP1 EpisodeReward=-1496.7141170670563\n",
      "EP2 EpisodeReward=-1656.9517538522468\n",
      "EP3 EpisodeReward=-1627.224366049164\n",
      "EP4 EpisodeReward=-1653.1309466441205\n",
      "Epoch=185\t Average reward=-1653.1309466441205\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1060.3227804932092\n",
      "EP1 EpisodeReward=-1502.4940031429053\n",
      "EP2 EpisodeReward=-1510.1840984756936\n",
      "EP3 EpisodeReward=-1641.876945011418\n",
      "EP4 EpisodeReward=-1068.7230913238607\n",
      "Epoch=186\t Average reward=-1068.7230913238607\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1654.484925584041\n",
      "EP1 EpisodeReward=-1061.7400881648136\n",
      "EP2 EpisodeReward=-1631.063816802925\n",
      "EP3 EpisodeReward=-1470.1112036756758\n",
      "EP4 EpisodeReward=-962.3586434605253\n",
      "Epoch=187\t Average reward=-962.3586434605253\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1523.043901768955\n",
      "EP1 EpisodeReward=-1444.4330689024187\n",
      "EP2 EpisodeReward=-1255.9718076376648\n",
      "EP3 EpisodeReward=-1653.3442246942664\n",
      "EP4 EpisodeReward=-1166.2300336271594\n",
      "Epoch=188\t Average reward=-1166.2300336271594\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1295.0140487515794\n",
      "EP1 EpisodeReward=-1173.3515379820028\n",
      "EP2 EpisodeReward=-1617.2076312116315\n",
      "EP3 EpisodeReward=-1512.9114789465582\n",
      "EP4 EpisodeReward=-1492.9023358567852\n",
      "Epoch=189\t Average reward=-1492.9023358567852\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1646.5115397607226\n",
      "EP1 EpisodeReward=-1627.8565163207413\n",
      "EP2 EpisodeReward=-1520.8792014867129\n",
      "EP3 EpisodeReward=-1051.2621907817943\n",
      "EP4 EpisodeReward=-1640.87102387885\n",
      "Epoch=190\t Average reward=-1640.87102387885\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1488.6039709395197\n",
      "EP1 EpisodeReward=-1474.1592954507587\n",
      "EP2 EpisodeReward=-1198.5612864852442\n",
      "EP3 EpisodeReward=-1251.1146588448828\n",
      "EP4 EpisodeReward=-1386.7441791686813\n",
      "Epoch=191\t Average reward=-1386.7441791686813\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1191.215853333517\n",
      "EP1 EpisodeReward=-1638.683238022365\n",
      "EP2 EpisodeReward=-1645.7563731702837\n",
      "EP3 EpisodeReward=-1626.9648838119901\n",
      "EP4 EpisodeReward=-1287.5684819698195\n",
      "Epoch=192\t Average reward=-1287.5684819698195\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1650.7965494670166\n",
      "EP1 EpisodeReward=-1062.8205222567778\n",
      "EP2 EpisodeReward=-1656.5301910104674\n",
      "EP3 EpisodeReward=-1382.1268022409688\n",
      "EP4 EpisodeReward=-1431.9731859652723\n",
      "Epoch=193\t Average reward=-1431.9731859652723\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1625.500589846792\n",
      "EP1 EpisodeReward=-1649.5764217671715\n",
      "EP2 EpisodeReward=-1500.8344555835008\n",
      "EP3 EpisodeReward=-1437.2498280290865\n",
      "EP4 EpisodeReward=-1357.7920243261715\n",
      "Epoch=194\t Average reward=-1357.7920243261715\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1566.7661531334775\n",
      "EP1 EpisodeReward=-1650.7652743970339\n",
      "EP2 EpisodeReward=-1650.6206697208645\n",
      "EP3 EpisodeReward=-1500.360553481115\n",
      "EP4 EpisodeReward=-1378.1938595223846\n",
      "Epoch=195\t Average reward=-1378.1938595223846\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1623.126540861077\n",
      "EP1 EpisodeReward=-1165.3669336708638\n",
      "EP2 EpisodeReward=-1563.1693003466773\n",
      "EP3 EpisodeReward=-1203.3573714233853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP4 EpisodeReward=-1175.3820461139994\n",
      "Epoch=196\t Average reward=-1175.3820461139994\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1499.624834190526\n",
      "EP1 EpisodeReward=-1512.4955576362802\n",
      "EP2 EpisodeReward=-1290.3207060086938\n",
      "EP3 EpisodeReward=-1650.8950370666755\n",
      "EP4 EpisodeReward=-1488.2100991371801\n",
      "Epoch=197\t Average reward=-1488.2100991371801\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-947.7084674232075\n",
      "EP1 EpisodeReward=-1518.1609479675137\n",
      "EP2 EpisodeReward=-1377.0016566092963\n",
      "EP3 EpisodeReward=-1494.5737143921624\n",
      "EP4 EpisodeReward=-1654.8268486545685\n",
      "Epoch=198\t Average reward=-1654.8268486545685\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1656.7361612959462\n",
      "EP1 EpisodeReward=-1595.5762209019827\n",
      "EP2 EpisodeReward=-1343.9552044854227\n",
      "EP3 EpisodeReward=-1481.1374377903874\n",
      "EP4 EpisodeReward=-1488.6522386863169\n",
      "Epoch=199\t Average reward=-1488.6522386863169\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 157512<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/n/home05/vqin/fasrc/es100_workspace/wandb/run-20201119_232717-3fnxst17/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/n/home05/vqin/fasrc/es100_workspace/wandb/run-20201119_232717-3fnxst17/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>Reward0</td><td>-1488.65224</td></tr><tr><td>_step</td><td>1199</td></tr><tr><td>_runtime</td><td>7864</td></tr><tr><td>_timestamp</td><td>1605836301</td></tr><tr><td>batch</td><td>199</td></tr><tr><td>Epoch</td><td>-1488.65224</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>Reward0</td><td></td></tr><tr><td>_step</td><td></td></tr><tr><td>_runtime</td><td></td></tr><tr><td>_timestamp</td><td></td></tr><tr><td>batch</td><td></td></tr><tr><td>Epoch</td><td></td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">PPO-single-long</strong>: <a href=\"https://wandb.ai/victor-qin/deep-rl-tf2/runs/3fnxst17\" target=\"_blank\">https://wandb.ai/victor-qin/deep-rl-tf2/runs/3fnxst17</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    try: wandb.finish()\n",
    "    except: pass\n",
    "    \n",
    "    ####configurations\n",
    "    wandb.init(name='PPO-single-long', project=\"deep-rl-tf2\")\n",
    "    env_name = 'Pendulum-v0'\n",
    "\n",
    "    \n",
    "    wandb.config.gamma = 0.99\n",
    "    wandb.config.update_interval = 5\n",
    "    wandb.config.actor_lr = 0.0005\n",
    "    wandb.config.critic_lr = 0.001\n",
    "    wandb.config.batch_size = 64\n",
    "    wandb.config.clip_ratio = 0.1\n",
    "    wandb.config.lmbda = 0.95\n",
    "    wandb.config.intervals = 3\n",
    "    \n",
    "    wandb.config.episodes = 5\n",
    "    wandb.config.num = 1\n",
    "    wandb.config.epochs = 200\n",
    "\n",
    "    wandb.config.actor = {'layer1': 32, 'layer2' : 32}\n",
    "    wandb.config.critic = {'layer1': 32, 'layer2' : 32, 'layer3': 16}\n",
    "    \n",
    "    print(wandb.config)\n",
    "    \n",
    "    # main run    \n",
    "    N = wandb.config.num\n",
    "    agents = []\n",
    "    \n",
    "    # set up the agent\n",
    "    for i in range(N):\n",
    "        env_t = gym.make(env_name)\n",
    "        agents.append(Agent(env_t, i))\n",
    "\n",
    "    # start the training\n",
    "    for z in range(wandb.config.epochs):\n",
    "\n",
    "        reward = 0\n",
    "        # train the agent\n",
    "        for j in range(len(agents)):\n",
    "            print('Training Agent {}'.format(agents[j].iden))\n",
    "            reward += agents[j].train(wandb.config.episodes)\n",
    "    \n",
    "        reward = reward / N\n",
    "        print('Epoch={}\\t Average reward={}'.format(z, reward))\n",
    "        wandb.log({'batch': z, 'Epoch': reward})\n",
    "\n",
    "\n",
    "        # get the average - actor and critic\n",
    "        critic_avg = []\n",
    "        actor_avg = []\n",
    "\n",
    "        for i in range(len(agents[0].actor.model.get_weights())):\n",
    "            \n",
    "            actor_t = agents[0].actor.model.get_weights()[i]\n",
    "\n",
    "            for j in range(1, N):\n",
    "                actor_t += agents[j].actor.model.get_weights()[i]\n",
    "\n",
    "            actor_t = actor_t / N\n",
    "            actor_avg.append(actor_t)\n",
    "\n",
    "\n",
    "        for i in range(len(agents[0].critic.model.get_weights())):\n",
    "            critic_t = agents[0].critic.model.get_weights()[i]\n",
    "\n",
    "            for j in range(1, N):\n",
    "                critic_t += agents[j].critic.model.get_weights()[i]\n",
    "\n",
    "            critic_t = critic_t / N\n",
    "            critic_avg.append(critic_t)\n",
    "\n",
    "\n",
    "        # set the average\n",
    "        for j in range(N):\n",
    "            agents[j].actor.model.set_weights(actor_avg)\n",
    "            agents[j].critic.model.set_weights(critic_avg)\n",
    "\n",
    "\n",
    "    # wrtie things out\n",
    "    for j in range(N):\n",
    "        with open(\"agent{}-actor.txt\".format(j), \"w\") as f:\n",
    "            f.write(str(agents[j].actor.model.get_weights()))\n",
    "            f.close()\n",
    "        wandb.save(\"agent{}-actor.txt\".format(j))\n",
    "        \n",
    "        \n",
    "        with open(\"agent{}-critic.txt\".format(j), \"w\") as f:\n",
    "            f.write(str(agents[j].critic.model.get_weights()))\n",
    "            f.close()\n",
    "        wandb.save(\"agent{}-critic.txt\".format(j))\n",
    "\n",
    "    \n",
    "    wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-es100]",
   "language": "python",
   "name": "conda-env-.conda-es100-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
