{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda\n",
    "\n",
    "import gym\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--gamma', type=float, default=0.99)\n",
    "# parser.add_argument('--update_interval', type=int, default=5)\n",
    "# parser.add_argument('--actor_lr', type=float, default=0.0005)\n",
    "# parser.add_argument('--critic_lr', type=float, default=0.001)\n",
    "# parser.add_argument('--clip_ratio', type=float, default=0.1)\n",
    "# parser.add_argument('--lmbda', type=float, default=0.95)\n",
    "# parser.add_argument('--epochs', type=int, default=3)\n",
    "\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# class Args:\n",
    "#     gamma = 0.99\n",
    "#     update_interval = 5\n",
    "#     actor_lr = 0.0005\n",
    "#     critic_lr = 0.001\n",
    "#     batch_size = 64\n",
    "#     clip_ratio = 0.1\n",
    "#     lmbda = 0.95\n",
    "#     intervals = 3\n",
    "    \n",
    "#     episodes = 10\n",
    "#     N = 3\n",
    "#     epochs = 100\n",
    "\n",
    "# args = Args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor:\n",
    "    def __init__(self, state_dim, action_dim, action_bound, std_bound):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.action_bound = action_bound\n",
    "        self.std_bound = std_bound\n",
    "        self.model = self.create_model()\n",
    "        self.opt = tf.keras.optimizers.Adam(wandb.config.actor_lr)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        state = np.reshape(state, [1, self.state_dim])\n",
    "        mu, std = self.model.predict(state)\n",
    "        action = np.random.normal(mu[0], std[0], size=self.action_dim)\n",
    "        action = np.clip(action, -self.action_bound, self.action_bound)\n",
    "        log_policy = self.log_pdf(mu, std, action)\n",
    "\n",
    "        return log_policy, action\n",
    "\n",
    "    def log_pdf(self, mu, std, action):\n",
    "        std = tf.clip_by_value(std, self.std_bound[0], self.std_bound[1])\n",
    "        var = std ** 2\n",
    "        log_policy_pdf = -0.5 * (action - mu) ** 2 / \\\n",
    "            var - 0.5 * tf.math.log(var * 2 * np.pi)\n",
    "        return tf.reduce_sum(log_policy_pdf, 1, keepdims=True)\n",
    "\n",
    "    def create_model(self):\n",
    "        state_input = Input((self.state_dim,))\n",
    "        dense_1 = Dense(wandb.config.actor['layer1'], activation='relu')(state_input)\n",
    "        dense_2 = Dense(wandb.config.actor['layer2'], activation='relu')(dense_1)\n",
    "        out_mu = Dense(self.action_dim, activation='tanh')(dense_2)\n",
    "        mu_output = Lambda(lambda x: x * self.action_bound)(out_mu)\n",
    "        std_output = Dense(self.action_dim, activation='softplus')(dense_2)\n",
    "        return tf.keras.models.Model(state_input, [mu_output, std_output])\n",
    "\n",
    "    def compute_loss(self, log_old_policy, log_new_policy, actions, gaes):\n",
    "        ratio = tf.exp(log_new_policy - tf.stop_gradient(log_old_policy))\n",
    "        gaes = tf.stop_gradient(gaes)\n",
    "        clipped_ratio = tf.clip_by_value(\n",
    "            ratio, 1.0-wandb.config.clip_ratio, 1.0+wandb.config.clip_ratio)\n",
    "        surrogate = -tf.minimum(ratio * gaes, clipped_ratio * gaes)\n",
    "        return tf.reduce_mean(surrogate)\n",
    "\n",
    "    def train(self, log_old_policy, states, actions, gaes):\n",
    "        with tf.GradientTape() as tape:\n",
    "            mu, std = self.model(states, training=True)\n",
    "            log_new_policy = self.log_pdf(mu, std, actions)\n",
    "            loss = self.compute_loss(\n",
    "                log_old_policy, log_new_policy, actions, gaes)\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.opt.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic:\n",
    "    def __init__(self, state_dim):\n",
    "        self.state_dim = state_dim\n",
    "        self.model = self.create_model()\n",
    "        self.opt = tf.keras.optimizers.Adam(wandb.config.critic_lr)\n",
    "\n",
    "    def create_model(self):\n",
    "        return tf.keras.Sequential([\n",
    "            Input((self.state_dim,)),\n",
    "            Dense(wandb.config.critic['layer1'], activation='relu'),\n",
    "            Dense(wandb.config.critic['layer2'], activation='relu'),\n",
    "            Dense(wandb.config.critic['layer3'], activation='relu'),\n",
    "            Dense(1, activation='linear')\n",
    "        ])\n",
    "\n",
    "    def compute_loss(self, v_pred, td_targets):\n",
    "        mse = tf.keras.losses.MeanSquaredError()\n",
    "        return mse(td_targets, v_pred)\n",
    "\n",
    "    def train(self, states, td_targets):\n",
    "        with tf.GradientTape() as tape:\n",
    "            v_pred = self.model(states, training=True)\n",
    "            assert v_pred.shape == td_targets.shape\n",
    "            loss = self.compute_loss(v_pred, tf.stop_gradient(td_targets))\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.opt.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env, iden = 0):\n",
    "        self.env = env\n",
    "        self.state_dim = self.env.observation_space.shape[0]\n",
    "        self.action_dim = self.env.action_space.shape[0]\n",
    "        self.action_bound = self.env.action_space.high[0]\n",
    "        self.std_bound = [1e-2, 1.0]\n",
    "\n",
    "        self.actor_opt = tf.keras.optimizers.Adam(wandb.config.actor_lr)\n",
    "        self.critic_opt = tf.keras.optimizers.Adam(wandb.config.critic_lr)\n",
    "        self.actor = Actor(self.state_dim, self.action_dim,\n",
    "                           self.action_bound, self.std_bound)\n",
    "        self.critic = Critic(self.state_dim)\n",
    "        \n",
    "        self.iden = iden\n",
    "\n",
    "    def gae_target(self, rewards, v_values, next_v_value, done):\n",
    "        n_step_targets = np.zeros_like(rewards)\n",
    "        gae = np.zeros_like(rewards)\n",
    "        gae_cumulative = 0\n",
    "        forward_val = 0\n",
    "\n",
    "        if not done:\n",
    "            forward_val = next_v_value\n",
    "\n",
    "        for k in reversed(range(0, len(rewards))):\n",
    "            delta = rewards[k] + wandb.config.gamma * forward_val - v_values[k]\n",
    "            gae_cumulative = wandb.config.gamma * wandb.config.lmbda * gae_cumulative + delta\n",
    "            gae[k] = gae_cumulative\n",
    "            forward_val = v_values[k]\n",
    "            n_step_targets[k] = gae[k] + v_values[k]\n",
    "        return gae, n_step_targets\n",
    "\n",
    "    def list_to_batch(self, list):\n",
    "        batch = list[0]\n",
    "        for elem in list[1:]:\n",
    "            batch = np.append(batch, elem, axis=0)\n",
    "        return batch\n",
    "\n",
    "    def train(self, max_episodes=1000):\n",
    "        for ep in range(max_episodes):\n",
    "            state_batch = []\n",
    "            action_batch = []\n",
    "            reward_batch = []\n",
    "            old_policy_batch = []\n",
    "\n",
    "            episode_reward, done = 0, False\n",
    "\n",
    "            state = self.env.reset()\n",
    "\n",
    "            while not done:\n",
    "                # self.env.render()\n",
    "                log_old_policy, action = self.actor.get_action(state)\n",
    "\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "\n",
    "                state = np.reshape(state, [1, self.state_dim])\n",
    "                action = np.reshape(action, [1, 1])\n",
    "                next_state = np.reshape(next_state, [1, self.state_dim])\n",
    "                reward = np.reshape(reward, [1, 1])\n",
    "                log_old_policy = np.reshape(log_old_policy, [1, 1])\n",
    "\n",
    "                state_batch.append(state)\n",
    "                action_batch.append(action)\n",
    "                reward_batch.append((reward+8)/8)\n",
    "                old_policy_batch.append(log_old_policy)\n",
    "\n",
    "                if len(state_batch) >= wandb.config.update_interval or done:\n",
    "                    states = self.list_to_batch(state_batch)\n",
    "                    actions = self.list_to_batch(action_batch)\n",
    "                    rewards = self.list_to_batch(reward_batch)\n",
    "                    old_policys = self.list_to_batch(old_policy_batch)\n",
    "\n",
    "                    v_values = self.critic.model.predict(states)\n",
    "                    next_v_value = self.critic.model.predict(next_state)\n",
    "\n",
    "                    gaes, td_targets = self.gae_target(\n",
    "                        rewards, v_values, next_v_value, done)\n",
    "\n",
    "                    for epoch in range(wandb.config.intervals):\n",
    "                        actor_loss = self.actor.train(\n",
    "                            old_policys, states, actions, gaes)\n",
    "                        critic_loss = self.critic.train(states, td_targets)\n",
    "\n",
    "                    state_batch = []\n",
    "                    action_batch = []\n",
    "                    reward_batch = []\n",
    "                    old_policy_batch = []\n",
    "\n",
    "                episode_reward += reward[0][0]\n",
    "                state = next_state[0]\n",
    "\n",
    "            print('EP{} EpisodeReward={}'.format(ep, episode_reward))\n",
    "            wandb.log({'Reward' + str(self.iden): episode_reward})\n",
    "        \n",
    "        return episode_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.10<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">PPO-multiple-long</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/victor-qin/deep-rl-tf2\" target=\"_blank\">https://wandb.ai/victor-qin/deep-rl-tf2</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/victor-qin/deep-rl-tf2/runs/1i8op8b0\" target=\"_blank\">https://wandb.ai/victor-qin/deep-rl-tf2/runs/1i8op8b0</a><br/>\n",
       "                Run data is saved locally in <code>/n/home05/vqin/fasrc/es100_workspace/wandb/run-20201119_235927-1i8op8b0</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.99, 'update_interval': 5, 'actor_lr': 0.0005, 'critic_lr': 0.001, 'batch_size': 64, 'clip_ratio': 0.1, 'lmbda': 0.95, 'intervals': 3, 'episodes': 5, 'num': 3, 'epochs': 200, 'actor': {'layer1': 32, 'layer2': 32}, 'critic': {'layer1': 32, 'layer2': 32, 'layer3': 16}}\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1664.5169533860847\n",
      "EP1 EpisodeReward=-1826.4128968716336\n",
      "EP2 EpisodeReward=-1087.9888521201874\n",
      "EP3 EpisodeReward=-1501.3778726966254\n",
      "EP4 EpisodeReward=-1342.9093457667946\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1396.1159420464223\n",
      "EP1 EpisodeReward=-1664.627745580286\n",
      "EP2 EpisodeReward=-1527.551076731109\n",
      "EP3 EpisodeReward=-1548.8385124540102\n",
      "EP4 EpisodeReward=-1526.9715217677403\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1498.6069248862202\n",
      "EP1 EpisodeReward=-1406.521938994423\n",
      "EP2 EpisodeReward=-1455.239321374338\n",
      "EP3 EpisodeReward=-1492.9091363408036\n",
      "EP4 EpisodeReward=-1596.2103027970516\n",
      "Epoch=0\t Average reward=-1488.6970567771957\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1432.5729383124021\n",
      "EP1 EpisodeReward=-1481.2344049792014\n",
      "EP2 EpisodeReward=-1704.229893102471\n",
      "EP3 EpisodeReward=-1356.6819097217278\n",
      "EP4 EpisodeReward=-1467.6308541251615\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1066.0414277730201\n",
      "EP1 EpisodeReward=-1116.9741217436047\n",
      "EP2 EpisodeReward=-1230.6982427443966\n",
      "EP3 EpisodeReward=-1015.666546011536\n",
      "EP4 EpisodeReward=-1280.4471269501971\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1229.0312872824873\n",
      "EP1 EpisodeReward=-1325.4878690309902\n",
      "EP2 EpisodeReward=-1181.2681658440483\n",
      "EP3 EpisodeReward=-1495.943947536945\n",
      "EP4 EpisodeReward=-1354.0809531013215\n",
      "Epoch=1\t Average reward=-1367.3863113922264\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1360.127898298971\n",
      "EP1 EpisodeReward=-1331.9971245109382\n",
      "EP2 EpisodeReward=-1343.3312739063572\n",
      "EP3 EpisodeReward=-1364.4179653228052\n",
      "EP4 EpisodeReward=-1333.9084163618913\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1260.5930462238402\n",
      "EP1 EpisodeReward=-1553.046937116986\n",
      "EP2 EpisodeReward=-1243.9355533107994\n",
      "EP3 EpisodeReward=-1448.920801729153\n",
      "EP4 EpisodeReward=-1345.155534545772\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1336.4879381468584\n",
      "EP1 EpisodeReward=-1325.24071012889\n",
      "EP2 EpisodeReward=-1365.6203501011835\n",
      "EP3 EpisodeReward=-1421.0744870334265\n",
      "EP4 EpisodeReward=-1491.0270105102675\n",
      "Epoch=2\t Average reward=-1390.0303204726436\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1504.6680580670181\n",
      "EP1 EpisodeReward=-1484.838058941655\n",
      "EP2 EpisodeReward=-1338.3790498234762\n",
      "EP3 EpisodeReward=-1316.6767869579846\n",
      "EP4 EpisodeReward=-1500.4546426666748\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1357.8662216781263\n",
      "EP1 EpisodeReward=-1206.108934607258\n",
      "EP2 EpisodeReward=-1668.6641144149858\n",
      "EP3 EpisodeReward=-1102.0094036908527\n",
      "EP4 EpisodeReward=-1396.2764614530188\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1193.995307347085\n",
      "EP1 EpisodeReward=-1152.0384937622377\n",
      "EP2 EpisodeReward=-1311.1255116881682\n",
      "EP3 EpisodeReward=-1245.2212127778594\n",
      "EP4 EpisodeReward=-1501.0683225892526\n",
      "Epoch=3\t Average reward=-1465.9331422363155\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1393.809469481472\n",
      "EP1 EpisodeReward=-1370.4436961211254\n",
      "EP2 EpisodeReward=-1532.2820601340252\n",
      "EP3 EpisodeReward=-1540.8801697380802\n",
      "EP4 EpisodeReward=-1494.626166938301\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1551.5818235183017\n",
      "EP1 EpisodeReward=-1332.0287137014566\n",
      "EP2 EpisodeReward=-1542.954826628145\n",
      "EP3 EpisodeReward=-1527.1623043980092\n",
      "EP4 EpisodeReward=-1510.0964102184482\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1108.0405273532033\n",
      "EP1 EpisodeReward=-1481.3384688231538\n",
      "EP2 EpisodeReward=-1241.0180078198885\n",
      "EP3 EpisodeReward=-1424.5205177266414\n",
      "EP4 EpisodeReward=-1369.631142273987\n",
      "Epoch=4\t Average reward=-1458.1179064769121\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1746.0787522730127\n",
      "EP1 EpisodeReward=-1322.794200884255\n",
      "EP2 EpisodeReward=-1540.9847640288026\n",
      "EP3 EpisodeReward=-1529.7636052947794\n",
      "EP4 EpisodeReward=-1129.9121155976463\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1196.384651517668\n",
      "EP1 EpisodeReward=-1325.4790468622668\n",
      "EP2 EpisodeReward=-1471.007848647204\n",
      "EP3 EpisodeReward=-1456.4505619153879\n",
      "EP4 EpisodeReward=-1478.8153104069502\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1283.873586552343\n",
      "EP1 EpisodeReward=-1252.66818896528\n",
      "EP2 EpisodeReward=-1255.1505247567295\n",
      "EP3 EpisodeReward=-1213.1177787414056\n",
      "EP4 EpisodeReward=-1492.5366043502997\n",
      "Epoch=5\t Average reward=-1367.0880101182986\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1332.8598304747795\n",
      "EP1 EpisodeReward=-1214.4198034187602\n",
      "EP2 EpisodeReward=-1462.9192314018983\n",
      "EP3 EpisodeReward=-1478.8339708662122\n",
      "EP4 EpisodeReward=-1359.9987035904257\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1075.42402990997\n",
      "EP1 EpisodeReward=-1135.7833332672067\n",
      "EP2 EpisodeReward=-1383.416479308004\n",
      "EP3 EpisodeReward=-1183.3323084824242\n",
      "EP4 EpisodeReward=-1391.0322962866014\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1224.3345019863007\n",
      "EP1 EpisodeReward=-1008.9744322948898\n",
      "EP2 EpisodeReward=-1020.8100291694941\n",
      "EP3 EpisodeReward=-1543.6550694188475\n",
      "EP4 EpisodeReward=-1466.4116476506858\n",
      "Epoch=6\t Average reward=-1405.814215842571\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1209.3760011865795\n",
      "EP1 EpisodeReward=-1366.6004252996688\n",
      "EP2 EpisodeReward=-1439.2866416647375\n",
      "EP3 EpisodeReward=-1321.5197948344305\n",
      "EP4 EpisodeReward=-1388.9969023116926\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1346.9421240057025\n",
      "EP1 EpisodeReward=-1032.2562600496042\n",
      "EP2 EpisodeReward=-1558.3736135787917\n",
      "EP3 EpisodeReward=-1006.722705784845\n",
      "EP4 EpisodeReward=-1406.4737736889101\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1185.7501797855648\n",
      "EP1 EpisodeReward=-1206.727328122549\n",
      "EP2 EpisodeReward=-1184.9230988666286\n",
      "EP3 EpisodeReward=-1328.9243787818089\n",
      "EP4 EpisodeReward=-1351.9884923609168\n",
      "Epoch=7\t Average reward=-1382.48638945384\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1343.0750730021775\n",
      "EP1 EpisodeReward=-1436.2099412920097\n",
      "EP2 EpisodeReward=-1428.3692632958337\n",
      "EP3 EpisodeReward=-1458.9770943846474\n",
      "EP4 EpisodeReward=-1461.844512807784\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1331.5395762442636\n",
      "EP1 EpisodeReward=-1258.4566964082576\n",
      "EP2 EpisodeReward=-1368.2068238767201\n",
      "EP3 EpisodeReward=-1349.1432397418328\n",
      "EP4 EpisodeReward=-1484.5552892197036\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1198.1584038288438\n",
      "EP1 EpisodeReward=-1360.206401800966\n",
      "EP2 EpisodeReward=-1334.8598414380551\n",
      "EP3 EpisodeReward=-1070.5356062516278\n",
      "EP4 EpisodeReward=-1024.3959569393344\n",
      "Epoch=8\t Average reward=-1323.598586322274\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1573.8434333334103\n",
      "EP1 EpisodeReward=-1149.5362116161716\n",
      "EP2 EpisodeReward=-1357.4051539904424\n",
      "EP3 EpisodeReward=-1265.9495888096922\n",
      "EP4 EpisodeReward=-1450.5327347200805\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1121.049450808516\n",
      "EP1 EpisodeReward=-1306.1650549883122\n",
      "EP2 EpisodeReward=-1076.418253814357\n",
      "EP3 EpisodeReward=-1179.901941095829\n",
      "EP4 EpisodeReward=-1225.0333435998823\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1285.3917123798058\n",
      "EP1 EpisodeReward=-1367.446628801983\n",
      "EP2 EpisodeReward=-1491.0597273145722\n",
      "EP3 EpisodeReward=-1447.5549525888875\n",
      "EP4 EpisodeReward=-1199.648789761897\n",
      "Epoch=9\t Average reward=-1291.73828936062\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1284.4023273373946\n",
      "EP1 EpisodeReward=-1192.3881262758387\n",
      "EP2 EpisodeReward=-1056.0246265549083\n",
      "EP3 EpisodeReward=-1568.2955224017603\n",
      "EP4 EpisodeReward=-1519.776784487843\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1152.7902795671973\n",
      "EP1 EpisodeReward=-1222.2816925393508\n",
      "EP2 EpisodeReward=-1384.4209848442167\n",
      "EP3 EpisodeReward=-1243.0830727430432\n",
      "EP4 EpisodeReward=-1235.4993641358635\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1141.7948277418789\n",
      "EP1 EpisodeReward=-1023.0299328359835\n",
      "EP2 EpisodeReward=-1197.355479177854\n",
      "EP3 EpisodeReward=-1204.943479388705\n",
      "EP4 EpisodeReward=-1183.6399793412756\n",
      "Epoch=10\t Average reward=-1312.972042654994\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1458.5291820632208\n",
      "EP1 EpisodeReward=-1211.389873300534\n",
      "EP2 EpisodeReward=-1223.024755262749\n",
      "EP3 EpisodeReward=-1220.821846978002\n",
      "EP4 EpisodeReward=-1306.9096233257956\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1322.369226732212\n",
      "EP1 EpisodeReward=-1184.7886846052725\n",
      "EP2 EpisodeReward=-1264.9376903824455\n",
      "EP3 EpisodeReward=-1168.8565550934502\n",
      "EP4 EpisodeReward=-1295.1799294885454\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1163.1906793941405\n",
      "EP1 EpisodeReward=-1310.3971486465257\n",
      "EP2 EpisodeReward=-1423.8381643033167\n",
      "EP3 EpisodeReward=-1430.6085565542078\n",
      "EP4 EpisodeReward=-1432.4717541281213\n",
      "Epoch=11\t Average reward=-1344.8537689808209\n",
      "Training Agent 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP0 EpisodeReward=-1416.405314580213\n",
      "EP1 EpisodeReward=-1320.6026979995781\n",
      "EP2 EpisodeReward=-1469.3904611527507\n",
      "EP3 EpisodeReward=-1420.2965380377652\n",
      "EP4 EpisodeReward=-1225.3596291322153\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1169.9040885320705\n",
      "EP1 EpisodeReward=-1338.0637944673774\n",
      "EP2 EpisodeReward=-1294.207154429655\n",
      "EP3 EpisodeReward=-1284.3358454679335\n",
      "EP4 EpisodeReward=-1197.5554429929546\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1306.4562591876474\n",
      "EP1 EpisodeReward=-1064.6915205550185\n",
      "EP2 EpisodeReward=-1411.7772989435746\n",
      "EP3 EpisodeReward=-1391.2531247276\n",
      "EP4 EpisodeReward=-1155.5039526482144\n",
      "Epoch=12\t Average reward=-1192.806341591128\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1160.9134278680904\n",
      "EP1 EpisodeReward=-1203.684040343666\n",
      "EP2 EpisodeReward=-1141.7336354262725\n",
      "EP3 EpisodeReward=-1423.0271945790255\n",
      "EP4 EpisodeReward=-1458.9110819065938\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1160.3408955231519\n",
      "EP1 EpisodeReward=-1242.8832873114904\n",
      "EP2 EpisodeReward=-1237.3430206093026\n",
      "EP3 EpisodeReward=-1280.9443904221719\n",
      "EP4 EpisodeReward=-1469.8908436157055\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1238.397339509049\n",
      "EP1 EpisodeReward=-1306.4478557981602\n",
      "EP2 EpisodeReward=-1186.247555552837\n",
      "EP3 EpisodeReward=-1182.0259160232865\n",
      "EP4 EpisodeReward=-969.6520112187006\n",
      "Epoch=13\t Average reward=-1299.4846455803333\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-761.3039179093882\n",
      "EP1 EpisodeReward=-1088.6412628102387\n",
      "EP2 EpisodeReward=-1622.4399881759869\n",
      "EP3 EpisodeReward=-1220.911496644697\n",
      "EP4 EpisodeReward=-1330.522416876852\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1250.5972900696372\n",
      "EP1 EpisodeReward=-1150.8112781851773\n",
      "EP2 EpisodeReward=-977.8809779179039\n",
      "EP3 EpisodeReward=-1609.0920703227864\n",
      "EP4 EpisodeReward=-999.5600080634798\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1342.5166725951897\n",
      "EP1 EpisodeReward=-951.9262866475456\n",
      "EP2 EpisodeReward=-1486.1430127542787\n",
      "EP3 EpisodeReward=-1387.383536317341\n",
      "EP4 EpisodeReward=-1343.0079881110196\n",
      "Epoch=14\t Average reward=-1224.363471017117\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1182.9393347067946\n",
      "EP1 EpisodeReward=-1275.1666586144556\n",
      "EP2 EpisodeReward=-1164.8510155146598\n",
      "EP3 EpisodeReward=-1195.5652175295145\n",
      "EP4 EpisodeReward=-1785.441236498444\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1478.2458702246317\n",
      "EP1 EpisodeReward=-1333.429035724153\n",
      "EP2 EpisodeReward=-1180.87502750857\n",
      "EP3 EpisodeReward=-1030.5624158182002\n",
      "EP4 EpisodeReward=-1068.5257852467535\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1593.5898410450473\n",
      "EP1 EpisodeReward=-1619.225457731584\n",
      "EP2 EpisodeReward=-1318.761550371674\n",
      "EP3 EpisodeReward=-1497.3144014212462\n",
      "EP4 EpisodeReward=-1396.661585483541\n",
      "Epoch=15\t Average reward=-1416.8762024095795\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1063.9805891762185\n",
      "EP1 EpisodeReward=-1208.0855483397265\n",
      "EP2 EpisodeReward=-1189.1622579782404\n",
      "EP3 EpisodeReward=-880.483370984336\n",
      "EP4 EpisodeReward=-1148.5191592714407\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1482.744605023837\n",
      "EP1 EpisodeReward=-1206.0105845715818\n",
      "EP2 EpisodeReward=-1093.5787980181096\n",
      "EP3 EpisodeReward=-1154.3211556403498\n",
      "EP4 EpisodeReward=-1001.0776792691656\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1183.587469992496\n",
      "EP1 EpisodeReward=-1200.0430034422782\n",
      "EP2 EpisodeReward=-1209.3211213294173\n",
      "EP3 EpisodeReward=-1244.570551028208\n",
      "EP4 EpisodeReward=-1476.3701089756084\n",
      "Epoch=16\t Average reward=-1208.6556491720714\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1007.7626419908959\n",
      "EP1 EpisodeReward=-1318.1874775346053\n",
      "EP2 EpisodeReward=-1158.52710683813\n",
      "EP3 EpisodeReward=-1116.2325659649107\n",
      "EP4 EpisodeReward=-1300.7271267864037\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1173.2001461951138\n",
      "EP1 EpisodeReward=-1165.7017463678633\n",
      "EP2 EpisodeReward=-1310.4302703447433\n",
      "EP3 EpisodeReward=-1178.975169592235\n",
      "EP4 EpisodeReward=-1361.5416882669526\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1148.988154108165\n",
      "EP1 EpisodeReward=-1325.541449933806\n",
      "EP2 EpisodeReward=-1241.5465940419106\n",
      "EP3 EpisodeReward=-881.0079057404262\n",
      "EP4 EpisodeReward=-1342.3237799111685\n",
      "Epoch=17\t Average reward=-1334.8641983215082\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-967.702948670945\n",
      "EP1 EpisodeReward=-1259.929295974413\n",
      "EP2 EpisodeReward=-1300.1626113136585\n",
      "EP3 EpisodeReward=-1227.5196172679923\n",
      "EP4 EpisodeReward=-1384.4347576899743\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1198.1533464439283\n",
      "EP1 EpisodeReward=-857.8555172367317\n",
      "EP2 EpisodeReward=-1098.626432984098\n",
      "EP3 EpisodeReward=-1079.5956418546696\n",
      "EP4 EpisodeReward=-1270.5123128745624\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1342.1817644577534\n",
      "EP1 EpisodeReward=-1326.1763368900117\n",
      "EP2 EpisodeReward=-1047.6667490242214\n",
      "EP3 EpisodeReward=-1179.8111366996077\n",
      "EP4 EpisodeReward=-1358.1593469158058\n",
      "Epoch=18\t Average reward=-1337.702139160114\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1274.8040801667398\n",
      "EP1 EpisodeReward=-1161.6245192273032\n",
      "EP2 EpisodeReward=-1224.4243237646497\n",
      "EP3 EpisodeReward=-858.9181734527598\n",
      "EP4 EpisodeReward=-1296.9500202320783\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1158.6433145497488\n",
      "EP1 EpisodeReward=-1191.6581760668525\n",
      "EP2 EpisodeReward=-1265.148992375504\n",
      "EP3 EpisodeReward=-1363.0060761720767\n",
      "EP4 EpisodeReward=-1137.0923126667185\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1168.345017377924\n",
      "EP1 EpisodeReward=-1183.7321884859548\n",
      "EP2 EpisodeReward=-1113.7661319143997\n",
      "EP3 EpisodeReward=-1290.424494649615\n",
      "EP4 EpisodeReward=-1374.5214927984173\n",
      "Epoch=19\t Average reward=-1269.5212752324048\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1319.0437264896113\n",
      "EP1 EpisodeReward=-1222.87475365076\n",
      "EP2 EpisodeReward=-1212.6161958148812\n",
      "EP3 EpisodeReward=-1131.948007797651\n",
      "EP4 EpisodeReward=-865.7834959586976\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-969.2531566709051\n",
      "EP1 EpisodeReward=-632.4349331959297\n",
      "EP2 EpisodeReward=-765.7119259402998\n",
      "EP3 EpisodeReward=-1034.6002975209958\n",
      "EP4 EpisodeReward=-1411.0313513200044\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1032.8234628271882\n",
      "EP1 EpisodeReward=-1255.9103816297713\n",
      "EP2 EpisodeReward=-1181.0446580626947\n",
      "EP3 EpisodeReward=-1215.7728619309812\n",
      "EP4 EpisodeReward=-1196.4202130752656\n",
      "Epoch=20\t Average reward=-1157.7450201179893\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1063.5945859062067\n",
      "EP1 EpisodeReward=-1281.9020952790795\n",
      "EP2 EpisodeReward=-1251.3519611248787\n",
      "EP3 EpisodeReward=-1317.4303971848342\n",
      "EP4 EpisodeReward=-1502.4881786138722\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1000.5032955533587\n",
      "EP1 EpisodeReward=-939.0764164747168\n",
      "EP2 EpisodeReward=-1025.4593541263216\n",
      "EP3 EpisodeReward=-1030.265460432173\n",
      "EP4 EpisodeReward=-1359.4165132308065\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1004.2946735812219\n",
      "EP1 EpisodeReward=-1164.1680866519253\n",
      "EP2 EpisodeReward=-1227.331595204545\n",
      "EP3 EpisodeReward=-1296.5480948031466\n",
      "EP4 EpisodeReward=-1136.7319437137269\n",
      "Epoch=21\t Average reward=-1332.8788785194683\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1178.5220169551515\n",
      "EP1 EpisodeReward=-757.2349772656673\n",
      "EP2 EpisodeReward=-821.3408574456435\n",
      "EP3 EpisodeReward=-992.1847500923759\n",
      "EP4 EpisodeReward=-1261.613434181251\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-915.8051755104769\n",
      "EP1 EpisodeReward=-1220.4290792525655\n",
      "EP2 EpisodeReward=-1160.474242759785\n",
      "EP3 EpisodeReward=-1050.4446292603363\n",
      "EP4 EpisodeReward=-1043.0378302473864\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1280.4861004294337\n",
      "EP1 EpisodeReward=-877.1118880319955\n",
      "EP2 EpisodeReward=-1288.2124832911431\n",
      "EP3 EpisodeReward=-1209.69170354422\n",
      "EP4 EpisodeReward=-1414.4802294579388\n",
      "Epoch=22\t Average reward=-1239.710497962192\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1090.0960619652997\n",
      "EP1 EpisodeReward=-1057.0309136923058\n",
      "EP2 EpisodeReward=-1428.285730603267\n",
      "EP3 EpisodeReward=-1214.7829071924373\n",
      "EP4 EpisodeReward=-1288.6199264849567\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1278.5628011132353\n",
      "EP1 EpisodeReward=-1062.3805032585406\n",
      "EP2 EpisodeReward=-928.5485323550287\n",
      "EP3 EpisodeReward=-1134.233697344902\n",
      "EP4 EpisodeReward=-1209.0522348663105\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1216.6616032226602\n",
      "EP1 EpisodeReward=-1215.4190040008252\n",
      "EP2 EpisodeReward=-1036.6693265491094\n",
      "EP3 EpisodeReward=-1045.4246541330215\n",
      "EP4 EpisodeReward=-1020.3887869060943\n",
      "Epoch=23\t Average reward=-1172.686982752454\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-982.3537294041897\n",
      "EP1 EpisodeReward=-929.501657815952\n",
      "EP2 EpisodeReward=-1055.6723650549445\n",
      "EP3 EpisodeReward=-1018.6567445594449\n",
      "EP4 EpisodeReward=-949.0437423231568\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-973.736665421421\n",
      "EP1 EpisodeReward=-1218.695720096315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP2 EpisodeReward=-1438.4183931230396\n",
      "EP3 EpisodeReward=-1375.6688828694248\n",
      "EP4 EpisodeReward=-1317.1221749136278\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-900.1419054918812\n",
      "EP1 EpisodeReward=-1100.3807876842752\n",
      "EP2 EpisodeReward=-1244.265729187993\n",
      "EP3 EpisodeReward=-1126.2865512228964\n",
      "EP4 EpisodeReward=-1175.821030245861\n",
      "Epoch=24\t Average reward=-1147.328982494215\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1341.405619568132\n",
      "EP1 EpisodeReward=-1231.3335916625354\n",
      "EP2 EpisodeReward=-905.3055835998216\n",
      "EP3 EpisodeReward=-1236.0816641256597\n",
      "EP4 EpisodeReward=-1526.5622310220692\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1164.0672243449992\n",
      "EP1 EpisodeReward=-1201.029064832471\n",
      "EP2 EpisodeReward=-841.4433186254979\n",
      "EP3 EpisodeReward=-1075.7901080614295\n",
      "EP4 EpisodeReward=-1220.582465308517\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1048.7765256887478\n",
      "EP1 EpisodeReward=-885.5903629245963\n",
      "EP2 EpisodeReward=-912.8750877399658\n",
      "EP3 EpisodeReward=-931.2089320645243\n",
      "EP4 EpisodeReward=-764.0553570525745\n",
      "Epoch=25\t Average reward=-1170.400017794387\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1161.6287152621514\n",
      "EP1 EpisodeReward=-1055.7438183856661\n",
      "EP2 EpisodeReward=-1412.6881915997938\n",
      "EP3 EpisodeReward=-1069.207729278592\n",
      "EP4 EpisodeReward=-1293.4173113506506\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-961.6933418736614\n",
      "EP1 EpisodeReward=-950.5173326935274\n",
      "EP2 EpisodeReward=-1184.639646885683\n",
      "EP3 EpisodeReward=-1357.5599876500298\n",
      "EP4 EpisodeReward=-1496.831627540745\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-900.6812523156898\n",
      "EP1 EpisodeReward=-768.37470395802\n",
      "EP2 EpisodeReward=-1176.6916657595775\n",
      "EP3 EpisodeReward=-878.8276903471766\n",
      "EP4 EpisodeReward=-779.3053727814859\n",
      "Epoch=26\t Average reward=-1189.8514372242937\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-793.9632294581361\n",
      "EP1 EpisodeReward=-761.6504453247176\n",
      "EP2 EpisodeReward=-1044.8808385752475\n",
      "EP3 EpisodeReward=-640.8530860743064\n",
      "EP4 EpisodeReward=-886.0555032651258\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-891.9405987886895\n",
      "EP1 EpisodeReward=-1057.6674414406689\n",
      "EP2 EpisodeReward=-1053.9270429079222\n",
      "EP3 EpisodeReward=-867.4860059252077\n",
      "EP4 EpisodeReward=-1066.309000064072\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-994.2968540375327\n",
      "EP1 EpisodeReward=-1210.214425784806\n",
      "EP2 EpisodeReward=-1279.254516542495\n",
      "EP3 EpisodeReward=-1168.3944034856677\n",
      "EP4 EpisodeReward=-1071.5026168028978\n",
      "Epoch=27\t Average reward=-1007.9557067106986\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-899.9205826120983\n",
      "EP1 EpisodeReward=-1057.4266609515735\n",
      "EP2 EpisodeReward=-762.1619481166964\n",
      "EP3 EpisodeReward=-880.4458503285067\n",
      "EP4 EpisodeReward=-892.3252535209374\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1050.856579752609\n",
      "EP1 EpisodeReward=-738.7813895484556\n",
      "EP2 EpisodeReward=-1034.0476304471433\n",
      "EP3 EpisodeReward=-1250.2227970356018\n",
      "EP4 EpisodeReward=-1274.3700919695243\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1006.1140983024446\n",
      "EP1 EpisodeReward=-869.9226754023598\n",
      "EP2 EpisodeReward=-1057.78315674123\n",
      "EP3 EpisodeReward=-1063.462088742424\n",
      "EP4 EpisodeReward=-1268.2026864775391\n",
      "Epoch=28\t Average reward=-1144.9660106560002\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1069.3926119670223\n",
      "EP1 EpisodeReward=-1206.167374560195\n",
      "EP2 EpisodeReward=-920.0988864588518\n",
      "EP3 EpisodeReward=-912.6616351800749\n",
      "EP4 EpisodeReward=-513.6188288368793\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-869.9790829563749\n",
      "EP1 EpisodeReward=-875.2293699645162\n",
      "EP2 EpisodeReward=-897.3942893599751\n",
      "EP3 EpisodeReward=-1103.448148077155\n",
      "EP4 EpisodeReward=-938.9537899232774\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-651.216220743885\n",
      "EP1 EpisodeReward=-875.64599000267\n",
      "EP2 EpisodeReward=-1141.6988225913874\n",
      "EP3 EpisodeReward=-1198.861373885479\n",
      "EP4 EpisodeReward=-1217.9531894657407\n",
      "Epoch=29\t Average reward=-890.1752694086325\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1190.8879064747894\n",
      "EP1 EpisodeReward=-1221.8124620649428\n",
      "EP2 EpisodeReward=-814.7013719250082\n",
      "EP3 EpisodeReward=-1235.9742204460465\n",
      "EP4 EpisodeReward=-875.2489797584674\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1187.711762927113\n",
      "EP1 EpisodeReward=-1260.857947998966\n",
      "EP2 EpisodeReward=-1018.0629186220534\n",
      "EP3 EpisodeReward=-1152.7532829474026\n",
      "EP4 EpisodeReward=-1058.726739222907\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1026.0458829356082\n",
      "EP1 EpisodeReward=-1098.0835563067965\n",
      "EP2 EpisodeReward=-1316.08503072989\n",
      "EP3 EpisodeReward=-1184.1839028024133\n",
      "EP4 EpisodeReward=-552.2533151809952\n",
      "Epoch=30\t Average reward=-828.7430113874566\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-780.9016252221246\n",
      "EP1 EpisodeReward=-1068.7277672719342\n",
      "EP2 EpisodeReward=-1067.3761084815128\n",
      "EP3 EpisodeReward=-1013.4188058894898\n",
      "EP4 EpisodeReward=-1019.9177401315645\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-504.46692671416\n",
      "EP1 EpisodeReward=-1017.0283626386589\n",
      "EP2 EpisodeReward=-1053.9834011412383\n",
      "EP3 EpisodeReward=-1067.227590707372\n",
      "EP4 EpisodeReward=-902.5269185947379\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1012.1628338217425\n",
      "EP1 EpisodeReward=-1060.3528649853247\n",
      "EP2 EpisodeReward=-1109.9042632296791\n",
      "EP3 EpisodeReward=-776.2796706615717\n",
      "EP4 EpisodeReward=-1071.6328477183588\n",
      "Epoch=31\t Average reward=-998.0258354815537\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-790.1514974369946\n",
      "EP1 EpisodeReward=-856.9876651628269\n",
      "EP2 EpisodeReward=-1203.3965769963465\n",
      "EP3 EpisodeReward=-1049.4033942128974\n",
      "EP4 EpisodeReward=-1052.7956577196526\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1027.3095795408376\n",
      "EP1 EpisodeReward=-1085.9173780116157\n",
      "EP2 EpisodeReward=-1178.5381648832583\n",
      "EP3 EpisodeReward=-755.7868628538765\n",
      "EP4 EpisodeReward=-1062.0874303334763\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1011.7712620496983\n",
      "EP1 EpisodeReward=-1031.6184986226883\n",
      "EP2 EpisodeReward=-264.7690718592003\n",
      "EP3 EpisodeReward=-958.644865166888\n",
      "EP4 EpisodeReward=-1591.2623762327962\n",
      "Epoch=32\t Average reward=-1235.3818214286416\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-932.6834766842285\n",
      "EP1 EpisodeReward=-904.1488362973106\n",
      "EP2 EpisodeReward=-648.4161848138351\n",
      "EP3 EpisodeReward=-1060.7564200236454\n",
      "EP4 EpisodeReward=-861.3384070911245\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1516.4566562084672\n",
      "EP1 EpisodeReward=-1585.3207508227838\n",
      "EP2 EpisodeReward=-1444.5504350260348\n",
      "EP3 EpisodeReward=-1046.709879935459\n",
      "EP4 EpisodeReward=-1395.6429967233719\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1009.0400671286286\n",
      "EP1 EpisodeReward=-1044.539685041059\n",
      "EP2 EpisodeReward=-1060.352514454479\n",
      "EP3 EpisodeReward=-922.1248476870333\n",
      "EP4 EpisodeReward=-1419.8079444617501\n",
      "Epoch=33\t Average reward=-1225.5964494254156\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-530.5861861512145\n",
      "EP1 EpisodeReward=-516.3607847574289\n",
      "EP2 EpisodeReward=-819.7093106492531\n",
      "EP3 EpisodeReward=-389.0400472333836\n",
      "EP4 EpisodeReward=-680.2475594706182\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-868.1912350895266\n",
      "EP1 EpisodeReward=-1036.8261045957513\n",
      "EP2 EpisodeReward=-932.553625701689\n",
      "EP3 EpisodeReward=-1077.5027834954387\n",
      "EP4 EpisodeReward=-1252.6848828125594\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1211.1058676548396\n",
      "EP1 EpisodeReward=-1063.7851413894577\n",
      "EP2 EpisodeReward=-908.4244189840559\n",
      "EP3 EpisodeReward=-946.8675300886708\n",
      "EP4 EpisodeReward=-1082.760264620114\n",
      "Epoch=34\t Average reward=-1005.2309023010972\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-942.6021670141062\n",
      "EP1 EpisodeReward=-967.3213821519392\n",
      "EP2 EpisodeReward=-1057.3662850156802\n",
      "EP3 EpisodeReward=-707.0412191421401\n",
      "EP4 EpisodeReward=-908.0226400727779\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-906.3682864596689\n",
      "EP1 EpisodeReward=-907.5296370399773\n",
      "EP2 EpisodeReward=-644.3831105219074\n",
      "EP3 EpisodeReward=-645.9672456854428\n",
      "EP4 EpisodeReward=-645.9559001838727\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-575.1364456681229\n",
      "EP1 EpisodeReward=-550.5398121310361\n",
      "EP2 EpisodeReward=-1194.4318057018543\n",
      "EP3 EpisodeReward=-268.265692174958\n",
      "EP4 EpisodeReward=-245.82422351989078\n",
      "Epoch=35\t Average reward=-599.9342545921804\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-514.6135492029931\n",
      "EP1 EpisodeReward=-986.5996132162468\n",
      "EP2 EpisodeReward=-811.2599831001187\n",
      "EP3 EpisodeReward=-390.9704085546321\n",
      "EP4 EpisodeReward=-256.9877075735934\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-980.4410937372386\n",
      "EP1 EpisodeReward=-996.8256810348335\n",
      "EP2 EpisodeReward=-841.3799359306416\n",
      "EP3 EpisodeReward=-393.79280860672236\n",
      "EP4 EpisodeReward=-919.3987504971237\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-660.1700572670694\n",
      "EP1 EpisodeReward=-626.6776094591414\n",
      "EP2 EpisodeReward=-406.30794921727477\n",
      "EP3 EpisodeReward=-524.3558836206263\n",
      "EP4 EpisodeReward=-658.4947176714076\n",
      "Epoch=36\t Average reward=-611.6270585807083\n",
      "Training Agent 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP0 EpisodeReward=-1211.6217032215966\n",
      "EP1 EpisodeReward=-953.8446537078573\n",
      "EP2 EpisodeReward=-912.7136734067799\n",
      "EP3 EpisodeReward=-957.1487538160691\n",
      "EP4 EpisodeReward=-901.8757037942061\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-952.6341771336023\n",
      "EP1 EpisodeReward=-392.0032231174125\n",
      "EP2 EpisodeReward=-266.9632388633265\n",
      "EP3 EpisodeReward=-1552.280328151975\n",
      "EP4 EpisodeReward=-763.2500476415464\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-792.3071063309568\n",
      "EP1 EpisodeReward=-930.8162010994047\n",
      "EP2 EpisodeReward=-390.9575169339331\n",
      "EP3 EpisodeReward=-671.1699342106837\n",
      "EP4 EpisodeReward=-970.9310360833389\n",
      "Epoch=37\t Average reward=-878.6855958396972\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-909.5646695758657\n",
      "EP1 EpisodeReward=-1329.3756022978919\n",
      "EP2 EpisodeReward=-928.9892827280554\n",
      "EP3 EpisodeReward=-648.9670723413113\n",
      "EP4 EpisodeReward=-274.5726403556639\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-518.0699817575805\n",
      "EP1 EpisodeReward=-519.6643358249678\n",
      "EP2 EpisodeReward=-780.1989553876448\n",
      "EP3 EpisodeReward=-653.8238554740196\n",
      "EP4 EpisodeReward=-932.9663775080818\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-913.2617888106204\n",
      "EP1 EpisodeReward=-767.4027257149701\n",
      "EP2 EpisodeReward=-598.0498992780158\n",
      "EP3 EpisodeReward=-406.2541136522394\n",
      "EP4 EpisodeReward=-394.23268331002697\n",
      "Epoch=38\t Average reward=-533.9239003912576\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-852.2063793889308\n",
      "EP1 EpisodeReward=-912.4181488276818\n",
      "EP2 EpisodeReward=-541.5583024808353\n",
      "EP3 EpisodeReward=-752.4877193121321\n",
      "EP4 EpisodeReward=-398.69374775493696\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1113.5819311754285\n",
      "EP1 EpisodeReward=-783.1314356732995\n",
      "EP2 EpisodeReward=-789.0464721858733\n",
      "EP3 EpisodeReward=-802.1209915926221\n",
      "EP4 EpisodeReward=-1110.3616283246008\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-781.2097629849259\n",
      "EP1 EpisodeReward=-651.9920949828524\n",
      "EP2 EpisodeReward=-915.987556295029\n",
      "EP3 EpisodeReward=-535.0035326768832\n",
      "EP4 EpisodeReward=-405.75365120554534\n",
      "Epoch=39\t Average reward=-638.2696757616944\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1027.0139391073712\n",
      "EP1 EpisodeReward=-958.3149126560307\n",
      "EP2 EpisodeReward=-905.8178467293244\n",
      "EP3 EpisodeReward=-1560.711193488438\n",
      "EP4 EpisodeReward=-801.1274914199787\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-936.3547642492073\n",
      "EP1 EpisodeReward=-658.1872393945589\n",
      "EP2 EpisodeReward=-1004.3452289985578\n",
      "EP3 EpisodeReward=-924.3315691257101\n",
      "EP4 EpisodeReward=-137.45534084864732\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-658.9882802082652\n",
      "EP1 EpisodeReward=-479.2119629967635\n",
      "EP2 EpisodeReward=-1195.7153282504948\n",
      "EP3 EpisodeReward=-1200.878785988181\n",
      "EP4 EpisodeReward=-1197.123938456402\n",
      "Epoch=40\t Average reward=-711.9022569083427\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-528.3570972372239\n",
      "EP1 EpisodeReward=-1010.5006048746886\n",
      "EP2 EpisodeReward=-534.9343823257916\n",
      "EP3 EpisodeReward=-933.2385842663916\n",
      "EP4 EpisodeReward=-679.9189639142093\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-396.885423252349\n",
      "EP1 EpisodeReward=-1205.1421252529174\n",
      "EP2 EpisodeReward=-1269.1616578723763\n",
      "EP3 EpisodeReward=-1555.2186940389702\n",
      "EP4 EpisodeReward=-1189.660629257051\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-403.21881231103634\n",
      "EP1 EpisodeReward=-680.8611681989181\n",
      "EP2 EpisodeReward=-1064.4939227925086\n",
      "EP3 EpisodeReward=-756.2103923212607\n",
      "EP4 EpisodeReward=-1185.160955879955\n",
      "Epoch=41\t Average reward=-1018.2468496837383\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-266.1746744556236\n",
      "EP1 EpisodeReward=-1111.8903951925888\n",
      "EP2 EpisodeReward=-924.2048571049336\n",
      "EP3 EpisodeReward=-517.1649769666376\n",
      "EP4 EpisodeReward=-653.652533797225\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1555.0767979421596\n",
      "EP1 EpisodeReward=-389.90513779010354\n",
      "EP2 EpisodeReward=-1543.075811598569\n",
      "EP3 EpisodeReward=-794.7993607060303\n",
      "EP4 EpisodeReward=-1466.587664675887\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1417.1375451177996\n",
      "EP1 EpisodeReward=-917.1091579667726\n",
      "EP2 EpisodeReward=-394.1024322356079\n",
      "EP3 EpisodeReward=-632.085675434214\n",
      "EP4 EpisodeReward=-1107.8936848520977\n",
      "Epoch=42\t Average reward=-1076.04462777507\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-392.5136961799819\n",
      "EP1 EpisodeReward=-821.7057762681147\n",
      "EP2 EpisodeReward=-781.7059670676944\n",
      "EP3 EpisodeReward=-929.3752184369304\n",
      "EP4 EpisodeReward=-141.1217542731014\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-406.29069165527414\n",
      "EP1 EpisodeReward=-1016.7797942987954\n",
      "EP2 EpisodeReward=-1.5293194627299742\n",
      "EP3 EpisodeReward=-903.396568726266\n",
      "EP4 EpisodeReward=-1591.2708791832679\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-834.6570192519911\n",
      "EP1 EpisodeReward=-517.337251335648\n",
      "EP2 EpisodeReward=-854.1562245325459\n",
      "EP3 EpisodeReward=-985.5570484868804\n",
      "EP4 EpisodeReward=-1210.4675083218106\n",
      "Epoch=43\t Average reward=-980.9533805927267\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-257.9436161179352\n",
      "EP1 EpisodeReward=-387.7057509011367\n",
      "EP2 EpisodeReward=-320.05477283453143\n",
      "EP3 EpisodeReward=-799.4528339221957\n",
      "EP4 EpisodeReward=-995.3236737033435\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-127.99586302035748\n",
      "EP1 EpisodeReward=-1539.1449807777972\n",
      "EP2 EpisodeReward=-819.092116800287\n",
      "EP3 EpisodeReward=-690.0749741037833\n",
      "EP4 EpisodeReward=-393.6096108153906\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-772.5218261572552\n",
      "EP1 EpisodeReward=-549.3690608947996\n",
      "EP2 EpisodeReward=-960.4049842707914\n",
      "EP3 EpisodeReward=-920.6654571118627\n",
      "EP4 EpisodeReward=-778.0008434609751\n",
      "Epoch=44\t Average reward=-722.3113759932363\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1135.9274126216587\n",
      "EP1 EpisodeReward=-1237.9004180991974\n",
      "EP2 EpisodeReward=-1103.3580548445655\n",
      "EP3 EpisodeReward=-953.7098292700263\n",
      "EP4 EpisodeReward=-933.4232924654575\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-811.5547498528647\n",
      "EP1 EpisodeReward=-389.6212913335349\n",
      "EP2 EpisodeReward=-1571.6890907216123\n",
      "EP3 EpisodeReward=-927.1371665073525\n",
      "EP4 EpisodeReward=-651.3656194992866\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1578.7601204740974\n",
      "EP1 EpisodeReward=-1565.6249355833986\n",
      "EP2 EpisodeReward=-912.6745896084974\n",
      "EP3 EpisodeReward=-1068.5849593461264\n",
      "EP4 EpisodeReward=-1110.3042330243165\n",
      "Epoch=45\t Average reward=-898.3643816630201\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-783.9416894896109\n",
      "EP1 EpisodeReward=-1046.974223104728\n",
      "EP2 EpisodeReward=-987.5419214681297\n",
      "EP3 EpisodeReward=-1560.8095436303918\n",
      "EP4 EpisodeReward=-1416.788682461506\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-938.4588403688549\n",
      "EP1 EpisodeReward=-910.1336467025021\n",
      "EP2 EpisodeReward=-127.74995629348552\n",
      "EP3 EpisodeReward=-1575.6125741062074\n",
      "EP4 EpisodeReward=-1553.6925500923642\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-781.2630432071509\n",
      "EP1 EpisodeReward=-653.8408557856596\n",
      "EP2 EpisodeReward=-972.1053449834702\n",
      "EP3 EpisodeReward=-778.7411938320886\n",
      "EP4 EpisodeReward=-871.1449419871686\n",
      "Epoch=46\t Average reward=-1280.5420581803462\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-151.59564683506954\n",
      "EP1 EpisodeReward=-1071.7309415083132\n",
      "EP2 EpisodeReward=-1095.0847696803944\n",
      "EP3 EpisodeReward=-914.6385078226464\n",
      "EP4 EpisodeReward=-1078.7213968355068\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-900.1314793716192\n",
      "EP1 EpisodeReward=-1044.2336906316823\n",
      "EP2 EpisodeReward=-1041.6473558798446\n",
      "EP3 EpisodeReward=-1133.6128348281109\n",
      "EP4 EpisodeReward=-996.8964703998151\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1317.0963445376444\n",
      "EP1 EpisodeReward=-938.0738518356914\n",
      "EP2 EpisodeReward=-509.78991430941466\n",
      "EP3 EpisodeReward=-1560.331415186731\n",
      "EP4 EpisodeReward=-1581.9788146583153\n",
      "Epoch=47\t Average reward=-1219.1988939645455\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-977.934412244753\n",
      "EP1 EpisodeReward=-911.5865897359831\n",
      "EP2 EpisodeReward=-772.0804842128234\n",
      "EP3 EpisodeReward=-7.554222524558272\n",
      "EP4 EpisodeReward=-583.4239680556997\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1303.4288313054728\n",
      "EP1 EpisodeReward=-1070.461365678707\n",
      "EP2 EpisodeReward=-1189.9683894518166\n",
      "EP3 EpisodeReward=-1061.5464239073324\n",
      "EP4 EpisodeReward=-811.3131635383347\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1065.5654620317885\n",
      "EP1 EpisodeReward=-1550.4315914159538\n",
      "EP2 EpisodeReward=-1012.6716910331522\n",
      "EP3 EpisodeReward=-118.04515517095757\n",
      "EP4 EpisodeReward=-348.49003721361925\n",
      "Epoch=48\t Average reward=-581.0757229358845\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-890.7770080788434\n",
      "EP1 EpisodeReward=-1560.9651453660335\n",
      "EP2 EpisodeReward=-814.6958709371678\n",
      "EP3 EpisodeReward=-379.1792299800308\n",
      "EP4 EpisodeReward=-1570.2564498269903\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1552.0278275184962\n",
      "EP1 EpisodeReward=-771.9254522765351\n",
      "EP2 EpisodeReward=-1503.7738155797147\n",
      "EP3 EpisodeReward=-790.7375200293488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP4 EpisodeReward=-127.92345705645045\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-772.8557601938979\n",
      "EP1 EpisodeReward=-647.1494573879572\n",
      "EP2 EpisodeReward=-1548.7651839001862\n",
      "EP3 EpisodeReward=-274.744395393136\n",
      "EP4 EpisodeReward=-1505.3583057774822\n",
      "Epoch=49\t Average reward=-1067.8460708869743\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-130.46694032656458\n",
      "EP1 EpisodeReward=-1559.8406628482928\n",
      "EP2 EpisodeReward=-660.3087702132489\n",
      "EP3 EpisodeReward=-1440.2329841090186\n",
      "EP4 EpisodeReward=-1062.9186713733152\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-793.0870978970604\n",
      "EP1 EpisodeReward=-257.01527206437214\n",
      "EP2 EpisodeReward=-925.9275613836684\n",
      "EP3 EpisodeReward=-1058.5376069175254\n",
      "EP4 EpisodeReward=-1172.3514819052018\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1033.8233882245052\n",
      "EP1 EpisodeReward=-642.2922559535938\n",
      "EP2 EpisodeReward=-1576.8393621274151\n",
      "EP3 EpisodeReward=-1077.8466404869941\n",
      "EP4 EpisodeReward=-1515.3668493653286\n",
      "Epoch=50\t Average reward=-1250.212334214615\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1042.665778257782\n",
      "EP1 EpisodeReward=-938.3896049362804\n",
      "EP2 EpisodeReward=-707.404256082339\n",
      "EP3 EpisodeReward=-4.278094383409152\n",
      "EP4 EpisodeReward=-130.22996798970453\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1251.872817450246\n",
      "EP1 EpisodeReward=-1049.5166759109488\n",
      "EP2 EpisodeReward=-1564.7995408332042\n",
      "EP3 EpisodeReward=-1191.6575899135491\n",
      "EP4 EpisodeReward=-1382.9759483877397\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1052.7552765702362\n",
      "EP1 EpisodeReward=-258.95683112347166\n",
      "EP2 EpisodeReward=-260.77019450836445\n",
      "EP3 EpisodeReward=-1079.5336214544886\n",
      "EP4 EpisodeReward=-924.3690807100974\n",
      "Epoch=51\t Average reward=-812.5249990291805\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-789.6465134983874\n",
      "EP1 EpisodeReward=-270.49980569469966\n",
      "EP2 EpisodeReward=-912.9454718652877\n",
      "EP3 EpisodeReward=-1224.976035384942\n",
      "EP4 EpisodeReward=-546.8541752750077\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-262.6329837360064\n",
      "EP1 EpisodeReward=-711.8995925735225\n",
      "EP2 EpisodeReward=-913.5471097364791\n",
      "EP3 EpisodeReward=-1295.2136993091237\n",
      "EP4 EpisodeReward=-1558.2472077910409\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-391.26387295916663\n",
      "EP1 EpisodeReward=-643.1803301250355\n",
      "EP2 EpisodeReward=-262.64252498278296\n",
      "EP3 EpisodeReward=-127.42316903436803\n",
      "EP4 EpisodeReward=-1055.2115240743162\n",
      "Epoch=52\t Average reward=-1053.437635713455\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1167.5747285707942\n",
      "EP1 EpisodeReward=-1599.2796624082532\n",
      "EP2 EpisodeReward=-1471.1721861496176\n",
      "EP3 EpisodeReward=-1578.2847641869948\n",
      "EP4 EpisodeReward=-1584.0954354174796\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-651.3263459008232\n",
      "EP1 EpisodeReward=-1571.8346826640702\n",
      "EP2 EpisodeReward=-758.4477594491292\n",
      "EP3 EpisodeReward=-268.4155115057034\n",
      "EP4 EpisodeReward=-142.4973579429715\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1085.2467430609165\n",
      "EP1 EpisodeReward=-124.67331437553892\n",
      "EP2 EpisodeReward=-765.5505928257666\n",
      "EP3 EpisodeReward=-1094.4153604088533\n",
      "EP4 EpisodeReward=-748.5484391178724\n",
      "Epoch=53\t Average reward=-825.0470774927745\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-539.2986119452406\n",
      "EP1 EpisodeReward=-1575.2752884890485\n",
      "EP2 EpisodeReward=-397.78986065831225\n",
      "EP3 EpisodeReward=-799.9634392497252\n",
      "EP4 EpisodeReward=-1077.1290922562494\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-269.63790569532904\n",
      "EP1 EpisodeReward=-1189.275731795515\n",
      "EP2 EpisodeReward=-267.21072392155605\n",
      "EP3 EpisodeReward=-1096.0835066785817\n",
      "EP4 EpisodeReward=-1206.1228900829137\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-976.7508511911087\n",
      "EP1 EpisodeReward=-647.5044053796363\n",
      "EP2 EpisodeReward=-780.3056007535707\n",
      "EP3 EpisodeReward=-938.2817988846695\n",
      "EP4 EpisodeReward=-904.7907177252525\n",
      "Epoch=54\t Average reward=-1062.6809000214719\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-512.629005449713\n",
      "EP1 EpisodeReward=-1572.3539894071669\n",
      "EP2 EpisodeReward=-921.8896031131511\n",
      "EP3 EpisodeReward=-796.5706968642488\n",
      "EP4 EpisodeReward=-933.441997974223\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-231.19243200614912\n",
      "EP1 EpisodeReward=-1252.9164562533058\n",
      "EP2 EpisodeReward=-1186.2981046448676\n",
      "EP3 EpisodeReward=-761.3765088044627\n",
      "EP4 EpisodeReward=-969.9824312573354\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-209.4057372168878\n",
      "EP1 EpisodeReward=-784.2525068880392\n",
      "EP2 EpisodeReward=-784.9938227992595\n",
      "EP3 EpisodeReward=-662.8260268570976\n",
      "EP4 EpisodeReward=-389.97106329113865\n",
      "Epoch=55\t Average reward=-764.4651641742324\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-262.3006348035307\n",
      "EP1 EpisodeReward=-552.1682400934328\n",
      "EP2 EpisodeReward=-775.4405562425088\n",
      "EP3 EpisodeReward=-55.923954201889586\n",
      "EP4 EpisodeReward=-643.0786545680934\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-394.76966503020924\n",
      "EP1 EpisodeReward=-257.16414658120107\n",
      "EP2 EpisodeReward=-134.40572106206952\n",
      "EP3 EpisodeReward=-1076.8955833963994\n",
      "EP4 EpisodeReward=-1380.3436597026634\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-524.6809872080731\n",
      "EP1 EpisodeReward=-1560.9332008131114\n",
      "EP2 EpisodeReward=-702.7283346606793\n",
      "EP3 EpisodeReward=-1053.9652817771444\n",
      "EP4 EpisodeReward=-1200.0137471374248\n",
      "Epoch=56\t Average reward=-1074.4786871360604\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-617.2073338296415\n",
      "EP1 EpisodeReward=-513.6156087315155\n",
      "EP2 EpisodeReward=-386.6815648550621\n",
      "EP3 EpisodeReward=-1262.2970290076769\n",
      "EP4 EpisodeReward=-491.87811774946505\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-131.82023085216096\n",
      "EP1 EpisodeReward=-643.5044041565166\n",
      "EP2 EpisodeReward=-719.0889502061855\n",
      "EP3 EpisodeReward=-502.87488273697517\n",
      "EP4 EpisodeReward=-519.475583536182\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-130.06221873115396\n",
      "EP1 EpisodeReward=-261.86198933283873\n",
      "EP2 EpisodeReward=-826.5141001625507\n",
      "EP3 EpisodeReward=-796.3496900125464\n",
      "EP4 EpisodeReward=-534.969968563296\n",
      "Epoch=57\t Average reward=-515.441223282981\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-260.69509200629363\n",
      "EP1 EpisodeReward=-132.04987351159676\n",
      "EP2 EpisodeReward=-256.7671243481485\n",
      "EP3 EpisodeReward=-124.17653915030412\n",
      "EP4 EpisodeReward=-134.3653228489609\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-529.9869194905468\n",
      "EP1 EpisodeReward=-639.4662806551463\n",
      "EP2 EpisodeReward=-1535.600779775118\n",
      "EP3 EpisodeReward=-428.1843101650828\n",
      "EP4 EpisodeReward=-395.7406973488545\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-528.0463111762261\n",
      "EP1 EpisodeReward=-259.05385884102907\n",
      "EP2 EpisodeReward=-266.06432623778306\n",
      "EP3 EpisodeReward=-782.8528207860053\n",
      "EP4 EpisodeReward=-972.0205726045983\n",
      "Epoch=58\t Average reward=-500.7088642674712\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-133.15985349709123\n",
      "EP1 EpisodeReward=-386.56137962775347\n",
      "EP2 EpisodeReward=-428.1131643811341\n",
      "EP3 EpisodeReward=-133.79110100079978\n",
      "EP4 EpisodeReward=-922.0299069691685\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-264.18579840609567\n",
      "EP1 EpisodeReward=-1172.888079677766\n",
      "EP2 EpisodeReward=-1338.2374844234885\n",
      "EP3 EpisodeReward=-1499.7765755244473\n",
      "EP4 EpisodeReward=-1360.0711979923583\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-264.04397146788045\n",
      "EP1 EpisodeReward=-139.2819476636376\n",
      "EP2 EpisodeReward=-380.7867434498294\n",
      "EP3 EpisodeReward=-129.3383035587049\n",
      "EP4 EpisodeReward=-947.181665264692\n",
      "Epoch=59\t Average reward=-1076.4275900754062\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1155.5280438989025\n",
      "EP1 EpisodeReward=-127.97123710158918\n",
      "EP2 EpisodeReward=-282.7525456222378\n",
      "EP3 EpisodeReward=-804.5937443711512\n",
      "EP4 EpisodeReward=-277.5349860900495\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-269.2128712498571\n",
      "EP1 EpisodeReward=-130.17078391661263\n",
      "EP2 EpisodeReward=-1075.8598130808805\n",
      "EP3 EpisodeReward=-1155.3007609595572\n",
      "EP4 EpisodeReward=-526.7287292377767\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-130.6939117851958\n",
      "EP1 EpisodeReward=-1.1276936260138215\n",
      "EP2 EpisodeReward=-282.76075165777246\n",
      "EP3 EpisodeReward=-1125.8843364573154\n",
      "EP4 EpisodeReward=-391.62200902783155\n",
      "Epoch=60\t Average reward=-398.6285747852193\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-813.089433382537\n",
      "EP1 EpisodeReward=-413.64418363174974\n",
      "EP2 EpisodeReward=-287.2730293657963\n",
      "EP3 EpisodeReward=-133.16525029439845\n",
      "EP4 EpisodeReward=-656.1346302527729\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-260.5451967051056\n",
      "EP1 EpisodeReward=-386.0918409561644\n",
      "EP2 EpisodeReward=-660.8203022130554\n",
      "EP3 EpisodeReward=-132.38302228307165\n",
      "EP4 EpisodeReward=-128.06802393557487\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-384.4559335865418\n",
      "EP1 EpisodeReward=-133.59995441515443\n",
      "EP2 EpisodeReward=-653.7440803467326\n",
      "EP3 EpisodeReward=-132.2410489560164\n",
      "EP4 EpisodeReward=-1582.2839675576893\n",
      "Epoch=61\t Average reward=-788.8288739153458\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-261.4481358258392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP1 EpisodeReward=-648.8189800180219\n",
      "EP2 EpisodeReward=-659.2986785497405\n",
      "EP3 EpisodeReward=-1236.5653451256662\n",
      "EP4 EpisodeReward=-1359.4075932773262\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-135.62120900925433\n",
      "EP1 EpisodeReward=-408.0468126369727\n",
      "EP2 EpisodeReward=-527.2714539398801\n",
      "EP3 EpisodeReward=-393.00091943870484\n",
      "EP4 EpisodeReward=-421.97872120160906\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-129.45099362264096\n",
      "EP1 EpisodeReward=-379.74938086332514\n",
      "EP2 EpisodeReward=-784.6973246633355\n",
      "EP3 EpisodeReward=-1209.9178479806762\n",
      "EP4 EpisodeReward=-1285.6589288319683\n",
      "Epoch=62\t Average reward=-1022.3484144369678\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1081.3828394143204\n",
      "EP1 EpisodeReward=-126.93953705783194\n",
      "EP2 EpisodeReward=-1084.9754151729708\n",
      "EP3 EpisodeReward=-924.887634440992\n",
      "EP4 EpisodeReward=-1094.039762647558\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-260.435241879705\n",
      "EP1 EpisodeReward=-653.9618464547968\n",
      "EP2 EpisodeReward=-127.84397679016706\n",
      "EP3 EpisodeReward=-258.63993981184376\n",
      "EP4 EpisodeReward=-789.9231784750916\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-388.609782326338\n",
      "EP1 EpisodeReward=-273.7484967996804\n",
      "EP2 EpisodeReward=-3.3840651259737378\n",
      "EP3 EpisodeReward=-145.88868519196453\n",
      "EP4 EpisodeReward=-259.7215610105027\n",
      "Epoch=63\t Average reward=-714.5615007110508\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-128.32862545148967\n",
      "EP1 EpisodeReward=-1579.87871511928\n",
      "EP2 EpisodeReward=-426.0927110617691\n",
      "EP3 EpisodeReward=-1358.3498282813189\n",
      "EP4 EpisodeReward=-1195.928349246498\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-132.69463453900568\n",
      "EP1 EpisodeReward=-1236.4403540445635\n",
      "EP2 EpisodeReward=-128.6267546931296\n",
      "EP3 EpisodeReward=-460.13114043021733\n",
      "EP4 EpisodeReward=-652.5978374985533\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-128.9785169608496\n",
      "EP1 EpisodeReward=-632.9071735361348\n",
      "EP2 EpisodeReward=-1440.7254793108573\n",
      "EP3 EpisodeReward=-263.6890994929763\n",
      "EP4 EpisodeReward=-1108.3347886445665\n",
      "Epoch=64\t Average reward=-985.6203251298726\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1083.5697539422608\n",
      "EP1 EpisodeReward=-1216.3765929568442\n",
      "EP2 EpisodeReward=-1144.2007093691864\n",
      "EP3 EpisodeReward=-600.1212246735071\n",
      "EP4 EpisodeReward=-810.6011953630538\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1151.1557218543876\n",
      "EP1 EpisodeReward=-1269.7017351788222\n",
      "EP2 EpisodeReward=-653.1928164002267\n",
      "EP3 EpisodeReward=-1069.0190131642837\n",
      "EP4 EpisodeReward=-1203.3455864741334\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1202.8942674749217\n",
      "EP1 EpisodeReward=-1258.2702316451628\n",
      "EP2 EpisodeReward=-1076.8837526254713\n",
      "EP3 EpisodeReward=-1251.1705956007256\n",
      "EP4 EpisodeReward=-960.3330399010288\n",
      "Epoch=65\t Average reward=-991.426607246072\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-270.47132274492907\n",
      "EP1 EpisodeReward=-782.9905257486541\n",
      "EP2 EpisodeReward=-130.31356126574124\n",
      "EP3 EpisodeReward=-657.681560629422\n",
      "EP4 EpisodeReward=-125.79512443526546\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1204.6090647882806\n",
      "EP1 EpisodeReward=-1084.6297088388203\n",
      "EP2 EpisodeReward=-258.4497832773294\n",
      "EP3 EpisodeReward=-384.2599182297443\n",
      "EP4 EpisodeReward=-253.53446844039382\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-522.8288579202714\n",
      "EP1 EpisodeReward=-1539.0595477096613\n",
      "EP2 EpisodeReward=-946.6972270009181\n",
      "EP3 EpisodeReward=-523.8813474636255\n",
      "EP4 EpisodeReward=-521.9351741632358\n",
      "Epoch=66\t Average reward=-300.42158901296506\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1562.5207715089066\n",
      "EP1 EpisodeReward=-1579.9969613666115\n",
      "EP2 EpisodeReward=-261.67390560270513\n",
      "EP3 EpisodeReward=-643.8184017355645\n",
      "EP4 EpisodeReward=-1067.4139851605291\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-0.9173149595192881\n",
      "EP1 EpisodeReward=-667.4142522387522\n",
      "EP2 EpisodeReward=-883.2566764931074\n",
      "EP3 EpisodeReward=-515.7342300813411\n",
      "EP4 EpisodeReward=-1.758962259172458\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-265.48887516622364\n",
      "EP1 EpisodeReward=-1570.9701123413417\n",
      "EP2 EpisodeReward=-385.29592812880276\n",
      "EP3 EpisodeReward=-390.2128751722116\n",
      "EP4 EpisodeReward=-659.8995871588359\n",
      "Epoch=67\t Average reward=-576.3575115261791\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-139.54328101274135\n",
      "EP1 EpisodeReward=-129.04712228772203\n",
      "EP2 EpisodeReward=-386.8352418628172\n",
      "EP3 EpisodeReward=-643.9335258808727\n",
      "EP4 EpisodeReward=-129.40414149530721\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-647.9685967355243\n",
      "EP1 EpisodeReward=-1495.0070030718107\n",
      "EP2 EpisodeReward=-561.7690828478042\n",
      "EP3 EpisodeReward=-399.9002820189162\n",
      "EP4 EpisodeReward=-802.8286429957683\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-521.9887593147423\n",
      "EP1 EpisodeReward=-522.0493746192279\n",
      "EP2 EpisodeReward=-638.7818837424991\n",
      "EP3 EpisodeReward=-586.9512899457391\n",
      "EP4 EpisodeReward=-644.5687368370637\n",
      "Epoch=68\t Average reward=-525.6005071093797\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-526.1570919209447\n",
      "EP1 EpisodeReward=-651.588868000721\n",
      "EP2 EpisodeReward=-667.240996572984\n",
      "EP3 EpisodeReward=-660.2335366690074\n",
      "EP4 EpisodeReward=-788.4760039517863\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1542.8352286207605\n",
      "EP1 EpisodeReward=-408.85732299385364\n",
      "EP2 EpisodeReward=-271.23639344755287\n",
      "EP3 EpisodeReward=-542.3872272896431\n",
      "EP4 EpisodeReward=-892.3808840651337\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-945.8731621278109\n",
      "EP1 EpisodeReward=-398.70023087878644\n",
      "EP2 EpisodeReward=-824.9263810383587\n",
      "EP3 EpisodeReward=-1553.9346031892997\n",
      "EP4 EpisodeReward=-967.2305723436024\n",
      "Epoch=69\t Average reward=-882.6958201201742\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-659.5272679836934\n",
      "EP1 EpisodeReward=-794.4087802849501\n",
      "EP2 EpisodeReward=-132.67913483331338\n",
      "EP3 EpisodeReward=-806.543486703952\n",
      "EP4 EpisodeReward=-524.3756123130522\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-656.6133908334368\n",
      "EP1 EpisodeReward=-1277.307387389467\n",
      "EP2 EpisodeReward=-1236.873392797244\n",
      "EP3 EpisodeReward=-1199.1995193224425\n",
      "EP4 EpisodeReward=-1040.9023773928752\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-938.8410884744745\n",
      "EP1 EpisodeReward=-893.4840205885145\n",
      "EP2 EpisodeReward=-990.5054197184324\n",
      "EP3 EpisodeReward=-1569.0777803556673\n",
      "EP4 EpisodeReward=-1182.8714107337462\n",
      "Epoch=70\t Average reward=-916.0498001465579\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-905.5761271769403\n",
      "EP1 EpisodeReward=-1246.7135972308256\n",
      "EP2 EpisodeReward=-850.7921126360029\n",
      "EP3 EpisodeReward=-1132.578487669112\n",
      "EP4 EpisodeReward=-917.7911256523306\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-784.2043735799915\n",
      "EP1 EpisodeReward=-915.7271202694361\n",
      "EP2 EpisodeReward=-525.6172531389532\n",
      "EP3 EpisodeReward=-789.0708831898502\n",
      "EP4 EpisodeReward=-1314.5373445914333\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-687.7848126336044\n",
      "EP1 EpisodeReward=-533.3910332110377\n",
      "EP2 EpisodeReward=-533.7836359277604\n",
      "EP3 EpisodeReward=-660.1761744540029\n",
      "EP4 EpisodeReward=-825.3843828978708\n",
      "Epoch=71\t Average reward=-1019.2376177138782\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-829.743602992355\n",
      "EP1 EpisodeReward=-771.5669886635101\n",
      "EP2 EpisodeReward=-517.5382261244708\n",
      "EP3 EpisodeReward=-696.3656095528378\n",
      "EP4 EpisodeReward=-657.750502666747\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-536.2802823279834\n",
      "EP1 EpisodeReward=-1346.071801594451\n",
      "EP2 EpisodeReward=-824.684043857789\n",
      "EP3 EpisodeReward=-584.6131092348719\n",
      "EP4 EpisodeReward=-645.4859569689336\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-537.7251250172344\n",
      "EP1 EpisodeReward=-806.8537550983974\n",
      "EP2 EpisodeReward=-441.33762610492425\n",
      "EP3 EpisodeReward=-297.15123922116413\n",
      "EP4 EpisodeReward=-653.7530041301366\n",
      "Epoch=72\t Average reward=-652.3298212552723\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1554.7397290865729\n",
      "EP1 EpisodeReward=-779.2270069451803\n",
      "EP2 EpisodeReward=-1370.0183608177315\n",
      "EP3 EpisodeReward=-646.9189502202826\n",
      "EP4 EpisodeReward=-1080.757454094912\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-887.7932762451633\n",
      "EP1 EpisodeReward=-938.7736610435138\n",
      "EP2 EpisodeReward=-936.7278838063835\n",
      "EP3 EpisodeReward=-860.9676729165714\n",
      "EP4 EpisodeReward=-1314.6433229865281\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-933.1313094731759\n",
      "EP1 EpisodeReward=-1332.5925150640862\n",
      "EP2 EpisodeReward=-895.4657291224185\n",
      "EP3 EpisodeReward=-1032.1575282218976\n",
      "EP4 EpisodeReward=-956.4790123758899\n",
      "Epoch=73\t Average reward=-1117.2932631524434\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1084.2701071901781\n",
      "EP1 EpisodeReward=-396.29798677856076\n",
      "EP2 EpisodeReward=-361.4975200695958\n",
      "EP3 EpisodeReward=-940.8070389390299\n",
      "EP4 EpisodeReward=-974.8436877381862\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-531.470354908726\n",
      "EP1 EpisodeReward=-1066.4789137415626\n",
      "EP2 EpisodeReward=-808.6728499324586\n",
      "EP3 EpisodeReward=-1279.709868842947\n",
      "EP4 EpisodeReward=-1088.0512591272382\n",
      "Training Agent 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP0 EpisodeReward=-658.1508237698991\n",
      "EP1 EpisodeReward=-533.8981173709428\n",
      "EP2 EpisodeReward=-722.3680653734565\n",
      "EP3 EpisodeReward=-924.4546803858444\n",
      "EP4 EpisodeReward=-780.7233802791905\n",
      "Epoch=74\t Average reward=-947.8727757148718\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-822.526254037127\n",
      "EP1 EpisodeReward=-1043.0421527149458\n",
      "EP2 EpisodeReward=-1186.361278587714\n",
      "EP3 EpisodeReward=-561.8895791439709\n",
      "EP4 EpisodeReward=-630.8149847355489\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-661.1858830621646\n",
      "EP1 EpisodeReward=-777.9846353924448\n",
      "EP2 EpisodeReward=-783.9926375743468\n",
      "EP3 EpisodeReward=-903.8254736684062\n",
      "EP4 EpisodeReward=-1206.317535543852\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1032.0682275495724\n",
      "EP1 EpisodeReward=-653.4400488350506\n",
      "EP2 EpisodeReward=-1562.2276026875938\n",
      "EP3 EpisodeReward=-1096.7607808311827\n",
      "EP4 EpisodeReward=-1048.6700973314264\n",
      "Epoch=75\t Average reward=-961.9342058702758\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-646.7264398343622\n",
      "EP1 EpisodeReward=-1049.0767832529257\n",
      "EP2 EpisodeReward=-1275.3188886617409\n",
      "EP3 EpisodeReward=-1311.9702985123135\n",
      "EP4 EpisodeReward=-1182.3046780423138\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-651.0844196142499\n",
      "EP1 EpisodeReward=-528.8375116852557\n",
      "EP2 EpisodeReward=-533.5790083594205\n",
      "EP3 EpisodeReward=-529.520666407695\n",
      "EP4 EpisodeReward=-540.8087950655173\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-529.7213394627018\n",
      "EP1 EpisodeReward=-643.8634508934681\n",
      "EP2 EpisodeReward=-797.3535124531389\n",
      "EP3 EpisodeReward=-651.20339609889\n",
      "EP4 EpisodeReward=-1083.8695839426537\n",
      "Epoch=76\t Average reward=-935.6610190168282\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-988.7894912605267\n",
      "EP1 EpisodeReward=-1270.9132479653797\n",
      "EP2 EpisodeReward=-1223.6730396437479\n",
      "EP3 EpisodeReward=-1079.0925532323135\n",
      "EP4 EpisodeReward=-1089.6019499438103\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1206.0532021956278\n",
      "EP1 EpisodeReward=-943.3782487222073\n",
      "EP2 EpisodeReward=-792.2331432880914\n",
      "EP3 EpisodeReward=-621.010476685721\n",
      "EP4 EpisodeReward=-991.2856796585219\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-658.1696578371226\n",
      "EP1 EpisodeReward=-564.9618187790815\n",
      "EP2 EpisodeReward=-647.9414654026199\n",
      "EP3 EpisodeReward=-1117.2531947260907\n",
      "EP4 EpisodeReward=-781.0960130620169\n",
      "Epoch=77\t Average reward=-953.994547554783\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-659.342711446423\n",
      "EP1 EpisodeReward=-672.5662845411874\n",
      "EP2 EpisodeReward=-776.5168232177326\n",
      "EP3 EpisodeReward=-969.6314332853016\n",
      "EP4 EpisodeReward=-825.7187241883353\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-779.7857221546255\n",
      "EP1 EpisodeReward=-543.4245306188535\n",
      "EP2 EpisodeReward=-1157.6177937980888\n",
      "EP3 EpisodeReward=-514.7427604191142\n",
      "EP4 EpisodeReward=-1179.077971977887\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-650.1217837388871\n",
      "EP1 EpisodeReward=-794.2002647120008\n",
      "EP2 EpisodeReward=-734.607853440263\n",
      "EP3 EpisodeReward=-1239.8747708331086\n",
      "EP4 EpisodeReward=-1234.02059772063\n",
      "Epoch=78\t Average reward=-1079.6057646289507\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-655.58063815729\n",
      "EP1 EpisodeReward=-906.1943203626934\n",
      "EP2 EpisodeReward=-739.9787843790149\n",
      "EP3 EpisodeReward=-1097.8766618165153\n",
      "EP4 EpisodeReward=-1231.6659569984347\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-790.0289808179255\n",
      "EP1 EpisodeReward=-566.1680245299061\n",
      "EP2 EpisodeReward=-588.7099166016646\n",
      "EP3 EpisodeReward=-779.2760381412356\n",
      "EP4 EpisodeReward=-524.4265498326257\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-660.2190515210756\n",
      "EP1 EpisodeReward=-764.25459766789\n",
      "EP2 EpisodeReward=-887.8665450454646\n",
      "EP3 EpisodeReward=-932.7840863448572\n",
      "EP4 EpisodeReward=-875.0390135846693\n",
      "Epoch=79\t Average reward=-877.0438401385765\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-807.8877303039857\n",
      "EP1 EpisodeReward=-839.3461672758931\n",
      "EP2 EpisodeReward=-273.3817033324962\n",
      "EP3 EpisodeReward=-719.1415486704121\n",
      "EP4 EpisodeReward=-677.9149644893573\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-632.6650513038278\n",
      "EP1 EpisodeReward=-1086.48256450968\n",
      "EP2 EpisodeReward=-561.4929539571796\n",
      "EP3 EpisodeReward=-849.6471229685619\n",
      "EP4 EpisodeReward=-534.8374931681483\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1191.346346564257\n",
      "EP1 EpisodeReward=-1206.078772223227\n",
      "EP2 EpisodeReward=-1080.2928455549343\n",
      "EP3 EpisodeReward=-531.227633452992\n",
      "EP4 EpisodeReward=-674.8947748543545\n",
      "Epoch=80\t Average reward=-629.21574417062\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-533.751283531534\n",
      "EP1 EpisodeReward=-855.2237822903394\n",
      "EP2 EpisodeReward=-655.6903418212613\n",
      "EP3 EpisodeReward=-461.15754105774676\n",
      "EP4 EpisodeReward=-763.9341789486472\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-407.67654643446275\n",
      "EP1 EpisodeReward=-800.9357750973703\n",
      "EP2 EpisodeReward=-803.0269723571279\n",
      "EP3 EpisodeReward=-527.8797056173076\n",
      "EP4 EpisodeReward=-523.8041228445186\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-532.963674855911\n",
      "EP1 EpisodeReward=-660.1675630412212\n",
      "EP2 EpisodeReward=-634.2819690367899\n",
      "EP3 EpisodeReward=-654.1487192050237\n",
      "EP4 EpisodeReward=-271.5292596462236\n",
      "Epoch=81\t Average reward=-519.7558538131298\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-520.3903088796782\n",
      "EP1 EpisodeReward=-656.9307512537833\n",
      "EP2 EpisodeReward=-398.6418215591143\n",
      "EP3 EpisodeReward=-829.8003686589948\n",
      "EP4 EpisodeReward=-1268.1352183534377\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-528.630930343701\n",
      "EP1 EpisodeReward=-1025.4104797280716\n",
      "EP2 EpisodeReward=-529.6491622414716\n",
      "EP3 EpisodeReward=-779.117005007243\n",
      "EP4 EpisodeReward=-405.18297625195885\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-655.1889623349442\n",
      "EP1 EpisodeReward=-532.1866852333077\n",
      "EP2 EpisodeReward=-1550.3558211584705\n",
      "EP3 EpisodeReward=-659.9524492285959\n",
      "EP4 EpisodeReward=-1115.9259596818029\n",
      "Epoch=82\t Average reward=-929.7480514290664\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-539.8760642256378\n",
      "EP1 EpisodeReward=-674.008488097743\n",
      "EP2 EpisodeReward=-562.8597314550119\n",
      "EP3 EpisodeReward=-1535.4429463080608\n",
      "EP4 EpisodeReward=-1005.3371263699114\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-389.54566497114763\n",
      "EP1 EpisodeReward=-1072.8365299925842\n",
      "EP2 EpisodeReward=-1092.0102023113154\n",
      "EP3 EpisodeReward=-430.1435742776638\n",
      "EP4 EpisodeReward=-656.8609276262426\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-267.1998374930475\n",
      "EP1 EpisodeReward=-1214.1454208768137\n",
      "EP2 EpisodeReward=-1235.2887193740855\n",
      "EP3 EpisodeReward=-1288.4956584426145\n",
      "EP4 EpisodeReward=-128.72607354243655\n",
      "Epoch=83\t Average reward=-596.9747091795302\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1536.5018359603234\n",
      "EP1 EpisodeReward=-135.4891230203531\n",
      "EP2 EpisodeReward=-654.2648826529444\n",
      "EP3 EpisodeReward=-1327.600043421477\n",
      "EP4 EpisodeReward=-277.9675096548807\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-267.284709742272\n",
      "EP1 EpisodeReward=-1081.9661462925537\n",
      "EP2 EpisodeReward=-127.33733368193674\n",
      "EP3 EpisodeReward=-270.2237932885063\n",
      "EP4 EpisodeReward=-1067.4306483641544\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1053.4092478643825\n",
      "EP1 EpisodeReward=-1201.3222366514374\n",
      "EP2 EpisodeReward=-540.8693161733846\n",
      "EP3 EpisodeReward=-1224.8133041267427\n",
      "EP4 EpisodeReward=-1212.9484706239455\n",
      "Epoch=84\t Average reward=-852.7822095476603\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1069.6595817749999\n",
      "EP1 EpisodeReward=-11.528772571159882\n",
      "EP2 EpisodeReward=-497.97545721937973\n",
      "EP3 EpisodeReward=-501.2656681292854\n",
      "EP4 EpisodeReward=-1468.8926820427062\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-871.4148904963519\n",
      "EP1 EpisodeReward=-683.5260457707526\n",
      "EP2 EpisodeReward=-787.9867290042606\n",
      "EP3 EpisodeReward=-1306.2358626791038\n",
      "EP4 EpisodeReward=-1290.4095668450277\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-571.0484400176239\n",
      "EP1 EpisodeReward=-532.2868228627971\n",
      "EP2 EpisodeReward=-130.85157066361154\n",
      "EP3 EpisodeReward=-968.7762748575606\n",
      "EP4 EpisodeReward=-806.9553321105477\n",
      "Epoch=85\t Average reward=-1188.7525269994273\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1124.7513602723411\n",
      "EP1 EpisodeReward=-1095.271522311258\n",
      "EP2 EpisodeReward=-528.8550746074801\n",
      "EP3 EpisodeReward=-402.39372127730735\n",
      "EP4 EpisodeReward=-268.94023217808945\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1123.6421533716937\n",
      "EP1 EpisodeReward=-998.5787502127417\n",
      "EP2 EpisodeReward=-672.8159931780605\n",
      "EP3 EpisodeReward=-393.5019980558739\n",
      "EP4 EpisodeReward=-924.4974718789048\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-679.6138713148903\n",
      "EP1 EpisodeReward=-656.0611054125716\n",
      "EP2 EpisodeReward=-648.9390688949891\n",
      "EP3 EpisodeReward=-1082.1411546997103\n",
      "EP4 EpisodeReward=-1081.9804609108444\n",
      "Epoch=86\t Average reward=-758.4727216559462\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-666.7886612325823\n",
      "EP1 EpisodeReward=-552.6962488732338\n",
      "EP2 EpisodeReward=-736.2156900409319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP3 EpisodeReward=-779.0720623963571\n",
      "EP4 EpisodeReward=-648.3325459292404\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-643.7966330889287\n",
      "EP1 EpisodeReward=-1092.902108474552\n",
      "EP2 EpisodeReward=-1217.0050654743582\n",
      "EP3 EpisodeReward=-1336.712286971902\n",
      "EP4 EpisodeReward=-1423.2271037599533\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-525.8118085108232\n",
      "EP1 EpisodeReward=-926.1542866069987\n",
      "EP2 EpisodeReward=-941.3208768962585\n",
      "EP3 EpisodeReward=-402.18475129258866\n",
      "EP4 EpisodeReward=-611.979928897726\n",
      "Epoch=87\t Average reward=-894.5131928623065\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-523.563647967728\n",
      "EP1 EpisodeReward=-263.5511297830764\n",
      "EP2 EpisodeReward=-533.5504980615223\n",
      "EP3 EpisodeReward=-524.9889738192319\n",
      "EP4 EpisodeReward=-269.69594760485563\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-658.2368343053081\n",
      "EP1 EpisodeReward=-652.4991679630031\n",
      "EP2 EpisodeReward=-825.2110859561964\n",
      "EP3 EpisodeReward=-1218.7687821481752\n",
      "EP4 EpisodeReward=-1310.9829328209707\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-931.9684158316553\n",
      "EP1 EpisodeReward=-925.6031749734731\n",
      "EP2 EpisodeReward=-1081.7709136786075\n",
      "EP3 EpisodeReward=-914.5337804226257\n",
      "EP4 EpisodeReward=-966.0951704891719\n",
      "Epoch=88\t Average reward=-848.9246836383327\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-933.0148332267702\n",
      "EP1 EpisodeReward=-637.9365750426731\n",
      "EP2 EpisodeReward=-398.17335767725893\n",
      "EP3 EpisodeReward=-522.3959471584556\n",
      "EP4 EpisodeReward=-133.81065052225273\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-516.27696989931\n",
      "EP1 EpisodeReward=-1081.2070539853617\n",
      "EP2 EpisodeReward=-643.9668853222752\n",
      "EP3 EpisodeReward=-386.1209533970981\n",
      "EP4 EpisodeReward=-1059.774986828849\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1057.2666703020375\n",
      "EP1 EpisodeReward=-396.04503562052264\n",
      "EP2 EpisodeReward=-1212.8193904160732\n",
      "EP3 EpisodeReward=-1200.1627820613855\n",
      "EP4 EpisodeReward=-1.235318339455276\n",
      "Epoch=89\t Average reward=-398.2736518968523\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-274.5170989863769\n",
      "EP1 EpisodeReward=-794.8757270411014\n",
      "EP2 EpisodeReward=-915.5056262033353\n",
      "EP3 EpisodeReward=-619.7371295886585\n",
      "EP4 EpisodeReward=-423.65954327651934\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-272.4461274912728\n",
      "EP1 EpisodeReward=-519.0612340605085\n",
      "EP2 EpisodeReward=-727.7023707344771\n",
      "EP3 EpisodeReward=-396.9314424893346\n",
      "EP4 EpisodeReward=-776.1472887857626\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-276.97351714967715\n",
      "EP1 EpisodeReward=-2.030956717655469\n",
      "EP2 EpisodeReward=-398.52964879299526\n",
      "EP3 EpisodeReward=-1192.0193252956126\n",
      "EP4 EpisodeReward=-385.4581359436916\n",
      "Epoch=90\t Average reward=-528.4216560019912\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1063.7966333619972\n",
      "EP1 EpisodeReward=-2.13152722761109\n",
      "EP2 EpisodeReward=-377.0835328630601\n",
      "EP3 EpisodeReward=-1044.3749450201703\n",
      "EP4 EpisodeReward=-703.6025784310848\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-7.509559808882541\n",
      "EP1 EpisodeReward=-639.9039493162203\n",
      "EP2 EpisodeReward=-646.5932125966008\n",
      "EP3 EpisodeReward=-848.2784821623085\n",
      "EP4 EpisodeReward=-514.8559680287588\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-137.28407656983438\n",
      "EP1 EpisodeReward=-256.280241700041\n",
      "EP2 EpisodeReward=-526.6828214126106\n",
      "EP3 EpisodeReward=-649.5671561164417\n",
      "EP4 EpisodeReward=-557.0027151623042\n",
      "Epoch=91\t Average reward=-591.8204205407159\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-927.6916650960693\n",
      "EP1 EpisodeReward=-927.4164120074571\n",
      "EP2 EpisodeReward=-879.2852020234774\n",
      "EP3 EpisodeReward=-645.2381184818714\n",
      "EP4 EpisodeReward=-916.6602682276725\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-649.2283330059745\n",
      "EP1 EpisodeReward=-649.405350288613\n",
      "EP2 EpisodeReward=-675.3972670482912\n",
      "EP3 EpisodeReward=-1090.4506075913325\n",
      "EP4 EpisodeReward=-305.48726998133094\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-677.2375842203096\n",
      "EP1 EpisodeReward=-768.7216101726592\n",
      "EP2 EpisodeReward=-843.3385381893403\n",
      "EP3 EpisodeReward=-1044.3759225036852\n",
      "EP4 EpisodeReward=-1526.6120931548223\n",
      "Epoch=92\t Average reward=-916.2532104546086\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-759.4379446491511\n",
      "EP1 EpisodeReward=-687.9624336812932\n",
      "EP2 EpisodeReward=-923.9858872049953\n",
      "EP3 EpisodeReward=-956.7273774318625\n",
      "EP4 EpisodeReward=-904.3842216569005\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-768.0214745413778\n",
      "EP1 EpisodeReward=-849.751247077567\n",
      "EP2 EpisodeReward=-831.5859563349417\n",
      "EP3 EpisodeReward=-390.429618634167\n",
      "EP4 EpisodeReward=-892.105046929013\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-523.1453236249068\n",
      "EP1 EpisodeReward=-774.3127576366627\n",
      "EP2 EpisodeReward=-788.8799153056827\n",
      "EP3 EpisodeReward=-541.7126516648718\n",
      "EP4 EpisodeReward=-821.0582300553081\n",
      "Epoch=93\t Average reward=-872.5158328804073\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-775.3565539001132\n",
      "EP1 EpisodeReward=-391.9662092888202\n",
      "EP2 EpisodeReward=-675.7763350749894\n",
      "EP3 EpisodeReward=-402.7034470334217\n",
      "EP4 EpisodeReward=-386.37722202880343\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-944.0181096460694\n",
      "EP1 EpisodeReward=-772.6984793209979\n",
      "EP2 EpisodeReward=-933.0145141459217\n",
      "EP3 EpisodeReward=-897.7124270196933\n",
      "EP4 EpisodeReward=-131.43625162318668\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-758.0909885048243\n",
      "EP1 EpisodeReward=-902.7103653531399\n",
      "EP2 EpisodeReward=-990.0110703085245\n",
      "EP3 EpisodeReward=-1031.6388384530508\n",
      "EP4 EpisodeReward=-650.5430065082148\n",
      "Epoch=94\t Average reward=-389.45216005340166\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-553.9386510963885\n",
      "EP1 EpisodeReward=-675.4811345617366\n",
      "EP2 EpisodeReward=-530.5543969469522\n",
      "EP3 EpisodeReward=-392.6384235697783\n",
      "EP4 EpisodeReward=-423.0133483105556\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-543.0596880734577\n",
      "EP1 EpisodeReward=-787.4449237707121\n",
      "EP2 EpisodeReward=-888.6824871701085\n",
      "EP3 EpisodeReward=-936.2187798545946\n",
      "EP4 EpisodeReward=-1027.0768743296885\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-281.78063939998856\n",
      "EP1 EpisodeReward=-551.8219153162423\n",
      "EP2 EpisodeReward=-793.8887506467931\n",
      "EP3 EpisodeReward=-769.1925526251687\n",
      "EP4 EpisodeReward=-698.3962036990044\n",
      "Epoch=95\t Average reward=-716.1621421130827\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-800.8689879714392\n",
      "EP1 EpisodeReward=-809.6547403174199\n",
      "EP2 EpisodeReward=-839.8339720857741\n",
      "EP3 EpisodeReward=-896.1133365139876\n",
      "EP4 EpisodeReward=-669.9652857047456\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-782.4484249818472\n",
      "EP1 EpisodeReward=-400.18774422456636\n",
      "EP2 EpisodeReward=-269.2715635891729\n",
      "EP3 EpisodeReward=-751.3740813760013\n",
      "EP4 EpisodeReward=-679.375753000702\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-902.7286243288256\n",
      "EP1 EpisodeReward=-1018.514301398239\n",
      "EP2 EpisodeReward=-790.6062715433799\n",
      "EP3 EpisodeReward=-919.5133898801794\n",
      "EP4 EpisodeReward=-1057.5067559208048\n",
      "Epoch=96\t Average reward=-802.282598208751\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-745.2581061080595\n",
      "EP1 EpisodeReward=-661.4446411957955\n",
      "EP2 EpisodeReward=-466.85235247384776\n",
      "EP3 EpisodeReward=-794.8393003247617\n",
      "EP4 EpisodeReward=-657.8512692098034\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-653.2448245455955\n",
      "EP1 EpisodeReward=-670.6940078062626\n",
      "EP2 EpisodeReward=-525.9743640941049\n",
      "EP3 EpisodeReward=-651.6467178027141\n",
      "EP4 EpisodeReward=-651.9106847809811\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-846.4825143761822\n",
      "EP1 EpisodeReward=-526.3822404130556\n",
      "EP2 EpisodeReward=-684.5755650307322\n",
      "EP3 EpisodeReward=-649.8339898272112\n",
      "EP4 EpisodeReward=-386.6468937896937\n",
      "Epoch=97\t Average reward=-565.4696159268261\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-545.2082446692125\n",
      "EP1 EpisodeReward=-736.9906576172106\n",
      "EP2 EpisodeReward=-670.218821063135\n",
      "EP3 EpisodeReward=-784.7771708375963\n",
      "EP4 EpisodeReward=-706.340155527262\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-672.7775996593618\n",
      "EP1 EpisodeReward=-488.788380596152\n",
      "EP2 EpisodeReward=-674.0228135284658\n",
      "EP3 EpisodeReward=-403.5259437141533\n",
      "EP4 EpisodeReward=-755.1779332029532\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-671.7933295139405\n",
      "EP1 EpisodeReward=-272.991249964519\n",
      "EP2 EpisodeReward=-676.6498262687724\n",
      "EP3 EpisodeReward=-443.6639991661168\n",
      "EP4 EpisodeReward=-636.6573998928947\n",
      "Epoch=98\t Average reward=-699.3918295410367\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-535.2505394244696\n",
      "EP1 EpisodeReward=-273.4155156990336\n",
      "EP2 EpisodeReward=-269.95932892698215\n",
      "EP3 EpisodeReward=-531.996033479568\n",
      "EP4 EpisodeReward=-802.1346427910543\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-375.72629219084797\n",
      "EP1 EpisodeReward=-768.0058913380088\n",
      "EP2 EpisodeReward=-526.2073945010666\n",
      "EP3 EpisodeReward=-796.3199115483375\n",
      "EP4 EpisodeReward=-656.0630550419678\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-362.24867586599186\n",
      "EP1 EpisodeReward=-581.8233537680197\n",
      "EP2 EpisodeReward=-405.2652034441849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP3 EpisodeReward=-8.46462679136666\n",
      "EP4 EpisodeReward=-6.547064901238234\n",
      "Epoch=99\t Average reward=-488.24825424475347\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-529.5260429791219\n",
      "EP1 EpisodeReward=-662.3452326200796\n",
      "EP2 EpisodeReward=-531.4755811439986\n",
      "EP3 EpisodeReward=-541.4844593910915\n",
      "EP4 EpisodeReward=-134.35219399414498\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-629.9073765807735\n",
      "EP1 EpisodeReward=-684.0966789278305\n",
      "EP2 EpisodeReward=-534.9945218373194\n",
      "EP3 EpisodeReward=-697.3446093551287\n",
      "EP4 EpisodeReward=-291.1987083881531\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-8.991064535706979\n",
      "EP1 EpisodeReward=-559.6518902371754\n",
      "EP2 EpisodeReward=-2.1740694528016915\n",
      "EP3 EpisodeReward=-732.189758703005\n",
      "EP4 EpisodeReward=-1062.958947349477\n",
      "Epoch=100\t Average reward=-496.1699499105917\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-270.26915737442005\n",
      "EP1 EpisodeReward=-269.82222635356777\n",
      "EP2 EpisodeReward=-263.17054431484047\n",
      "EP3 EpisodeReward=-1064.8564512964772\n",
      "EP4 EpisodeReward=-7.61764361232555\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-264.99432835585367\n",
      "EP1 EpisodeReward=-133.13546051213436\n",
      "EP2 EpisodeReward=-137.88105781307814\n",
      "EP3 EpisodeReward=-130.6165393360912\n",
      "EP4 EpisodeReward=-386.2345788171244\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-260.14686132433684\n",
      "EP1 EpisodeReward=-998.5790565046282\n",
      "EP2 EpisodeReward=-1111.9511047891556\n",
      "EP3 EpisodeReward=-1.1695507763578492\n",
      "EP4 EpisodeReward=-277.76319090829713\n",
      "Epoch=101\t Average reward=-223.87180444591567\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-276.66070547107205\n",
      "EP1 EpisodeReward=-796.0059430261815\n",
      "EP2 EpisodeReward=-925.2614189703125\n",
      "EP3 EpisodeReward=-1194.910579403694\n",
      "EP4 EpisodeReward=-1021.8532083754914\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-397.276586655356\n",
      "EP1 EpisodeReward=-272.9411502353094\n",
      "EP2 EpisodeReward=-666.8460134569648\n",
      "EP3 EpisodeReward=-679.9847846871173\n",
      "EP4 EpisodeReward=-819.4081461845377\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1433.180106533331\n",
      "EP1 EpisodeReward=-397.3836964298766\n",
      "EP2 EpisodeReward=-575.6152304909639\n",
      "EP3 EpisodeReward=-1.4782142710324675\n",
      "EP4 EpisodeReward=-686.6372600635987\n",
      "Epoch=102\t Average reward=-842.6328715412092\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-405.32904287905734\n",
      "EP1 EpisodeReward=-540.8772662149876\n",
      "EP2 EpisodeReward=-698.0915329898335\n",
      "EP3 EpisodeReward=-403.7085646186924\n",
      "EP4 EpisodeReward=-134.37401397031127\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-262.55432482878234\n",
      "EP1 EpisodeReward=-2.94862981734691\n",
      "EP2 EpisodeReward=-137.0702440003122\n",
      "EP3 EpisodeReward=-408.85919523397683\n",
      "EP4 EpisodeReward=-272.444539935626\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-687.9940376648939\n",
      "EP1 EpisodeReward=-393.60543711318576\n",
      "EP2 EpisodeReward=-403.4969056292896\n",
      "EP3 EpisodeReward=-261.4507438945953\n",
      "EP4 EpisodeReward=-1.3332510093164276\n",
      "Epoch=103\t Average reward=-136.0506016384179\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-559.4587354222675\n",
      "EP1 EpisodeReward=-752.5538585479392\n",
      "EP2 EpisodeReward=-953.9246078320062\n",
      "EP3 EpisodeReward=-1108.9936772740843\n",
      "EP4 EpisodeReward=-1119.0638423524535\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-136.95309553625876\n",
      "EP1 EpisodeReward=-388.1705483579199\n",
      "EP2 EpisodeReward=-269.7196224799742\n",
      "EP3 EpisodeReward=-544.6357972967345\n",
      "EP4 EpisodeReward=-2.2563756991288666\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-741.089370500106\n",
      "EP1 EpisodeReward=-564.3833954174179\n",
      "EP2 EpisodeReward=-131.40748052569495\n",
      "EP3 EpisodeReward=-274.82102907587574\n",
      "EP4 EpisodeReward=-276.55316599485434\n",
      "Epoch=104\t Average reward=-465.9577946821457\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1017.4025955491891\n",
      "EP1 EpisodeReward=-266.0177539733201\n",
      "EP2 EpisodeReward=-138.96249971995246\n",
      "EP3 EpisodeReward=-1.7819309477204766\n",
      "EP4 EpisodeReward=-267.87986003938187\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-136.24760784869719\n",
      "EP1 EpisodeReward=-407.33794415543673\n",
      "EP2 EpisodeReward=-9.88273203480746\n",
      "EP3 EpisodeReward=-399.9813851482869\n",
      "EP4 EpisodeReward=-685.5492208836708\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-131.77564990153346\n",
      "EP1 EpisodeReward=-279.20826655021017\n",
      "EP2 EpisodeReward=-134.91542173013968\n",
      "EP3 EpisodeReward=-846.3935605397211\n",
      "EP4 EpisodeReward=-360.43217876851423\n",
      "Epoch=105\t Average reward=-437.9537532305223\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-406.61848042236653\n",
      "EP1 EpisodeReward=-1211.2763109717941\n",
      "EP2 EpisodeReward=-0.7926744542660126\n",
      "EP3 EpisodeReward=-542.223698518689\n",
      "EP4 EpisodeReward=-410.6099422754469\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-675.5472319224676\n",
      "EP1 EpisodeReward=-1.2063620442591154\n",
      "EP2 EpisodeReward=-273.47017260987855\n",
      "EP3 EpisodeReward=-264.5786473537342\n",
      "EP4 EpisodeReward=-554.9883112825929\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-276.65703370564853\n",
      "EP1 EpisodeReward=-4.909319919504162\n",
      "EP2 EpisodeReward=-377.5584878073455\n",
      "EP3 EpisodeReward=-1095.7933053058682\n",
      "EP4 EpisodeReward=-262.59525298644377\n",
      "Epoch=106\t Average reward=-409.3978355148279\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-547.1557135090627\n",
      "EP1 EpisodeReward=-406.1049769129451\n",
      "EP2 EpisodeReward=-389.80699067707064\n",
      "EP3 EpisodeReward=-278.21623777483705\n",
      "EP4 EpisodeReward=-412.221162048432\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-398.1449312417698\n",
      "EP1 EpisodeReward=-0.7531216553679321\n",
      "EP2 EpisodeReward=-269.748938954588\n",
      "EP3 EpisodeReward=-721.6337484004445\n",
      "EP4 EpisodeReward=-133.4692370423987\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-271.57463790675786\n",
      "EP1 EpisodeReward=-131.425589412874\n",
      "EP2 EpisodeReward=-595.4395813291168\n",
      "EP3 EpisodeReward=-130.7854342036267\n",
      "EP4 EpisodeReward=-673.6399535740236\n",
      "Epoch=107\t Average reward=-406.44345088828476\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-282.19048529590486\n",
      "EP1 EpisodeReward=-6.786383807732647\n",
      "EP2 EpisodeReward=-613.1427182508237\n",
      "EP3 EpisodeReward=-966.2403703515499\n",
      "EP4 EpisodeReward=-985.732674080023\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-138.4409413464131\n",
      "EP1 EpisodeReward=-130.4505050047608\n",
      "EP2 EpisodeReward=-131.2213390718365\n",
      "EP3 EpisodeReward=-134.39932725391145\n",
      "EP4 EpisodeReward=-415.5846297731111\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-278.7383051913911\n",
      "EP1 EpisodeReward=-133.68267045553432\n",
      "EP2 EpisodeReward=-641.6886593850361\n",
      "EP3 EpisodeReward=-403.0304557188664\n",
      "EP4 EpisodeReward=-268.87127941396756\n",
      "Epoch=108\t Average reward=-556.7295277557006\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-0.6999733165464687\n",
      "EP1 EpisodeReward=-395.1726874838603\n",
      "EP2 EpisodeReward=-0.6990697545900052\n",
      "EP3 EpisodeReward=-264.3671041820728\n",
      "EP4 EpisodeReward=-399.3638223330854\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-536.4866886194372\n",
      "EP1 EpisodeReward=-951.4779404586284\n",
      "EP2 EpisodeReward=-265.4281727262018\n",
      "EP3 EpisodeReward=-539.9645914062825\n",
      "EP4 EpisodeReward=-136.62919693773762\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1140.3530727827017\n",
      "EP1 EpisodeReward=-820.9410723729412\n",
      "EP2 EpisodeReward=-677.620992106305\n",
      "EP3 EpisodeReward=-537.312114247458\n",
      "EP4 EpisodeReward=-560.725432302272\n",
      "Epoch=109\t Average reward=-365.5728171910317\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-409.0487631053476\n",
      "EP1 EpisodeReward=-133.74485654266775\n",
      "EP2 EpisodeReward=-584.2842806556065\n",
      "EP3 EpisodeReward=-596.8100793820163\n",
      "EP4 EpisodeReward=-274.0083997231445\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-2.124517463811028\n",
      "EP1 EpisodeReward=-2.5815836157466934\n",
      "EP2 EpisodeReward=-579.121001938446\n",
      "EP3 EpisodeReward=-411.8913886110106\n",
      "EP4 EpisodeReward=-1157.5352482299697\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-2.3290650530241614\n",
      "EP1 EpisodeReward=-408.1818952517213\n",
      "EP2 EpisodeReward=-0.9032243464484652\n",
      "EP3 EpisodeReward=-269.2881382647682\n",
      "EP4 EpisodeReward=-1.0006753478639991\n",
      "Epoch=110\t Average reward=-477.5147744336594\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-688.871598998262\n",
      "EP1 EpisodeReward=-715.9497225729891\n",
      "EP2 EpisodeReward=-266.5008801449215\n",
      "EP3 EpisodeReward=-551.2686634735815\n",
      "EP4 EpisodeReward=-693.7732664959744\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-398.79262566024295\n",
      "EP1 EpisodeReward=-679.6740724905002\n",
      "EP2 EpisodeReward=-813.8030057293722\n",
      "EP3 EpisodeReward=-813.4411155297206\n",
      "EP4 EpisodeReward=-267.10101872583897\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-621.4141501169202\n",
      "EP1 EpisodeReward=-132.76228936998734\n",
      "EP2 EpisodeReward=-136.17599104662287\n",
      "EP3 EpisodeReward=-135.2524167314489\n",
      "EP4 EpisodeReward=-0.5439005269772577\n",
      "Epoch=111\t Average reward=-320.4727285829302\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-135.04771063744957\n",
      "EP1 EpisodeReward=-133.9515770211734\n",
      "EP2 EpisodeReward=-565.7051650161658\n",
      "EP3 EpisodeReward=-755.1697592919581\n",
      "EP4 EpisodeReward=-636.8179082088736\n",
      "Training Agent 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP0 EpisodeReward=-267.5360051176558\n",
      "EP1 EpisodeReward=-405.0225761902247\n",
      "EP2 EpisodeReward=-194.01341245246826\n",
      "EP3 EpisodeReward=-1193.719352540475\n",
      "EP4 EpisodeReward=-913.5784964910605\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-405.3382382686698\n",
      "EP1 EpisodeReward=-3.1949985509955647\n",
      "EP2 EpisodeReward=-5.162671379614226\n",
      "EP3 EpisodeReward=-533.5387030381347\n",
      "EP4 EpisodeReward=-534.1519577455998\n",
      "Epoch=112\t Average reward=-694.8494541485112\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-9.619072137106569\n",
      "EP1 EpisodeReward=-277.4916371867635\n",
      "EP2 EpisodeReward=-674.6260481140527\n",
      "EP3 EpisodeReward=-927.6399404745637\n",
      "EP4 EpisodeReward=-136.23815910627778\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-803.7553290237868\n",
      "EP1 EpisodeReward=-407.45445546792934\n",
      "EP2 EpisodeReward=-273.35743237482353\n",
      "EP3 EpisodeReward=-263.1062942792274\n",
      "EP4 EpisodeReward=-133.48809829394798\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-276.79922943016726\n",
      "EP1 EpisodeReward=-647.5412177920211\n",
      "EP2 EpisodeReward=-808.0733951965143\n",
      "EP3 EpisodeReward=-403.87539937890585\n",
      "EP4 EpisodeReward=-819.7813046179881\n",
      "Epoch=113\t Average reward=-363.1691873394046\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-134.9093614254051\n",
      "EP1 EpisodeReward=-596.8439938248404\n",
      "EP2 EpisodeReward=-135.75355495737148\n",
      "EP3 EpisodeReward=-4.296677244428793\n",
      "EP4 EpisodeReward=-402.4529079375375\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-535.1642837766829\n",
      "EP1 EpisodeReward=-547.7020251324458\n",
      "EP2 EpisodeReward=-141.89536722739916\n",
      "EP3 EpisodeReward=-433.00440906766136\n",
      "EP4 EpisodeReward=-271.968623589779\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-398.7988672730709\n",
      "EP1 EpisodeReward=-131.82484645063772\n",
      "EP2 EpisodeReward=-415.55522557565257\n",
      "EP3 EpisodeReward=-132.82641536486733\n",
      "EP4 EpisodeReward=-263.96009647082894\n",
      "Epoch=114\t Average reward=-312.7938759993818\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-413.95821458692353\n",
      "EP1 EpisodeReward=-130.56422986086542\n",
      "EP2 EpisodeReward=-270.33292830778873\n",
      "EP3 EpisodeReward=-2.812771390842753\n",
      "EP4 EpisodeReward=-283.4705756643918\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-268.2626081633205\n",
      "EP1 EpisodeReward=-543.0554778755015\n",
      "EP2 EpisodeReward=-273.8881785860007\n",
      "EP3 EpisodeReward=-277.38749473642036\n",
      "EP4 EpisodeReward=-273.73277794756854\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-420.8482775676578\n",
      "EP1 EpisodeReward=-0.4250686763917007\n",
      "EP2 EpisodeReward=-418.0469261264109\n",
      "EP3 EpisodeReward=-416.5278464255216\n",
      "EP4 EpisodeReward=-545.0605930564703\n",
      "Epoch=115\t Average reward=-367.42131555614355\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-431.1893284879298\n",
      "EP1 EpisodeReward=-269.78412094683756\n",
      "EP2 EpisodeReward=-267.011155373466\n",
      "EP3 EpisodeReward=-131.53599854489227\n",
      "EP4 EpisodeReward=-522.4529975164248\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-266.77978154289724\n",
      "EP1 EpisodeReward=-139.22041860734288\n",
      "EP2 EpisodeReward=-399.98566232632913\n",
      "EP3 EpisodeReward=-556.5403296149191\n",
      "EP4 EpisodeReward=-420.06918881786277\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-132.16483303175107\n",
      "EP1 EpisodeReward=-132.18489359485653\n",
      "EP2 EpisodeReward=-136.99419744500028\n",
      "EP3 EpisodeReward=-272.2693655652882\n",
      "EP4 EpisodeReward=-133.91731728599333\n",
      "Epoch=116\t Average reward=-358.81316787342695\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-132.71826276900586\n",
      "EP1 EpisodeReward=-273.6660583298373\n",
      "EP2 EpisodeReward=-260.75873120221286\n",
      "EP3 EpisodeReward=-132.21321660685558\n",
      "EP4 EpisodeReward=-133.13486688093465\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-133.67198022946394\n",
      "EP1 EpisodeReward=-527.906448416264\n",
      "EP2 EpisodeReward=-436.44233407724425\n",
      "EP3 EpisodeReward=-134.45540068777495\n",
      "EP4 EpisodeReward=-429.30890262387544\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-274.93983072976414\n",
      "EP1 EpisodeReward=-285.5296341003891\n",
      "EP2 EpisodeReward=-440.52118644781245\n",
      "EP3 EpisodeReward=-0.34610863021689525\n",
      "EP4 EpisodeReward=-424.6926363020141\n",
      "Epoch=117\t Average reward=-329.04546860227475\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-816.8415093139769\n",
      "EP1 EpisodeReward=-1174.5577510592623\n",
      "EP2 EpisodeReward=-135.82125508598816\n",
      "EP3 EpisodeReward=-416.1021952301473\n",
      "EP4 EpisodeReward=-1233.9519129632797\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-423.65173988127333\n",
      "EP1 EpisodeReward=-930.1222851642711\n",
      "EP2 EpisodeReward=-130.9658386534561\n",
      "EP3 EpisodeReward=-414.0247801128611\n",
      "EP4 EpisodeReward=-269.48269517011806\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-128.74846025532875\n",
      "EP1 EpisodeReward=-278.504054921694\n",
      "EP2 EpisodeReward=-279.9653287771475\n",
      "EP3 EpisodeReward=-288.45301474942346\n",
      "EP4 EpisodeReward=-294.10848921076223\n",
      "Epoch=118\t Average reward=-599.1810324480533\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1.2982800336213773\n",
      "EP1 EpisodeReward=-0.38428788639560135\n",
      "EP2 EpisodeReward=-1101.7986556646229\n",
      "EP3 EpisodeReward=-1048.8188576997875\n",
      "EP4 EpisodeReward=-133.7992919351313\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-131.20934678077265\n",
      "EP1 EpisodeReward=-132.30524223726684\n",
      "EP2 EpisodeReward=-1275.4532483796982\n",
      "EP3 EpisodeReward=-1216.4364323399075\n",
      "EP4 EpisodeReward=-535.5957217957707\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1354.309046225734\n",
      "EP1 EpisodeReward=-1235.1492884120792\n",
      "EP2 EpisodeReward=-266.17863669335446\n",
      "EP3 EpisodeReward=-1289.133300416547\n",
      "EP4 EpisodeReward=-277.2466726394495\n",
      "Epoch=119\t Average reward=-315.54722879011723\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-461.8831247268891\n",
      "EP1 EpisodeReward=-452.8191490672274\n",
      "EP2 EpisodeReward=-797.9601113301411\n",
      "EP3 EpisodeReward=-137.21276772193727\n",
      "EP4 EpisodeReward=-384.7279666284012\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-420.3112226308338\n",
      "EP1 EpisodeReward=-819.6249036900394\n",
      "EP2 EpisodeReward=-1070.4692258682267\n",
      "EP3 EpisodeReward=-558.7755011314439\n",
      "EP4 EpisodeReward=-990.0341507143144\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-133.9505628954273\n",
      "EP1 EpisodeReward=-1349.4073986939945\n",
      "EP2 EpisodeReward=-406.42601762762723\n",
      "EP3 EpisodeReward=-267.52785122264004\n",
      "EP4 EpisodeReward=-133.0909305109362\n",
      "Epoch=120\t Average reward=-502.61768261788393\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-131.29562099203864\n",
      "EP1 EpisodeReward=-1100.481603579117\n",
      "EP2 EpisodeReward=-134.94946419489062\n",
      "EP3 EpisodeReward=-1130.8377079045017\n",
      "EP4 EpisodeReward=-1306.244471838679\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-282.3089426456878\n",
      "EP1 EpisodeReward=-1223.4398788291903\n",
      "EP2 EpisodeReward=-1283.0263902594022\n",
      "EP3 EpisodeReward=-1258.2556211521605\n",
      "EP4 EpisodeReward=-1155.5071926354185\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-934.9586392486218\n",
      "EP1 EpisodeReward=-125.54861628525842\n",
      "EP2 EpisodeReward=-269.7292594267467\n",
      "EP3 EpisodeReward=-928.3470614096711\n",
      "EP4 EpisodeReward=-10.677015102603384\n",
      "Epoch=121\t Average reward=-824.1428931922336\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1099.2765230273562\n",
      "EP1 EpisodeReward=-1088.049454418212\n",
      "EP2 EpisodeReward=-1320.1419943930523\n",
      "EP3 EpisodeReward=-1080.1272055796726\n",
      "EP4 EpisodeReward=-1372.8591224835623\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1193.4013574904177\n",
      "EP1 EpisodeReward=-1196.5979956977842\n",
      "EP2 EpisodeReward=-937.1905807165213\n",
      "EP3 EpisodeReward=-529.7617561478705\n",
      "EP4 EpisodeReward=-391.0159502639881\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1180.617848136017\n",
      "EP1 EpisodeReward=-1115.9406813839223\n",
      "EP2 EpisodeReward=-970.7965032486188\n",
      "EP3 EpisodeReward=-1061.0180211449053\n",
      "EP4 EpisodeReward=-1063.1123467311925\n",
      "Epoch=122\t Average reward=-942.3291398262476\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-802.7117274033798\n",
      "EP1 EpisodeReward=-1106.2765760948694\n",
      "EP2 EpisodeReward=-1259.029263971551\n",
      "EP3 EpisodeReward=-856.3349145484393\n",
      "EP4 EpisodeReward=-438.9002383808648\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-943.9650392203189\n",
      "EP1 EpisodeReward=-651.2679469150158\n",
      "EP2 EpisodeReward=-948.283462834202\n",
      "EP3 EpisodeReward=-789.6051835744335\n",
      "EP4 EpisodeReward=-685.2554603593046\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-804.1564976491417\n",
      "EP1 EpisodeReward=-929.3001030172577\n",
      "EP2 EpisodeReward=-827.2591459521464\n",
      "EP3 EpisodeReward=-272.2667273089595\n",
      "EP4 EpisodeReward=-135.05772773266722\n",
      "Epoch=123\t Average reward=-419.73780882427883\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1081.6960331551436\n",
      "EP1 EpisodeReward=-5.030771160587025\n",
      "EP2 EpisodeReward=-132.09101624780712\n",
      "EP3 EpisodeReward=-390.00101179571556\n",
      "EP4 EpisodeReward=-128.62979871983416\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-397.14965736000346\n",
      "EP1 EpisodeReward=-404.2747686233701\n",
      "EP2 EpisodeReward=-264.00494250417614\n",
      "EP3 EpisodeReward=-133.19174009931922\n",
      "EP4 EpisodeReward=-1296.371483821063\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1174.105824340773\n",
      "EP1 EpisodeReward=-137.46780874107904\n",
      "EP2 EpisodeReward=-131.03115353122212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP3 EpisodeReward=-1114.2700107602586\n",
      "EP4 EpisodeReward=-1107.7659085804503\n",
      "Epoch=124\t Average reward=-844.2557303737825\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1041.5752195709633\n",
      "EP1 EpisodeReward=-255.4458722277431\n",
      "EP2 EpisodeReward=-273.6793791765754\n",
      "EP3 EpisodeReward=-128.5075329439765\n",
      "EP4 EpisodeReward=-265.1109561615597\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-130.79354184095092\n",
      "EP1 EpisodeReward=-660.1483036086596\n",
      "EP2 EpisodeReward=-458.5710305973335\n",
      "EP3 EpisodeReward=-267.4137737909638\n",
      "EP4 EpisodeReward=-1089.6823463958453\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-128.19816858386423\n",
      "EP1 EpisodeReward=-1086.4193988561237\n",
      "EP2 EpisodeReward=-418.21664056409463\n",
      "EP3 EpisodeReward=-272.81269544426704\n",
      "EP4 EpisodeReward=-264.23013064330905\n",
      "Epoch=125\t Average reward=-539.6744777335713\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-132.3302635564835\n",
      "EP1 EpisodeReward=-135.01869397428132\n",
      "EP2 EpisodeReward=-724.668218778763\n",
      "EP3 EpisodeReward=-1232.8559136679867\n",
      "EP4 EpisodeReward=-1223.2029108507795\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-0.7620157421447626\n",
      "EP1 EpisodeReward=-616.8414285225714\n",
      "EP2 EpisodeReward=-270.4067170776149\n",
      "EP3 EpisodeReward=-755.0264686049111\n",
      "EP4 EpisodeReward=-261.58970209799577\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-556.4090492081334\n",
      "EP1 EpisodeReward=-126.39068709674885\n",
      "EP2 EpisodeReward=-384.96439948699833\n",
      "EP3 EpisodeReward=-539.1064710170168\n",
      "EP4 EpisodeReward=-902.4537625295499\n",
      "Epoch=126\t Average reward=-795.7487918261085\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-991.6909232031596\n",
      "EP1 EpisodeReward=-268.4798476473915\n",
      "EP2 EpisodeReward=-401.7986361875587\n",
      "EP3 EpisodeReward=-277.6376997149924\n",
      "EP4 EpisodeReward=-134.16278773460155\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-828.810977609003\n",
      "EP1 EpisodeReward=-555.3473804165934\n",
      "EP2 EpisodeReward=-776.9373383352957\n",
      "EP3 EpisodeReward=-658.9516111341687\n",
      "EP4 EpisodeReward=-479.8776091110548\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-470.0402313605668\n",
      "EP1 EpisodeReward=-264.8225223775834\n",
      "EP2 EpisodeReward=-273.1278889515531\n",
      "EP3 EpisodeReward=-275.70982161153256\n",
      "EP4 EpisodeReward=-2.627509660267325\n",
      "Epoch=127\t Average reward=-205.5559688353079\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-524.8965278897889\n",
      "EP1 EpisodeReward=-541.4953096480608\n",
      "EP2 EpisodeReward=-2.3129323791359453\n",
      "EP3 EpisodeReward=-1.267387217034304\n",
      "EP4 EpisodeReward=-0.7942123182190105\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-383.087906737586\n",
      "EP1 EpisodeReward=-270.21800688077576\n",
      "EP2 EpisodeReward=-0.45071721591181657\n",
      "EP3 EpisodeReward=-137.69268567884606\n",
      "EP4 EpisodeReward=-1199.6607707656885\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-2.4413137737321526\n",
      "EP1 EpisodeReward=-131.25085722782973\n",
      "EP2 EpisodeReward=-541.392675151059\n",
      "EP3 EpisodeReward=-1204.8405702940381\n",
      "EP4 EpisodeReward=-1294.769157752259\n",
      "Epoch=128\t Average reward=-831.7413802787222\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-268.39910552839484\n",
      "EP1 EpisodeReward=-6.225541016633189\n",
      "EP2 EpisodeReward=-404.058134311284\n",
      "EP3 EpisodeReward=-483.4906146458386\n",
      "EP4 EpisodeReward=-929.5705882808642\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-135.00919549914494\n",
      "EP1 EpisodeReward=-281.4605652062545\n",
      "EP2 EpisodeReward=-395.13973441266205\n",
      "EP3 EpisodeReward=-582.857220999476\n",
      "EP4 EpisodeReward=-267.61347607413666\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-442.8284609640747\n",
      "EP1 EpisodeReward=-127.95230563893153\n",
      "EP2 EpisodeReward=-139.73970556769999\n",
      "EP3 EpisodeReward=-1057.4721499333784\n",
      "EP4 EpisodeReward=-787.1203616637071\n",
      "Epoch=129\t Average reward=-661.4348086729027\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-133.96326245901784\n",
      "EP1 EpisodeReward=-266.2168202549218\n",
      "EP2 EpisodeReward=-128.5105728378945\n",
      "EP3 EpisodeReward=-961.3112108716305\n",
      "EP4 EpisodeReward=-1237.5622292135827\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-529.0459469860466\n",
      "EP1 EpisodeReward=-394.71673532471635\n",
      "EP2 EpisodeReward=-134.89970362404048\n",
      "EP3 EpisodeReward=-131.22908301398846\n",
      "EP4 EpisodeReward=-625.7925376649465\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-132.64356296535115\n",
      "EP1 EpisodeReward=-127.48198842504577\n",
      "EP2 EpisodeReward=-1.1987539681992696\n",
      "EP3 EpisodeReward=-260.46223617407964\n",
      "EP4 EpisodeReward=-261.79912375200837\n",
      "Epoch=130\t Average reward=-708.3846302101791\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-263.0885603338739\n",
      "EP1 EpisodeReward=-129.0193983675607\n",
      "EP2 EpisodeReward=-982.5004910128741\n",
      "EP3 EpisodeReward=-1155.720545918648\n",
      "EP4 EpisodeReward=-261.5120789468334\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-128.74732305898814\n",
      "EP1 EpisodeReward=-0.8166413124605265\n",
      "EP2 EpisodeReward=-267.4945073860656\n",
      "EP3 EpisodeReward=-732.5875310517755\n",
      "EP4 EpisodeReward=-131.21758440932817\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-537.3077021077213\n",
      "EP1 EpisodeReward=-131.21492787053543\n",
      "EP2 EpisodeReward=-268.3800043156174\n",
      "EP3 EpisodeReward=-264.06016132738273\n",
      "EP4 EpisodeReward=-853.1687781918246\n",
      "Epoch=131\t Average reward=-415.2994805159954\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-834.2915569628112\n",
      "EP1 EpisodeReward=-788.4067930894818\n",
      "EP2 EpisodeReward=-712.6119310544142\n",
      "EP3 EpisodeReward=-659.2485782188168\n",
      "EP4 EpisodeReward=-671.4241038936068\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-400.36678529313014\n",
      "EP1 EpisodeReward=-671.1190587314082\n",
      "EP2 EpisodeReward=-526.9295013869628\n",
      "EP3 EpisodeReward=-934.8629039425849\n",
      "EP4 EpisodeReward=-1109.170092689719\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-412.42179187993145\n",
      "EP1 EpisodeReward=-128.4506522070611\n",
      "EP2 EpisodeReward=-786.5970881875082\n",
      "EP3 EpisodeReward=-291.6098273554253\n",
      "EP4 EpisodeReward=-2.9589832790737858\n",
      "Epoch=132\t Average reward=-594.5177266207999\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1075.6366105876903\n",
      "EP1 EpisodeReward=-1138.9087057550703\n",
      "EP2 EpisodeReward=-963.8063542559155\n",
      "EP3 EpisodeReward=-974.7013268591326\n",
      "EP4 EpisodeReward=-272.4902238118645\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1203.0391176386315\n",
      "EP1 EpisodeReward=-668.1500836867204\n",
      "EP2 EpisodeReward=-515.8961454440529\n",
      "EP3 EpisodeReward=-660.4925290501768\n",
      "EP4 EpisodeReward=-782.3200786298241\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1076.6916205660743\n",
      "EP1 EpisodeReward=-843.2447015728557\n",
      "EP2 EpisodeReward=-750.1956078491238\n",
      "EP3 EpisodeReward=-665.0383240430143\n",
      "EP4 EpisodeReward=-805.7142118122368\n",
      "Epoch=133\t Average reward=-620.1748380846419\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-578.3290984712589\n",
      "EP1 EpisodeReward=-265.4093020996634\n",
      "EP2 EpisodeReward=-403.3864237208789\n",
      "EP3 EpisodeReward=-440.5043038716242\n",
      "EP4 EpisodeReward=-9.467981711036709\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-272.4274331301677\n",
      "EP1 EpisodeReward=-134.76490348151813\n",
      "EP2 EpisodeReward=-917.3746133209577\n",
      "EP3 EpisodeReward=-402.5044454868473\n",
      "EP4 EpisodeReward=-806.1738234818373\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-403.5649404450892\n",
      "EP1 EpisodeReward=-513.6071087517248\n",
      "EP2 EpisodeReward=-691.489910117153\n",
      "EP3 EpisodeReward=-941.4896341537342\n",
      "EP4 EpisodeReward=-991.9479549262317\n",
      "Epoch=134\t Average reward=-602.5299200397019\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-130.39485241459906\n",
      "EP1 EpisodeReward=-260.0580962136266\n",
      "EP2 EpisodeReward=-526.6337334589847\n",
      "EP3 EpisodeReward=-3.8499647711043172\n",
      "EP4 EpisodeReward=-527.9883790728984\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-263.27491965059636\n",
      "EP1 EpisodeReward=-726.4662568072115\n",
      "EP2 EpisodeReward=-133.6777257223547\n",
      "EP3 EpisodeReward=-3.263567531826866\n",
      "EP4 EpisodeReward=-261.17512643257777\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-136.50626971509357\n",
      "EP1 EpisodeReward=-2.6875822768399584\n",
      "EP2 EpisodeReward=-135.13896234937368\n",
      "EP3 EpisodeReward=-381.9479408538229\n",
      "EP4 EpisodeReward=-544.3661592291946\n",
      "Epoch=135\t Average reward=-444.5098882448903\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-408.65115487452886\n",
      "EP1 EpisodeReward=-133.4444476300317\n",
      "EP2 EpisodeReward=-532.0910067605632\n",
      "EP3 EpisodeReward=-669.5236190836468\n",
      "EP4 EpisodeReward=-543.3227518470009\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-554.4649017277516\n",
      "EP1 EpisodeReward=-561.1827948531737\n",
      "EP2 EpisodeReward=-666.2957886296956\n",
      "EP3 EpisodeReward=-139.4088951863249\n",
      "EP4 EpisodeReward=-130.70867095044278\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-659.7537359446715\n",
      "EP1 EpisodeReward=-132.99521758796615\n",
      "EP2 EpisodeReward=-0.8784523436654817\n",
      "EP3 EpisodeReward=-131.7582492102125\n",
      "EP4 EpisodeReward=-134.3275840796476\n",
      "Epoch=136\t Average reward=-269.45300229236375\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-131.76456521957334\n",
      "EP1 EpisodeReward=-388.4912901205321\n",
      "EP2 EpisodeReward=-2.3084594179562576\n",
      "EP3 EpisodeReward=-269.8527909347388\n",
      "EP4 EpisodeReward=-132.0035625503983\n",
      "Training Agent 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP0 EpisodeReward=-262.72205290694546\n",
      "EP1 EpisodeReward=-790.7954865266731\n",
      "EP2 EpisodeReward=-136.0774828408385\n",
      "EP3 EpisodeReward=-5.149091665200754\n",
      "EP4 EpisodeReward=-940.0167902457343\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-135.0027355165069\n",
      "EP1 EpisodeReward=-165.9555822493747\n",
      "EP2 EpisodeReward=-270.8407931618315\n",
      "EP3 EpisodeReward=-410.4462471475787\n",
      "EP4 EpisodeReward=-139.21302596966137\n",
      "Epoch=137\t Average reward=-403.744459588598\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-555.2919933845791\n",
      "EP1 EpisodeReward=-398.64264095304867\n",
      "EP2 EpisodeReward=-528.3748421638891\n",
      "EP3 EpisodeReward=-435.89823976547916\n",
      "EP4 EpisodeReward=-526.666936227943\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-276.20544671611765\n",
      "EP1 EpisodeReward=-693.193027534103\n",
      "EP2 EpisodeReward=-4.776841787809774\n",
      "EP3 EpisodeReward=-265.95610326649285\n",
      "EP4 EpisodeReward=-3.081238332299956\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-417.0740657738691\n",
      "EP1 EpisodeReward=-129.39089510079867\n",
      "EP2 EpisodeReward=-263.95316891521253\n",
      "EP3 EpisodeReward=-274.5145612311736\n",
      "EP4 EpisodeReward=-132.56744559896148\n",
      "Epoch=138\t Average reward=-220.77187338640147\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-392.88265388674654\n",
      "EP1 EpisodeReward=-522.5462491541319\n",
      "EP2 EpisodeReward=-651.1807702322494\n",
      "EP3 EpisodeReward=-522.8763822621866\n",
      "EP4 EpisodeReward=-654.1123706499899\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-390.7320418404511\n",
      "EP1 EpisodeReward=-132.03745285594275\n",
      "EP2 EpisodeReward=-130.71823883530598\n",
      "EP3 EpisodeReward=-1.7874567612412322\n",
      "EP4 EpisodeReward=-270.3515080715235\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1.3554928230792147\n",
      "EP1 EpisodeReward=-256.96034443253455\n",
      "EP2 EpisodeReward=-397.8820853496216\n",
      "EP3 EpisodeReward=-268.8462993071661\n",
      "EP4 EpisodeReward=-128.72610493687168\n",
      "Epoch=139\t Average reward=-351.0633278861283\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-276.8913185931234\n",
      "EP1 EpisodeReward=-271.190715622386\n",
      "EP2 EpisodeReward=-0.8079256169076182\n",
      "EP3 EpisodeReward=-133.10245242145288\n",
      "EP4 EpisodeReward=-458.43747217876313\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-404.3730961732652\n",
      "EP1 EpisodeReward=-0.9714124004286688\n",
      "EP2 EpisodeReward=-382.4762978422772\n",
      "EP3 EpisodeReward=-258.32051678609236\n",
      "EP4 EpisodeReward=-671.7701365564517\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-437.38934542518444\n",
      "EP1 EpisodeReward=-273.26820839953695\n",
      "EP2 EpisodeReward=-517.5475031237175\n",
      "EP3 EpisodeReward=-672.6276386196897\n",
      "EP4 EpisodeReward=-133.93171809048528\n",
      "Epoch=140\t Average reward=-421.37977560856666\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-532.3953770536623\n",
      "EP1 EpisodeReward=-135.81291760571963\n",
      "EP2 EpisodeReward=-139.31003549739447\n",
      "EP3 EpisodeReward=-650.8593957109629\n",
      "EP4 EpisodeReward=-441.8508678177576\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-263.1451871277555\n",
      "EP1 EpisodeReward=-381.840244780507\n",
      "EP2 EpisodeReward=-261.29532775368955\n",
      "EP3 EpisodeReward=-527.7705246101615\n",
      "EP4 EpisodeReward=-134.99356715505812\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-274.5897906116551\n",
      "EP1 EpisodeReward=-136.01616528347947\n",
      "EP2 EpisodeReward=-134.14387085787126\n",
      "EP3 EpisodeReward=-1.6293647143056131\n",
      "EP4 EpisodeReward=-427.8831646082418\n",
      "Epoch=141\t Average reward=-334.90919986035254\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-395.0669668307055\n",
      "EP1 EpisodeReward=-132.2240363734357\n",
      "EP2 EpisodeReward=-260.5379349883716\n",
      "EP3 EpisodeReward=-378.4532508851499\n",
      "EP4 EpisodeReward=-2.400355081642087\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-271.5552813821924\n",
      "EP1 EpisodeReward=-480.12236967625586\n",
      "EP2 EpisodeReward=-783.8730886989661\n",
      "EP3 EpisodeReward=-265.1177258028253\n",
      "EP4 EpisodeReward=-264.5726329460905\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-392.8070736138654\n",
      "EP1 EpisodeReward=-668.7951863568474\n",
      "EP2 EpisodeReward=-846.8653397855708\n",
      "EP3 EpisodeReward=-1071.0914399893106\n",
      "EP4 EpisodeReward=-1228.9643258827252\n",
      "Epoch=142\t Average reward=-498.6457713034859\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-916.1227929222333\n",
      "EP1 EpisodeReward=-525.0926549838072\n",
      "EP2 EpisodeReward=-395.4599363324077\n",
      "EP3 EpisodeReward=-517.6547782081069\n",
      "EP4 EpisodeReward=-178.1374627448588\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-135.0720429432446\n",
      "EP1 EpisodeReward=-130.51554099162055\n",
      "EP2 EpisodeReward=-1073.9116880087913\n",
      "EP3 EpisodeReward=-1.7246149968131568\n",
      "EP4 EpisodeReward=-260.9946178438356\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1.6212060236172583\n",
      "EP1 EpisodeReward=-130.13159133794602\n",
      "EP2 EpisodeReward=-1.1148968968621829\n",
      "EP3 EpisodeReward=-431.7047927330262\n",
      "EP4 EpisodeReward=-1016.7987120463841\n",
      "Epoch=143\t Average reward=-485.3102642116928\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-258.42984848101975\n",
      "EP1 EpisodeReward=-384.65616990586346\n",
      "EP2 EpisodeReward=-409.1876755664886\n",
      "EP3 EpisodeReward=-258.7742309775831\n",
      "EP4 EpisodeReward=-133.47528416794745\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-130.278721820296\n",
      "EP1 EpisodeReward=-136.52067546010565\n",
      "EP2 EpisodeReward=-131.8383883538731\n",
      "EP3 EpisodeReward=-4.209383586931362\n",
      "EP4 EpisodeReward=-139.35923097313298\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1070.7804119456812\n",
      "EP1 EpisodeReward=-129.41544450554505\n",
      "EP2 EpisodeReward=-265.1744347439963\n",
      "EP3 EpisodeReward=-543.4890243375588\n",
      "EP4 EpisodeReward=-136.2956445483359\n",
      "Epoch=144\t Average reward=-136.3767198964721\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-138.56942836240844\n",
      "EP1 EpisodeReward=-3.068674235292413\n",
      "EP2 EpisodeReward=-390.46762099508265\n",
      "EP3 EpisodeReward=-3.356736620168792\n",
      "EP4 EpisodeReward=-1.640223679330284\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-394.1915368615779\n",
      "EP1 EpisodeReward=-128.62044829413009\n",
      "EP2 EpisodeReward=-779.5856688506516\n",
      "EP3 EpisodeReward=-382.492611753727\n",
      "EP4 EpisodeReward=-520.3097518635101\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-391.9961781216534\n",
      "EP1 EpisodeReward=-130.27453307433558\n",
      "EP2 EpisodeReward=-658.0947587091955\n",
      "EP3 EpisodeReward=-397.16106891088936\n",
      "EP4 EpisodeReward=-491.8116809810547\n",
      "Epoch=145\t Average reward=-337.92055217463167\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-526.559745947327\n",
      "EP1 EpisodeReward=-605.0978440206849\n",
      "EP2 EpisodeReward=-393.72611113857744\n",
      "EP3 EpisodeReward=-530.2207834055299\n",
      "EP4 EpisodeReward=-552.6688941553942\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-259.7690167554668\n",
      "EP1 EpisodeReward=-150.81228687171148\n",
      "EP2 EpisodeReward=-917.2057793455525\n",
      "EP3 EpisodeReward=-743.7207479932629\n",
      "EP4 EpisodeReward=-395.3134107153494\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-129.69390232414045\n",
      "EP1 EpisodeReward=-937.4932084830027\n",
      "EP2 EpisodeReward=-658.6306703692375\n",
      "EP3 EpisodeReward=-262.9538999603678\n",
      "EP4 EpisodeReward=-131.95469781940216\n",
      "Epoch=146\t Average reward=-359.97900089671526\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-129.87598702071608\n",
      "EP1 EpisodeReward=-127.58043137955178\n",
      "EP2 EpisodeReward=-269.11055595877946\n",
      "EP3 EpisodeReward=-812.5315045242457\n",
      "EP4 EpisodeReward=-1234.3282069303566\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-397.6503918896772\n",
      "EP1 EpisodeReward=-130.63068447669644\n",
      "EP2 EpisodeReward=-517.4228708033806\n",
      "EP3 EpisodeReward=-1036.9361487460214\n",
      "EP4 EpisodeReward=-130.24669532096158\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-129.81643253072153\n",
      "EP1 EpisodeReward=-530.7796949810428\n",
      "EP2 EpisodeReward=-554.5829015624854\n",
      "EP3 EpisodeReward=-660.3992234221129\n",
      "EP4 EpisodeReward=-523.3000370680024\n",
      "Epoch=147\t Average reward=-629.2916464397736\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-730.0645553083419\n",
      "EP1 EpisodeReward=-514.6951836931429\n",
      "EP2 EpisodeReward=-432.19616345667276\n",
      "EP3 EpisodeReward=-684.6050378690767\n",
      "EP4 EpisodeReward=-392.4835579468068\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-132.05596885070335\n",
      "EP1 EpisodeReward=-523.23956275643\n",
      "EP2 EpisodeReward=-130.18517093269622\n",
      "EP3 EpisodeReward=-399.4963390885503\n",
      "EP4 EpisodeReward=-1.8040499688480445\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-133.29684850168346\n",
      "EP1 EpisodeReward=-544.813480577625\n",
      "EP2 EpisodeReward=-776.1049384503804\n",
      "EP3 EpisodeReward=-464.4876966456902\n",
      "EP4 EpisodeReward=-779.6655285693263\n",
      "Epoch=148\t Average reward=-391.3177121616604\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-264.6413334947772\n",
      "EP1 EpisodeReward=-263.309297837005\n",
      "EP2 EpisodeReward=-825.9916714516237\n",
      "EP3 EpisodeReward=-536.4887956100291\n",
      "EP4 EpisodeReward=-666.0496762678174\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-724.2992330015996\n",
      "EP1 EpisodeReward=-842.9175884554007\n",
      "EP2 EpisodeReward=-655.2528410529387\n",
      "EP3 EpisodeReward=-417.51151611198486\n",
      "EP4 EpisodeReward=-235.7047736011629\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-588.6470460640003\n",
      "EP1 EpisodeReward=-772.9432638734021\n",
      "EP2 EpisodeReward=-534.2643435019503\n",
      "EP3 EpisodeReward=-568.3240166944985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP4 EpisodeReward=-130.01668004365848\n",
      "Epoch=149\t Average reward=-343.9237099708796\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-522.1397747584975\n",
      "EP1 EpisodeReward=-630.2208678056818\n",
      "EP2 EpisodeReward=-784.9676356338454\n",
      "EP3 EpisodeReward=-526.4822187755668\n",
      "EP4 EpisodeReward=-654.4956555332884\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-129.63067722517934\n",
      "EP1 EpisodeReward=-278.7945469576061\n",
      "EP2 EpisodeReward=-754.4962224305647\n",
      "EP3 EpisodeReward=-1098.7104983030792\n",
      "EP4 EpisodeReward=-1229.5276828901517\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-134.19476945600664\n",
      "EP1 EpisodeReward=-400.15799106574474\n",
      "EP2 EpisodeReward=-542.1381138260218\n",
      "EP3 EpisodeReward=-526.7155133421926\n",
      "EP4 EpisodeReward=-569.4991996282162\n",
      "Epoch=150\t Average reward=-817.8408460172187\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-397.28882207937403\n",
      "EP1 EpisodeReward=-940.7316909313917\n",
      "EP2 EpisodeReward=-1064.42097023561\n",
      "EP3 EpisodeReward=-769.8771075518608\n",
      "EP4 EpisodeReward=-819.4640926023505\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-134.59391225065662\n",
      "EP1 EpisodeReward=-261.4789984228058\n",
      "EP2 EpisodeReward=-527.4112652697764\n",
      "EP3 EpisodeReward=-268.4172970609022\n",
      "EP4 EpisodeReward=-526.8842141759185\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-544.7267186132608\n",
      "EP1 EpisodeReward=-981.6128520556344\n",
      "EP2 EpisodeReward=-920.4851901272341\n",
      "EP3 EpisodeReward=-522.2208055819242\n",
      "EP4 EpisodeReward=-616.2308467716579\n",
      "Epoch=151\t Average reward=-654.193051183309\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-760.8548756494235\n",
      "EP1 EpisodeReward=-661.4559902972447\n",
      "EP2 EpisodeReward=-715.152308511522\n",
      "EP3 EpisodeReward=-808.3260927313353\n",
      "EP4 EpisodeReward=-660.4540312680323\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-760.9590686511856\n",
      "EP1 EpisodeReward=-794.3496657977714\n",
      "EP2 EpisodeReward=-778.1273877058394\n",
      "EP3 EpisodeReward=-764.6252963465951\n",
      "EP4 EpisodeReward=-653.4783521977062\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-679.9753045214759\n",
      "EP1 EpisodeReward=-1206.9453814206183\n",
      "EP2 EpisodeReward=-1314.730581063805\n",
      "EP3 EpisodeReward=-1184.5078455265175\n",
      "EP4 EpisodeReward=-1180.4991781668034\n",
      "Epoch=152\t Average reward=-831.4771872108473\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-708.814794462325\n",
      "EP1 EpisodeReward=-393.2404871322095\n",
      "EP2 EpisodeReward=-527.3164992758649\n",
      "EP3 EpisodeReward=-782.0736717537036\n",
      "EP4 EpisodeReward=-526.9114880975042\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-779.0184363469992\n",
      "EP1 EpisodeReward=-518.6690566521573\n",
      "EP2 EpisodeReward=-135.48745473637635\n",
      "EP3 EpisodeReward=-775.8842142977037\n",
      "EP4 EpisodeReward=-920.8215586970292\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-528.5711534831937\n",
      "EP1 EpisodeReward=-576.4745001198442\n",
      "EP2 EpisodeReward=-963.3847965046883\n",
      "EP3 EpisodeReward=-688.2084769230765\n",
      "EP4 EpisodeReward=-1050.6929947529343\n",
      "Epoch=153\t Average reward=-832.8086805158226\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-945.166382677871\n",
      "EP1 EpisodeReward=-531.7449330215921\n",
      "EP2 EpisodeReward=-389.6214390026473\n",
      "EP3 EpisodeReward=-515.7800350121242\n",
      "EP4 EpisodeReward=-647.2748383815351\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-549.0279225471261\n",
      "EP1 EpisodeReward=-658.0505562065812\n",
      "EP2 EpisodeReward=-846.8238416582071\n",
      "EP3 EpisodeReward=-835.4014205100407\n",
      "EP4 EpisodeReward=-1035.9106295621755\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1012.9763119447749\n",
      "EP1 EpisodeReward=-519.0372824043193\n",
      "EP2 EpisodeReward=-525.9187193257941\n",
      "EP3 EpisodeReward=-3.1665628751984354\n",
      "EP4 EpisodeReward=-525.0498546784804\n",
      "Epoch=154\t Average reward=-736.0784408740637\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-912.2518256520498\n",
      "EP1 EpisodeReward=-1044.3362338501279\n",
      "EP2 EpisodeReward=-1183.6867658169492\n",
      "EP3 EpisodeReward=-1274.0963207927114\n",
      "EP4 EpisodeReward=-1325.1412108865663\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-585.7214239088719\n",
      "EP1 EpisodeReward=-899.8953722134822\n",
      "EP2 EpisodeReward=-1101.328264254345\n",
      "EP3 EpisodeReward=-785.2676342477118\n",
      "EP4 EpisodeReward=-7.8604002296362365\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-652.0274693783871\n",
      "EP1 EpisodeReward=-652.4314454130987\n",
      "EP2 EpisodeReward=-958.5730713898458\n",
      "EP3 EpisodeReward=-770.3732496325763\n",
      "EP4 EpisodeReward=-901.9199436270887\n",
      "Epoch=155\t Average reward=-744.9738515810972\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1090.907236642249\n",
      "EP1 EpisodeReward=-1008.6124259828026\n",
      "EP2 EpisodeReward=-996.9670488872176\n",
      "EP3 EpisodeReward=-1071.6213059454221\n",
      "EP4 EpisodeReward=-1049.3017651322486\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-881.4216182185759\n",
      "EP1 EpisodeReward=-1088.5374163734662\n",
      "EP2 EpisodeReward=-1076.159894689302\n",
      "EP3 EpisodeReward=-1003.3966021605636\n",
      "EP4 EpisodeReward=-1149.1544553312676\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-982.4361428584712\n",
      "EP1 EpisodeReward=-1055.0531329204007\n",
      "EP2 EpisodeReward=-1011.2276471065592\n",
      "EP3 EpisodeReward=-950.010260123091\n",
      "EP4 EpisodeReward=-1079.6455600648062\n",
      "Epoch=156\t Average reward=-1092.700593509441\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1036.396896783445\n",
      "EP1 EpisodeReward=-1036.944482097445\n",
      "EP2 EpisodeReward=-1072.85308700687\n",
      "EP3 EpisodeReward=-950.6547140972795\n",
      "EP4 EpisodeReward=-1032.3780264671993\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1060.0691652756104\n",
      "EP1 EpisodeReward=-1186.6257177618063\n",
      "EP2 EpisodeReward=-1116.6103827725954\n",
      "EP3 EpisodeReward=-811.4200827514371\n",
      "EP4 EpisodeReward=-1102.754238734381\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1188.4348742339866\n",
      "EP1 EpisodeReward=-1298.6433039530568\n",
      "EP2 EpisodeReward=-1062.9866667441322\n",
      "EP3 EpisodeReward=-1260.9166493306575\n",
      "EP4 EpisodeReward=-3.2892385802624466\n",
      "Epoch=157\t Average reward=-712.8071679272811\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-783.7672669066851\n",
      "EP1 EpisodeReward=-1125.2322845437413\n",
      "EP2 EpisodeReward=-1147.6621453711352\n",
      "EP3 EpisodeReward=-1076.478519237123\n",
      "EP4 EpisodeReward=-1085.3098564365407\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1164.19175443945\n",
      "EP1 EpisodeReward=-1101.9389842699138\n",
      "EP2 EpisodeReward=-1091.368182302533\n",
      "EP3 EpisodeReward=-1187.9203651309113\n",
      "EP4 EpisodeReward=-936.6075852596543\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1076.3722546098131\n",
      "EP1 EpisodeReward=-1170.8305736175798\n",
      "EP2 EpisodeReward=-1033.4916217648636\n",
      "EP3 EpisodeReward=-936.5050232311279\n",
      "EP4 EpisodeReward=-992.77740790182\n",
      "Epoch=158\t Average reward=-1004.8982831993384\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1030.724205591071\n",
      "EP1 EpisodeReward=-1138.3668169515252\n",
      "EP2 EpisodeReward=-1162.692525265069\n",
      "EP3 EpisodeReward=-1062.2147375766444\n",
      "EP4 EpisodeReward=-1019.7860884119558\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1096.3121627451033\n",
      "EP1 EpisodeReward=-1110.4840031837728\n",
      "EP2 EpisodeReward=-1131.592804500964\n",
      "EP3 EpisodeReward=-945.2748467511997\n",
      "EP4 EpisodeReward=-2.997367237583264\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1055.233374069745\n",
      "EP1 EpisodeReward=-1309.691729598648\n",
      "EP2 EpisodeReward=-1280.102391343704\n",
      "EP3 EpisodeReward=-1248.1791856087434\n",
      "EP4 EpisodeReward=-785.4075987949316\n",
      "Epoch=159\t Average reward=-602.7303514814902\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1104.6605063155698\n",
      "EP1 EpisodeReward=-1169.3613542893668\n",
      "EP2 EpisodeReward=-1280.0285773211845\n",
      "EP3 EpisodeReward=-530.1714316134313\n",
      "EP4 EpisodeReward=-1060.3366881667037\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1072.8686685918242\n",
      "EP1 EpisodeReward=-1150.6564619226024\n",
      "EP2 EpisodeReward=-948.7294827494615\n",
      "EP3 EpisodeReward=-1024.375541138238\n",
      "EP4 EpisodeReward=-1066.6078577483736\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1159.3976208086199\n",
      "EP1 EpisodeReward=-1059.8405023730115\n",
      "EP2 EpisodeReward=-964.2296924454679\n",
      "EP3 EpisodeReward=-1080.1968647136748\n",
      "EP4 EpisodeReward=-1102.2312876440906\n",
      "Epoch=160\t Average reward=-1076.3919445197228\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1009.1776260208454\n",
      "EP1 EpisodeReward=-1117.9296103624606\n",
      "EP2 EpisodeReward=-1241.0492316376135\n",
      "EP3 EpisodeReward=-1366.4493306434313\n",
      "EP4 EpisodeReward=-1070.4726227976319\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1234.6307820789284\n",
      "EP1 EpisodeReward=-917.1322557576754\n",
      "EP2 EpisodeReward=-1029.6175416160386\n",
      "EP3 EpisodeReward=-412.23602766403724\n",
      "EP4 EpisodeReward=-528.9238858703196\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-897.7406672912974\n",
      "EP1 EpisodeReward=-911.3677697319198\n",
      "EP2 EpisodeReward=-802.934339807529\n",
      "EP3 EpisodeReward=-909.5031273518407\n",
      "EP4 EpisodeReward=-1091.5396480792783\n",
      "Epoch=161\t Average reward=-896.9787189157432\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-967.5645313223813\n",
      "EP1 EpisodeReward=-799.239362716775\n",
      "EP2 EpisodeReward=-1168.1930633538798\n",
      "EP3 EpisodeReward=-1146.9735424080914\n",
      "EP4 EpisodeReward=-1349.8946476039046\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-971.7237431022221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP1 EpisodeReward=-1091.1989749560926\n",
      "EP2 EpisodeReward=-1069.8455575872972\n",
      "EP3 EpisodeReward=-1146.3403524096916\n",
      "EP4 EpisodeReward=-1112.155596203772\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1076.174856244368\n",
      "EP1 EpisodeReward=-1088.1070685463155\n",
      "EP2 EpisodeReward=-942.8163977969052\n",
      "EP3 EpisodeReward=-745.2043662067287\n",
      "EP4 EpisodeReward=-911.8329776858549\n",
      "Epoch=162\t Average reward=-1124.627740497844\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1110.8672995734055\n",
      "EP1 EpisodeReward=-928.0567102456266\n",
      "EP2 EpisodeReward=-889.586185000728\n",
      "EP3 EpisodeReward=-1037.3863001103734\n",
      "EP4 EpisodeReward=-1000.3073324380814\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1196.4189519320419\n",
      "EP1 EpisodeReward=-1119.5860464043767\n",
      "EP2 EpisodeReward=-797.1394677386371\n",
      "EP3 EpisodeReward=-999.405006286803\n",
      "EP4 EpisodeReward=-794.4902256218747\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-929.7471071323723\n",
      "EP1 EpisodeReward=-1167.9107662438075\n",
      "EP2 EpisodeReward=-1115.0197400053667\n",
      "EP3 EpisodeReward=-1228.5728835956063\n",
      "EP4 EpisodeReward=-1103.4963737501387\n",
      "Epoch=163\t Average reward=-966.0979772700316\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1039.648997392203\n",
      "EP1 EpisodeReward=-960.8439674553924\n",
      "EP2 EpisodeReward=-837.9931337931664\n",
      "EP3 EpisodeReward=-1024.1231098451074\n",
      "EP4 EpisodeReward=-986.5411933197437\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1015.5035020638292\n",
      "EP1 EpisodeReward=-1043.7514765649491\n",
      "EP2 EpisodeReward=-913.9852880633046\n",
      "EP3 EpisodeReward=-786.5718947525563\n",
      "EP4 EpisodeReward=-567.0710523078537\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-926.8110165842102\n",
      "EP1 EpisodeReward=-1005.6550635807115\n",
      "EP2 EpisodeReward=-676.9408564713355\n",
      "EP3 EpisodeReward=-654.5030899327933\n",
      "EP4 EpisodeReward=-658.6219663093647\n",
      "Epoch=164\t Average reward=-737.4114039789874\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-897.2490189594021\n",
      "EP1 EpisodeReward=-1129.6548810670924\n",
      "EP2 EpisodeReward=-911.3592088710637\n",
      "EP3 EpisodeReward=-1076.4858005961114\n",
      "EP4 EpisodeReward=-948.0644421558675\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1095.4384214107363\n",
      "EP1 EpisodeReward=-784.3695022643757\n",
      "EP2 EpisodeReward=-780.5774562440306\n",
      "EP3 EpisodeReward=-767.2502611371648\n",
      "EP4 EpisodeReward=-788.5937181533666\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1040.0881539914278\n",
      "EP1 EpisodeReward=-923.1745654151306\n",
      "EP2 EpisodeReward=-1172.1031799230568\n",
      "EP3 EpisodeReward=-978.8913458321753\n",
      "EP4 EpisodeReward=-941.2379423958539\n",
      "Epoch=165\t Average reward=-892.6320342350294\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-906.4363570838492\n",
      "EP1 EpisodeReward=-1040.3262125486574\n",
      "EP2 EpisodeReward=-846.2050596601243\n",
      "EP3 EpisodeReward=-1050.293307165012\n",
      "EP4 EpisodeReward=-1032.521272637679\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1048.4512130883352\n",
      "EP1 EpisodeReward=-925.9458987936886\n",
      "EP2 EpisodeReward=-964.6743802429901\n",
      "EP3 EpisodeReward=-1042.1839355137424\n",
      "EP4 EpisodeReward=-1180.6306429137956\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-904.4523789306545\n",
      "EP1 EpisodeReward=-921.3340331439653\n",
      "EP2 EpisodeReward=-991.6446809758366\n",
      "EP3 EpisodeReward=-875.5929594471637\n",
      "EP4 EpisodeReward=-884.5519629994867\n",
      "Epoch=166\t Average reward=-1032.567959516987\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1073.7399357800846\n",
      "EP1 EpisodeReward=-915.6166130853171\n",
      "EP2 EpisodeReward=-942.3556015764415\n",
      "EP3 EpisodeReward=-1182.1373252457722\n",
      "EP4 EpisodeReward=-965.6467148122517\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1034.4082713375087\n",
      "EP1 EpisodeReward=-1081.118800685899\n",
      "EP2 EpisodeReward=-1130.7590462663977\n",
      "EP3 EpisodeReward=-1090.9569332433216\n",
      "EP4 EpisodeReward=-1094.7829495883936\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1089.599409640438\n",
      "EP1 EpisodeReward=-1108.3726492268083\n",
      "EP2 EpisodeReward=-1047.9626391344548\n",
      "EP3 EpisodeReward=-1128.6566545601193\n",
      "EP4 EpisodeReward=-1082.75937739296\n",
      "Epoch=167\t Average reward=-1047.7296805978685\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1053.5998058997206\n",
      "EP1 EpisodeReward=-891.0462053297499\n",
      "EP2 EpisodeReward=-1435.5169448379804\n",
      "EP3 EpisodeReward=-1208.1490098159418\n",
      "EP4 EpisodeReward=-978.6888505595794\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1104.860097047819\n",
      "EP1 EpisodeReward=-1178.7740739918363\n",
      "EP2 EpisodeReward=-1111.04920335578\n",
      "EP3 EpisodeReward=-1191.9008291002995\n",
      "EP4 EpisodeReward=-1179.1218485754314\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1090.4762600092577\n",
      "EP1 EpisodeReward=-1120.6027122922935\n",
      "EP2 EpisodeReward=-921.6992074517474\n",
      "EP3 EpisodeReward=-1096.331921519277\n",
      "EP4 EpisodeReward=-1044.6773184431704\n",
      "Epoch=168\t Average reward=-1067.4960058593936\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-986.9118563371901\n",
      "EP1 EpisodeReward=-1311.2058912363145\n",
      "EP2 EpisodeReward=-1205.189389108269\n",
      "EP3 EpisodeReward=-1122.986857021754\n",
      "EP4 EpisodeReward=-1089.1462523844611\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1031.185054216469\n",
      "EP1 EpisodeReward=-1187.5870406603472\n",
      "EP2 EpisodeReward=-926.4388312076028\n",
      "EP3 EpisodeReward=-1197.7176387056677\n",
      "EP4 EpisodeReward=-1336.3802741265774\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1092.859468699041\n",
      "EP1 EpisodeReward=-1206.2977660311935\n",
      "EP2 EpisodeReward=-1247.21048952081\n",
      "EP3 EpisodeReward=-1061.8382108033397\n",
      "EP4 EpisodeReward=-662.3849834273028\n",
      "Epoch=169\t Average reward=-1029.3038366461137\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-957.4590998449845\n",
      "EP1 EpisodeReward=-922.0528499059265\n",
      "EP2 EpisodeReward=-1060.1247914688736\n",
      "EP3 EpisodeReward=-940.1894725608488\n",
      "EP4 EpisodeReward=-916.5679083472487\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1040.735254952216\n",
      "EP1 EpisodeReward=-1064.7608508756477\n",
      "EP2 EpisodeReward=-1054.3109677482623\n",
      "EP3 EpisodeReward=-947.6185367212664\n",
      "EP4 EpisodeReward=-1063.5404276756815\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1114.0898385891473\n",
      "EP1 EpisodeReward=-1044.296909384317\n",
      "EP2 EpisodeReward=-940.019492011868\n",
      "EP3 EpisodeReward=-810.0891450292231\n",
      "EP4 EpisodeReward=-911.9282634801642\n",
      "Epoch=170\t Average reward=-964.0121998343648\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-905.4895081472872\n",
      "EP1 EpisodeReward=-928.9202250885068\n",
      "EP2 EpisodeReward=-916.7270818952524\n",
      "EP3 EpisodeReward=-912.7630887641456\n",
      "EP4 EpisodeReward=-773.7261007603669\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-1042.3098952879727\n",
      "EP1 EpisodeReward=-945.0536431616351\n",
      "EP2 EpisodeReward=-895.6143813097615\n",
      "EP3 EpisodeReward=-667.4014923883473\n",
      "EP4 EpisodeReward=-772.3300836344392\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-811.5785957541862\n",
      "EP1 EpisodeReward=-542.3028543601233\n",
      "EP2 EpisodeReward=-1085.493373758484\n",
      "EP3 EpisodeReward=-798.5312136883475\n",
      "EP4 EpisodeReward=-797.9637527081401\n",
      "Epoch=171\t Average reward=-781.3399790343154\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-1016.1022341739217\n",
      "EP1 EpisodeReward=-1056.3918892466904\n",
      "EP2 EpisodeReward=-919.5963125548515\n",
      "EP3 EpisodeReward=-1075.8601762691437\n",
      "EP4 EpisodeReward=-1074.5822004185277\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-734.5995300780888\n",
      "EP1 EpisodeReward=-660.2290993121225\n",
      "EP2 EpisodeReward=-785.703879999677\n",
      "EP3 EpisodeReward=-774.8271974267019\n",
      "EP4 EpisodeReward=-660.8186691557046\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-906.1738818657359\n",
      "EP1 EpisodeReward=-1026.784769773186\n",
      "EP2 EpisodeReward=-1119.8881385582506\n",
      "EP3 EpisodeReward=-1051.6669568664147\n",
      "EP4 EpisodeReward=-1183.7739812122586\n",
      "Epoch=172\t Average reward=-973.058283595497\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-891.656614657809\n",
      "EP1 EpisodeReward=-1031.1322322751082\n",
      "EP2 EpisodeReward=-1560.1277055767366\n",
      "EP3 EpisodeReward=-1453.8518617452205\n",
      "EP4 EpisodeReward=-1552.8289609609408\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-955.4457355549577\n",
      "EP1 EpisodeReward=-1021.6999281168336\n",
      "EP2 EpisodeReward=-903.5735210569255\n",
      "EP3 EpisodeReward=-913.6846683032448\n",
      "EP4 EpisodeReward=-911.7493235275261\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1059.9817253729166\n",
      "EP1 EpisodeReward=-1050.4280164954077\n",
      "EP2 EpisodeReward=-936.6278564428209\n",
      "EP3 EpisodeReward=-1018.6007241791624\n",
      "EP4 EpisodeReward=-937.0198578550202\n",
      "Epoch=173\t Average reward=-1133.866047447829\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-904.3501396783948\n",
      "EP1 EpisodeReward=-660.2081813389179\n",
      "EP2 EpisodeReward=-684.7485028063134\n",
      "EP3 EpisodeReward=-1230.2425070176255\n",
      "EP4 EpisodeReward=-985.9730549976993\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-910.9317321509068\n",
      "EP1 EpisodeReward=-930.0287631725506\n",
      "EP2 EpisodeReward=-793.2409889387617\n",
      "EP3 EpisodeReward=-784.971453363568\n",
      "EP4 EpisodeReward=-791.1994942192424\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-1069.4864739286593\n",
      "EP1 EpisodeReward=-1011.1027479937474\n",
      "EP2 EpisodeReward=-1047.273712399411\n",
      "EP3 EpisodeReward=-1093.0428159231446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP4 EpisodeReward=-991.1856506894906\n",
      "Epoch=174\t Average reward=-922.7860666354774\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-956.8313820838428\n",
      "EP1 EpisodeReward=-925.3585874655217\n",
      "EP2 EpisodeReward=-786.9315048819855\n",
      "EP3 EpisodeReward=-849.2912519029929\n",
      "EP4 EpisodeReward=-1298.3014490855635\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-884.1684868797339\n",
      "EP1 EpisodeReward=-1142.52069212976\n",
      "EP2 EpisodeReward=-937.657714223539\n",
      "EP3 EpisodeReward=-775.2275565212576\n",
      "EP4 EpisodeReward=-808.2463911864784\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-935.8094286807219\n",
      "EP1 EpisodeReward=-867.8054205207739\n",
      "EP2 EpisodeReward=-940.2115682646527\n",
      "EP3 EpisodeReward=-1036.1957761548467\n",
      "EP4 EpisodeReward=-964.3947911951142\n",
      "Epoch=175\t Average reward=-1023.6475438223853\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-818.4463217650477\n",
      "EP1 EpisodeReward=-530.3287294656997\n",
      "EP2 EpisodeReward=-1212.0469561747213\n",
      "EP3 EpisodeReward=-676.24712123654\n",
      "EP4 EpisodeReward=-1219.4906649601185\n",
      "Training Agent 1\n",
      "EP0 EpisodeReward=-909.577698789975\n",
      "EP1 EpisodeReward=-803.1121901179542\n",
      "EP2 EpisodeReward=-767.1509522341626\n",
      "EP3 EpisodeReward=-1043.0121220171702\n",
      "EP4 EpisodeReward=-1057.5447965601606\n",
      "Training Agent 2\n",
      "EP0 EpisodeReward=-785.3172820993366\n",
      "EP1 EpisodeReward=-797.328659579276\n",
      "EP2 EpisodeReward=-1219.495345224649\n",
      "EP3 EpisodeReward=-1094.6460098053446\n",
      "EP4 EpisodeReward=-1358.1274949140995\n",
      "Epoch=176\t Average reward=-1211.720985478126\n",
      "Training Agent 0\n",
      "EP0 EpisodeReward=-792.3294924687912\n",
      "EP1 EpisodeReward=-891.4846711492593\n",
      "EP2 EpisodeReward=-1083.2909506897813\n",
      "EP3 EpisodeReward=-1081.0721560997943\n",
      "EP4 EpisodeReward=-1211.8158816565697\n",
      "Training Agent 1\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    try: wandb.finish()\n",
    "    except: pass\n",
    "    \n",
    "    ####configurations\n",
    "    wandb.init(name='PPO-multiple-long', project=\"deep-rl-tf2\")\n",
    "    env_name = 'Pendulum-v0'\n",
    "\n",
    "    \n",
    "    wandb.config.gamma = 0.99\n",
    "    wandb.config.update_interval = 5\n",
    "    wandb.config.actor_lr = 0.0005\n",
    "    wandb.config.critic_lr = 0.001\n",
    "    wandb.config.batch_size = 64\n",
    "    wandb.config.clip_ratio = 0.1\n",
    "    wandb.config.lmbda = 0.95\n",
    "    wandb.config.intervals = 3\n",
    "    \n",
    "    wandb.config.episodes = 5\n",
    "    wandb.config.num = 3\n",
    "    wandb.config.epochs = 200\n",
    "\n",
    "    wandb.config.actor = {'layer1': 32, 'layer2' : 32}\n",
    "    wandb.config.critic = {'layer1': 32, 'layer2' : 32, 'layer3': 16}\n",
    "    \n",
    "    print(wandb.config)\n",
    "    \n",
    "    # main run    \n",
    "    N = wandb.config.num\n",
    "    agents = []\n",
    "    \n",
    "    # set up the agent\n",
    "    for i in range(N):\n",
    "        env_t = gym.make(env_name)\n",
    "        agents.append(Agent(env_t, i))\n",
    "\n",
    "    # start the training\n",
    "    for z in range(wandb.config.epochs):\n",
    "\n",
    "        reward = 0\n",
    "        # train the agent\n",
    "        for j in range(len(agents)):\n",
    "            print('Training Agent {}'.format(agents[j].iden))\n",
    "            reward += agents[j].train(wandb.config.episodes)\n",
    "    \n",
    "        reward = reward / N\n",
    "        print('Epoch={}\\t Average reward={}'.format(z, reward))\n",
    "        wandb.log({'batch': z, 'Epoch': reward})\n",
    "\n",
    "\n",
    "        # get the average - actor and critic\n",
    "        critic_avg = []\n",
    "        actor_avg = []\n",
    "\n",
    "        for i in range(len(agents[0].actor.model.get_weights())):\n",
    "            \n",
    "            actor_t = agents[0].actor.model.get_weights()[i]\n",
    "\n",
    "            for j in range(1, N):\n",
    "                actor_t += agents[j].actor.model.get_weights()[i]\n",
    "\n",
    "            actor_t = actor_t / N\n",
    "            actor_avg.append(actor_t)\n",
    "\n",
    "\n",
    "        for i in range(len(agents[0].critic.model.get_weights())):\n",
    "            critic_t = agents[0].critic.model.get_weights()[i]\n",
    "\n",
    "            for j in range(1, N):\n",
    "                critic_t += agents[j].critic.model.get_weights()[i]\n",
    "\n",
    "            critic_t = critic_t / N\n",
    "            critic_avg.append(critic_t)\n",
    "\n",
    "\n",
    "        # set the average\n",
    "        for j in range(N):\n",
    "            agents[j].actor.model.set_weights(actor_avg)\n",
    "            agents[j].critic.model.set_weights(critic_avg)\n",
    "\n",
    "\n",
    "    # wrtie things out\n",
    "    for j in range(N):\n",
    "        with open(\"agent{}-actor.txt\".format(j), \"w\") as f:\n",
    "            f.write(str(agents[j].actor.model.get_weights()))\n",
    "            f.close()\n",
    "        wandb.save(\"agent{}-actor.txt\".format(j))\n",
    "        \n",
    "        \n",
    "        with open(\"agent{}-critic.txt\".format(j), \"w\") as f:\n",
    "            f.write(str(agents[j].critic.model.get_weights()))\n",
    "            f.close()\n",
    "        wandb.save(\"agent{}-critic.txt\".format(j))\n",
    "\n",
    "    \n",
    "    wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-es100]",
   "language": "python",
   "name": "conda-env-.conda-es100-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
