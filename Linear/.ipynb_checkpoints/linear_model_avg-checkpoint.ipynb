{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda\n",
    "\n",
    "import gym\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pendulum:\n",
    "    \n",
    "    def __init__(self, env, iden = 0):\n",
    "        self.env = env\n",
    "        self.state_dim = self.env.observation_space.shape[0]\n",
    "        self.action_dim = self.env.action_space.shape[0]\n",
    "        self.action_bound = self.env.action_space.high[0]\n",
    "        self.std_bound = [1e-2, 1.0]\n",
    "\n",
    "        self.iden = iden\n",
    "    \n",
    "    def train(self, max_episodes=1000):\n",
    "        for ep in range(max_episodes):\n",
    "            state_batch = []\n",
    "            action_batch = []\n",
    "            reward_batch = []\n",
    "            old_policy_batch = []\n",
    "\n",
    "            episode_reward, done = 0, False\n",
    "\n",
    "            state = self.env.reset()\n",
    "\n",
    "            while not done:\n",
    "                # self.env.render()\n",
    "                log_old_policy, action = self.actor.get_action(state)\n",
    "\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "\n",
    "                state = np.reshape(state, [1, self.state_dim])\n",
    "                action = np.reshape(action, [1, 1])\n",
    "                next_state = np.reshape(next_state, [1, self.state_dim])\n",
    "                reward = np.reshape(reward, [1, 1])\n",
    "                log_old_policy = np.reshape(log_old_policy, [1, 1])\n",
    "\n",
    "            print('EP{} EpisodeReward={}'.format(ep, episode_reward))\n",
    "            wandb.log({'Reward' + str(self.iden): episode_reward})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    def __init__(self, env, Kp, gain):\n",
    "        \n",
    "        # initial proportional controller\n",
    "        # real and test (test used for gradient descent)f\n",
    "        # need to be numpy vectors\n",
    "        self.Kp_r = Kp\n",
    "        self.gain_r = gain\n",
    "        \n",
    "#         self.Kp_t = self.Kp_r\n",
    "#         self.gain_t = self.gain_r\n",
    "        \n",
    "#         self.x0 = x0\n",
    "        \n",
    "        self.radius = 0.001\n",
    "        \n",
    "        # define the environment\n",
    "        self.env = env\n",
    "        self.state_dim = self.env.observation_space.shape[0]\n",
    "        self.action_dim = self.env.action_space.shape[0]\n",
    "        self.action_bound = self.env.action_space.high[0]\n",
    "        self.std_bound = [1e-2, 1.0]\n",
    "\n",
    "        self.iden = iden\n",
    "        \n",
    "    # Simulate the environment and get the reward out\n",
    "    def simulate(self, prop, g):\n",
    "        state_batch = []\n",
    "        action_batch = []\n",
    "        reward_batch = []\n",
    "        old_policy_batch = []\n",
    "\n",
    "        episode_reward, done = 0, False\n",
    "\n",
    "        state = self.env.reset()\n",
    "\n",
    "        # define the action taken\n",
    "        def get_action(self, st):\n",
    "            action = prop @ state + g\n",
    "            return(action)\n",
    "        \n",
    "        while not done:\n",
    "            action = get_action(state)\n",
    "            next_state, reward, done, _ = self.env.step(action)\n",
    "\n",
    "            state = np.reshape(state, [1, self.state_dim])\n",
    "            action = np.reshape(action, [1, 1])\n",
    "            next_state = np.reshape(next_state, [1, self.state_dim])\n",
    "            reward = np.reshape(reward, [1, 1])\n",
    "            \n",
    "            episode_reward += reward\n",
    "\n",
    "#         print('EP{} EpisodeReward={}'.format(ep, episode_reward))\n",
    "#         wandb.log({'Reward' + str(self.iden): episode_reward})\n",
    "        return episode_reward\n",
    "\n",
    "    \n",
    "    def learn(self):\n",
    "        \n",
    "         # pick a vector on unit sphere to move Kp in\n",
    "        # vec = (np.random.rand(self.Kp.size + self.gain.size) - 0.5)\n",
    "        vec = (np.random.rand(self.Kp_r.size + 1) - 0.5)\n",
    "        vec = self.radius * vec / np.linalg.norm(vec)\n",
    "        vec_k = np.reshape(vec[:self.Kp_r.size], self.Kp_r.shape)\n",
    "\n",
    "        vec_g = vec[-1:]\n",
    "\n",
    "        rand_start = self.x0#  + np.random.randn(self.x0.size) * 0.1\n",
    "\n",
    "        # two-point gradient descent estimate\n",
    "        self.Kp_t = self.Kp_r + vec_k\n",
    "        self.gain_t = self.gain_r + vec_g\n",
    "        _, err1 = self.simulate(rand_start)\n",
    "\n",
    "        self.Kp_t = self.Kp_r - vec_k\n",
    "        self.gain_t = self.gain_r - vec_g\n",
    "        _, err2 = self.simulate(rand_start)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "Kp = np.array([0, 0])\n",
    "gain = np.array([0])\n",
    "\n",
    "ag = Agent(Kp, gain)\n",
    "\n",
    "ag.simulate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-es100]",
   "language": "python",
   "name": "conda-env-.conda-es100-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
